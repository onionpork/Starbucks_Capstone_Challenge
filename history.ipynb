 1/1:
import pands as pd 
df = pd.read_csv('BME_TEST.txt', header=None)
df.head()
 1/2:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', header=None)
df.head()
 1/3:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', header=None)
df.head(150)
 1/4:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', header=None)
print(df.[0])
 1/5:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', header=None)
print(df.loc[0])
 1/6:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', header=None)
print(df.loc[0].shape())
 1/7:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', header=None)
print(df.loc[0])
 1/8:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', header=None)
print(df.ndim)
 1/9:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', header=None)
print(df.shape)
1/10:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', header=None)

df.info()
1/11:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', sep="",header=None)
1/12:
import pandas as pd 
df = pd.read_tabel('BME_TEST.txt',header=None)
1/13:
import pandas as pd 
df = pd.read_table('BME_TEST.txt',header=None)
1/14:
import pandas as pd 
df = pd.read_table('BME_TEST.txt',header=None)
df.head()
1/15:
import pandas as pd 
df = pd.read_table('BME_TEST.txt',header=None)
print(df.head())
1/16:
import pandas as pd 
df = pd.read_csv('BME_TRAIN.text', sep=' ', head=None)
df.head()
1/17:
import pandas as pd 
df = pd.read_csv('BME_TRAIN.text', sep=' ', header=None)
df.head()
1/18:
import pandas as pd 
df = pd.read_csv('BME_TRAIN.text', sep=' ', head=None)
df.head()
1/19:
import pandas as pd 
df = pd.read_csv('BME_TRAIN.text', sep=' ')
df.head()
1/20:
import pandas as pd 
df = pd.read_csv('BME_TRAIN.txt', sep=' ', head=None)
df.head()
1/21:
import pandas as pd 
df = pd.read_csv('BME_TRAIN.txt', sep=' ')
df.head()
1/22:
import pandas as pd 
df = pd.read_csv('BME_TRAIN.txt', sep=" ",header=None)
df.head()
1/23:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', sep="",header=None)
df.head()
1/24:
import pandas as pd 
df = pd.read_csv('BME_TEST.txt', sep=" ", header=None)
df.head()
 2/1:
import pandas as pd 
df = pd.read_csv('BME_TRAIN.txt', sep=" ")
df.head()
 2/2:
import pandas as pd 
df = pd.read_csv('BME_TRAIN.txt', sep="",header=None)
df.head()
 2/3:
import pandas as pd 
df = pd.read_csv('BME_TRAIN.txt',header=None)
df.head()
 2/4:
import pandas as pd 
df = pd.read_fwf('BME_TRAIN.txt')
 2/5:
import pandas as pd 
df = pd.read_fwf('BME_TRAIN.txt')
df.head()
 2/6:
import pandas as pd 
df = pd.read_fwf('BME_TRAIN.txt',header=None)
df.head()
 2/7:
import pandas as pd 
df = pd.read_fwf('BME_TRAIN.txt',header=None)
df.info()
 2/8:
import pandas as pd 
df = pd.read_fwf('BME_TRAIN.txt',header=None)
df.head()
 2/9:
import pandas as pd 
df = pd.read_fwf('BME_TRAIN.txt',header=None)
df.loc[0]
2/10:
import pandas as pd 
df = pd.read_fwf('BME_TRAIN.txt',header=None)
df.info()
2/11:
import pandas as pd 
df = pd.read_fwf('BME_TRAIN.txt',header=None)
df.shape[1]
2/12:
import pandas as pd 
df = pd.read_fwf('BME_TRAIN.txt',header=None)
df.shape[0]
2/13:
def df_new(df):
    df_new = df
    for i in range(df.shape[0]):
        df_new.loc[i] = np.diff(df.loc[i])
    return df_new
2/14: df_new = df_new(df)
2/15:
import numpy as np
def df_new(df):
    df_new = df
    for i in range(df.shape[0]):
        df_new.loc[i] = np.diff(df.loc[i])
    return df_new
2/16: df_new = df_new(df)
2/17:
import numpy as np
def df_new(df):
    df_new = df[:-1]
    for i in range(df.shape[0]):
        df_new.loc[i] = np.diff(df.loc[i])
    return df_new
2/18: df_new = df_new(df)
2/19:
import pandas as pd 
df = pd.read_fwf('BME_TRAIN.txt',header=None)
df.shape[0]
df_new = df[:-2]
2/20:
import pandas as pd 
df = pd.read_fwf('BME_TRAIN.txt',header=None)
df.shape[0]
df_new = df[:-2]
df_new.shape()
 3/1:
import csv
txt_file = r"cbas.result"
csv_file = r"cbas.csv"
in_txt = csv.reader(open(txt_file),'rb', delimiter = '\t')
out_csv = csv.writer(open(csv_file),'wb')
out_csv.writerows(in_txt)
 3/2:
import csv
txt_file = r"cbas.result"
csv_file = r"cbas.csv"
in_txt = csv.reader(open(txt_file,'rb'), delimiter = '\t')
out_csv = csv.writer(open(csv_file),'wb'))
out_csv.writerows(in_txt)
 3/3:
import csv
txt_file = r"cbas.result"
csv_file = r"cbas.csv"
in_txt = csv.reader(open(txt_file,'rb'), delimiter = '\t')
out_csv = csv.writer(open(csv_file,'wb'))
out_csv.writerows(in_txt)
 3/4:
import csv
txt_file = r"cbas.result"
csv_file = r"cbas.csv"
in_txt = csv.reader(open(txt_file,'rb'), delimiter = '\t')
out_csv = csv.writer(open(csv_file,'wb'))
out_csv.writerows(in_txt)
 3/5:
import csv
df = csv.read('cabs.result')
 3/6:
import csv
import pandas as pd
df = pd.read('cabs.result')
 3/7:
import csv
import pandas as pd
df = pd.read_csv('cabs.result')
 3/8:
import csv
import pandas as pd
df = pd.read_txt('cabs.result')
 3/9:
import csv
import pandas as pd
df = pd.read_txt('cabs.result', header = None)
3/10:
import csv
import pandas as pd
df = pd.read_csv('cabs.result', header = None)
3/11:
import csv
import pandas as pd
df = pd.read_csv('cabs.result', sep= " " header = None)
3/12:
import csv
import pandas as pd
df = pd.read_csv('cabs.result', sep= " " header = None)
3/13:
import csv
import pandas as pd
df = pd.read_csv('cabs.result', sep= " "ï¼Œ header = None)
3/14:
import csv
import pandas as pd
df = pd.read_csv('cabs.result', sep= " ", header = None)
3/15:
import csv
import pandas as pd
df = pd.read_csv('cabs.csv',header = None)
3/16:
import pandas as pd
df = pd.read_csv('cabs.csv', header = None)
3/17:
import pandas as pd
df = pd.read_csv('cbas.csv', header = None)
3/18: print(df)
 4/1:
import pandas as pd
df = pd.read_csv('cbas_canu.result')
 4/2: df.head()
 4/3:
import pandas as pd
df = pd.read_csv('cbas_canu.result', sep ='\t', header = None)
 4/4: df.head()
 4/5: import seaborn as sns
 4/6: df.groupby[0]
 4/7: df.groupby(level =0)
 4/8: df.groupby(level =0).head()
 4/9:
import pandas as pd
import seaborn as sns
df = pd.read_csv('cbas_canu.result', sep ='\t',  names=['contig','locus', 'coverage'])
4/10: df.head()
4/11:
df.hist(bins=30, color = 'steeblue', edgecolor='black', linewidth=1.0, xlabelsize=8, ylabelsize=8, grid=False)
plt.tight_layout(rect=(0, 0, 1.2, 1.2))
4/12: df.groupby('contig')
4/13: print(df.groupby('contig'))
4/14: print(df.groupby('contig').head(10)
4/15: df.groupby('contig').head(10)
4/16: df.groupby('contig').size()
4/17: df.groupby('contig').describe()
4/18: df.groupby('contig').info()
4/19: df.info()
4/20: df2.info()
4/21: df2 = df.groupby('contig')
4/22: df2.info()
4/23:
contig_name = df['contigs']
contig_name = contig_name.drop_duplicates()
4/24:
contig_name = df['contig']
contig_name = contig_name.drop_duplicates()
4/25:
contig_name = df['contig']
contig_name = contig_name.drop_duplicates()
print('contig_name')
4/26:
contig_name = df['contig']
contig_name = contig_name.drop_duplicates()
print(contig_name)
4/27:
contig_name = df['contig']
contig_name = contig_name.drop_duplicates()
contig_name.size()
4/28:
contig_name = df['contig']
contig_name = contig_name.drop_duplicates()
contig_name.info()
4/29:
contig_name = df['contig']
contig_name = contig_name.drop_duplicates()
contig_name.info()
4/30:
contig_name = df['contig']
contig_name = contig_name.drop_duplicates()
contig_name.size()
4/31:
contig_name = df['contig']
contig_name = contig_name.drop_duplicates()
print(len(contig_name))
4/32:
contig_name = df['contig']
contig_name = contig_name.drop_duplicates()
type(contig_name)
4/33:
contig_name = df['contig']
contig_name = contig_name.drop_duplicates()
contig_name_numpy = ser.to_numpy(contig_name)
4/34:
contig_name = df['contig']
contig_name = contig_name.drop_duplicates()
contig_name_numpy = contig_name.to_numpy()
4/35: print(contig_name_numpy)
4/36: for i in range(len(contig_name_numpy)):
4/37:
g = sns.FacetGrid(df, col = 'locus', row = 'contig')
g = g.map(plt.hist, "total_bill")
 5/1:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
df = pd.read_csv('cbas_canu.result', sep ='\t',  names=['contig','locus', 'coverage'])
 5/2:
g = sns.FacetGrid(df, col = 'locus', row = 'contig')
g = g.map(plt.hist, 'contig')
 6/1:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
 6/2: df = pd.read_csv(cbas_idbaud.result, sep ='\t',  names=['contig','locus', 'coverage'])
 6/3: df = pd.read_csv('cbas_idbaud.result', sep ='\t',  names=['contig','locus', 'coverage'])
 6/4: df.head()
 8/1:
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1KvxyyF3QCtvIx0J7_8iWDEtFQpLgd0Yq")
  
print(data.head())
 8/2:
import pandas
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1KvxyyF3QCtvIx0J7_8iWDEtFQpLgd0Yq")
  
print(data.head())
 8/3: print(data.describe())
 8/4: print(data.groupby(['country']).size())
 8/5: print(data.groupby(['country']))
 8/6: print(data.groupby(['country']).size())
 8/7: data.groupby(['country']).size()
 8/8:
import matplotlib.pyplot as plt
from matplotlib import rcParams
rcParams.update({'figure.autolayout': True})
  
data.groupby(['country'])['converted'].mean().plot.bar()
plt.show()
 8/9:
import matplotlib.pyplot as plt
%matplotlib inline
from matplotlib import rcParams
rcParams.update({'figure.autolayout': True})
  
data.groupby(['country'])['converted'].mean().plot.bar()
plt.show()
8/10:
import matplotlib.pyplot as plt
%matplotlib inline
from matplotlib import rcParams
#rcParams.update({'figure.autolayout': True})
  
data.groupby(['country'])['converted'].mean().plot.bar()
plt.show()
8/11:
import matplotlib.pyplot as plt
%matplotlib inline
from matplotlib import rcParams
#rcParams.update({'figure.autolayout': True})
  
data.groupby(['country'])['converted'].mean().plot.bar()
plt.show()
8/12: print(data.groupby(['country'])['converted'])
8/13: print(data.groupby(['country'])['converted'].size())
8/14: print(data.groupby(['country'])['converted'].mean())
8/15: print(data.groupby(['country'])['converted'].values())
8/16: print(data['converted'].groupby(['country']).size())
8/17: print(data['converted'].groupby(['country']))
8/18: print(data.groupby(['country'])['converted'])
8/19: print(data.groupby(['country'])['converted'])
8/20:
import matplotlib.pyplot as plt
%matplotlib inline
from matplotlib import rcParams
#rcParams.update({'figure.autolayout': True})

data.groupby(['country'])['converted'].mean().plot.bar()
plt.show()
8/21:
import matplotlib.pyplot as plt
%matplotlib inline
from matplotlib import rcParams
rcParams.update({'figure.autolayout': True})
data.groupby(['country'])['converted'].mean().plot.bar()
plt.show()
8/22:
import matplotlib.pyplot as plt
%matplotlib inline
#from matplotlib import rcParams
#rcParams.update({'figure.autolayout': True})
data.groupby(['country'])['converted'].mean().plot.bar()
plt.show()
8/23:
data.groupby(['total_pages_visited'])['converted'].mean().plot()
plt.show()
8/24: data_dummy = pandas.get_dummies(data, drop_first=True)
8/25: data_dummy.head()
8/26: data_dummy.describe()
8/27: data_dummy2 = pandas.get_dummies(data)
8/28: data_dummy2.describe()
8/29: data_dummy.describe()
8/30:
import numpy as np
from sklearn.ensemble import RandomForestClassifier
8/31:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
np.random.seed(4684)
8/32: train, test = train_test_split(data_dummy, test_size = 0.34)
8/33:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
np.random.seed(4684)
  
#split into train and test to avoid overfitting
train, test = train_test_split(data_dummy, test_size = 0.34)
  
#build the model
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('converted', axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
8/34: print(rf.oob_decision_function_)
8/35: print(rf.oob_decision_function_[:,1])
8/36: print(rf.oob_decision_function_[:,1].round())
8/37:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
np.random.seed(4684)
  
#split into train and test to avoid overfitting
train, test = train_test_split(data_dummy, test_size = 0.34)
  
#build the model
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('converted', axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[x, y]))
)
8/38:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
np.random.seed(4684)
  
#split into train and test to avoid overfitting
train, test = train_test_split(data_dummy, test_size = 0.34)
  
#build the model
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('converted', axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=['x', 'y']))
)
8/39:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
np.random.seed(4684)
  
#split into train and test to avoid overfitting
train, test = train_test_split(data_dummy, test_size = 0.34)
  
#build the model
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('converted', axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
8/40:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
np.random.seed(4684)
  
#split into train and test to avoid overfitting
train, test = train_test_split(data_dummy, test_size = 0.34)
  
#build the model
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('converted', axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_, labels=[0, 1]))
)
8/41:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
np.random.seed(4684)
  
#split into train and test to avoid overfitting
train, test = train_test_split(data_dummy, test_size = 0.34)
  
#build the model
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('converted', axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
8/42:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
np.random.seed(4684)
  
#split into train and test to avoid overfitting
train, test = train_test_split(data_dummy, test_size = 0.34)
  
#build the model
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('converted', axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,0].round(), labels=[0, 1]))
)
8/43:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
np.random.seed(4684)
  
#split into train and test to avoid overfitting
train, test = train_test_split(data_dummy, test_size = 0.34)
  
#build the model
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('converted', axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
8/44: rf.oob_decision_function_[:,1]
8/45: rf.oob_decision_function_[:,0]
8/46:
#and let's print test accuracy and confusion matrix
print(
"Test accuracy is", rf.score(test.drop('converted', axis=1),test['converted']), 
"\n", 
"Test Set Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(test['converted'], rf.predict(test.drop('converted', axis=1)), labels=[0, 1]))
)
8/47:
feat_importances = pandas.Series(rf.feature_importances_, index=train.drop('converted', axis=1).columns)
feat_importances.sort_values().plot(kind='barh')
plt.show()
8/48: feat_importance.head()
8/49: feat_importance
8/50: feat_importances.head()
8/51: rf.feature_importances_.head()
8/52: rf.feature_importances_
8/53:
feat_importances = pandas.Series(rf.feature_importances_, index=train.drop('converted', axis=1).columns)
feat_importances.sort().plot(kind='barh')
plt.show()
8/54:
feat_importances = pandas.Series(rf.feature_importances_, index=train.drop('converted', axis=1).columns)
feat_importances.sort_values().plot(kind='barh')
plt.show()
8/55:
#build the model without total_pages_visited
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True, class_weight={0:1, 1:10})
rf.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
8/56:
print(
"Test accuracy is", rf.score(test.drop(['converted', 'total_pages_visited'], axis=1),test['converted']), 
"\n", 
"Test Set Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(test['converted'], rf.predict(test.drop(['converted', 'total_pages_visited'], axis=1)), labels=[0, 1]))
)
8/57:
#build the model without total_pages_visited
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True, class_weight='balanced')
rf.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
8/58:
print(
"Test accuracy is", rf.score(test.drop(['converted', 'total_pages_visited'], axis=1),test['converted']), 
"\n", 
"Test Set Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(test['converted'], rf.predict(test.drop(['converted', 'total_pages_visited'], axis=1)), labels=[0, 1]))
)
8/59:
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True, class_weight={0:1, 1:10})
rf.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
8/60:
#and let's print test accuracy and confusion matrix
print(
"Test accuracy is", rf.score(test.drop(['converted', 'total_pages_visited'], axis=1),test['converted']), 
"\n", 
"Test Set Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(test['converted'], rf.predict(test.drop(['converted', 'total_pages_visited'], axis=1)), labels=[0, 1]))
)
8/61:
feat_importances = pandas.Series(rf.feature_importances_, index=train.drop(['converted', 'total_pages_visited'], axis=1).columns)
feat_importances.sort_values().plot(kind='barh')
plt.show()
8/62:
from pdpbox import pdp, info_plots
  
#country
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['country_Germany', 'country_UK', 'country_US'], 
                          num_grid_points=50)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Country')
plt.show()
8/63:
from pdpbox import pdp, info_plots
  
#country
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['country_Germany', 'country_UK', 'country_US'], 
                          num_grid_points=50)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Country')
plt.show()
8/64:
from pdpbox import pdp, info_plots
  
#country
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['country_Germany', 'country_UK', 'country_US'], 
                          num_grid_points=50)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Country')
plt.show()
8/65:
from pdpbox import pdp, info_plots
  
#country
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['country_Germany', 'country_UK', 'country_US'], 
                          num_grid_points=50)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values().plot(kind='bar', title='Country')
plt.show()
8/66:
from pdpbox import pdp, info_plots
  
#country
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['country_Germany', 'country_UK', 'country_US'], 
                          num_grid_points=50)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Country')
plt.show()
8/67:
from pdpbox import pdp, info_plots
  
#country
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['country_Germany', 'country_UK', 'country_US'])
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Country')
plt.show()
8/68:
from pdpbox import pdp, info_plots
  
#country
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['country_Germany', 'country_UK', 'country_US'], 
                          num_grid_points=50)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Country')
plt.show()
8/69: pdp_iso.pdp.head()
8/70: pdp_iso.pdp
8/71:
from pdpbox import pdp, info_plots
  
#country
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['country_Germany', 'country_UK', 'country_US'], 
                          num_grid_points=10)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Country')
plt.show()
8/72: pdp_iso.pdp
8/73:
#source
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['source_Direct', 'source_Seo'], 
                          num_grid_points=50)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Source')
plt.show()
 9/1:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()
  
s = Source.from_file("tree_conversion.dot")
s.view()
 9/2:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()
  
s = Source.from_file("tree_conversion.dot")
s.view()
 9/3:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()
  
s = Source.from_file("tree_conversion.dot")
s.view()
 9/4:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()

s = Source.from_file("tree_conversion.dot")
s.view()
 9/5:
import pandas
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1KvxyyF3QCtvIx0J7_8iWDEtFQpLgd0Yq")
  
print(data.head())
 9/6: print(data.describe())
 9/7: print(data.groupby(['country'])['converted'])
 9/8:
import matplotlib.pyplot as plt
%matplotlib inline
#from matplotlib import rcParams
#rcParams.update({'figure.autolayout': True})
data.groupby(['country'])['converted'].mean().plot.bar()
plt.show()
 9/9:
data.groupby(['total_pages_visited'])['converted'].mean().plot()
plt.show()
9/10: data_dummy = pandas.get_dummies(data, drop_first=True)
9/11:
import numpy as np
from sklearn.ensemble import RandomForestClassifier
9/12:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
np.random.seed(4684)
  
#split into train and test to avoid overfitting
train, test = train_test_split(data_dummy, test_size = 0.34)
  
#build the model
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('converted', axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
9/13: #rf.oob_decision_function_[:,0]
9/14:
#and let's print test accuracy and confusion matrix
print(
"Test accuracy is", rf.score(test.drop('converted', axis=1),test['converted']), 
"\n", 
"Test Set Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(test['converted'], rf.predict(test.drop('converted', axis=1)), labels=[0, 1]))
)
9/15:
feat_importances = pandas.Series(rf.feature_importances_, index=train.drop('converted', axis=1).columns)
feat_importances.sort_values().plot(kind='barh')
plt.show()
9/16:
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True, class_weight={0:1, 1:10})
rf.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
9/17:
#and let's print test accuracy and confusion matrix
print(
"Test accuracy is", rf.score(test.drop(['converted', 'total_pages_visited'], axis=1),test['converted']), 
"\n", 
"Test Set Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(test['converted'], rf.predict(test.drop(['converted', 'total_pages_visited'], axis=1)), labels=[0, 1]))
)
9/18:
feat_importances = pandas.Series(rf.feature_importances_, index=train.drop(['converted', 'total_pages_visited'], axis=1).columns)
feat_importances.sort_values().plot(kind='barh')
plt.show()
9/19:
from pdpbox import pdp, info_plots
  
#country
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['country_Germany', 'country_UK', 'country_US'], 
                          num_grid_points=50)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Country')
plt.show()
9/20:
#source
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['source_Direct', 'source_Seo'], 
                          num_grid_points=50)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Source')
plt.show()
9/21:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()

s = Source.from_file("tree_conversion.dot")
s.view()
10/1:
import pandas
import numpy as np
pandas.set_option('display.max_columns', 10)
pandas.set_option('display.width', 350)
  
#read from google drive
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1_5RXUSwsvEjh0_DelLhHa8znq_Q4_7uc")
  
print(data.head())
10/2:
from datatime import datatime

#make them a data
data['join_date'] = pandas.to_datetime(data['join_date'])
10/3:
from dateime import datetime

#make them a data
data['join_date'] = pandas.to_datetime(data['join_date'])
10/4:
from datetime import datetime

#make them a data
data['join_date'] = pandas.to_datetime(data['join_date'])
10/5: data.head()
10/6:
from datetime import datetime
  
#make them a date
data['join_date'] = pandas.to_datetime(data['join_date']) 
data['quit_date'] = pandas.to_datetime(data['quit_date']) 
  
#everything seems to make sense, some simple plots would help double check that
data.describe(include="all")
10/7:
import pandas
import numpy as np
pandas.set_option('display.max_columns', 10)
pandas.set_option('display.width', 350)
  
#read from google drive
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1_5RXUSwsvEjh0_DelLhHa8znq_Q4_7uc")
  
print(data.head())
10/8:
import pandas
import numpy as np
pandas.set_option('display.max_columns', 10)
pandas.set_option('display.width', 350)
  
#read from google drive
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1_5RXUSwsvEjh0_DelLhHa8znq_Q4_7uc")
  
print(data.head())
10/9:
import pandas
import numpy as np
pandas.set_option('display.max_columns', 10)
pandas.set_option('display.width', 350)
  
#read from google drive
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1_5RXUSwsvEjh0_DelLhHa8znq_Q4_7uc")
  
print(data.head())
10/10:
from datetime import datetime

#make them a data
data['join_date'] = pandas.to_datetime(data['join_date'])
data['quit_date'] = pandas.to_datetime(data['quit_date']) 
  
#everything seems to make sense, some simple plots would help double check that
data.describe(include="all")
10/11:
from datetime import datetime

#make them a data
data['join_date'] = pandas.to_datetime(data['join_date'])
data['quit_date'] = pandas.to_datetime(data['quit_date']) 
  
#everything seems to make sense, some simple plots would help double check that
data.describe()
10/12:
from datetime import datetime

#make them a data
data['join_date'] = pandas.to_datetime(data['join_date'])
data['quit_date'] = pandas.to_datetime(data['quit_date']) 
  
#everything seems to make sense, some simple plots would help double check that
data.describe(include="all")
10/13:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011-01-24', end='2015-12-13')
unique_companies = data['company_id'].unique()

data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
10/14: data_headcount.head()
10/15: data_headcount
10/16:
array1 = ['bar','foo']
array2 = ['one','two']
index = pandas.MultdiIndex.from_product([array1, array2], names = ["date", "company_id"])
10/17:
array1 = ['bar','foo']
array2 = ['one','two']
index = pandas.MultiIndex.from_product([array1, array2], names = ["date", "company_id"])
10/18: index
10/19: index.head()
10/20: index
10/21:
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
10/22: data_headcount.head()
10/23:
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
# data_join.columns.values[0]='date'
# data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
# data_quit.columns.values[0]='date'
10/24: data_join
10/25:
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
# data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
# data_quit.columns.values[0]='date'
10/26: data_join
10/27:
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
10/28:
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
10/29:
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
10/30:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011/01/24', end='2015/12/13')
  
#create list of unique companies
unique_companies = data['company_id'].unique()
  
#cross join so I get all combinations of dates and companies. Will need it later.
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
  
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
10/31:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011/01/24', end='2015/12/13')
  
#create list of unique companies
unique_companies = data['company_id'].unique()
  
#cross join so I get all combinations of dates and companies. Will need it later.
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
  
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
10/32: data_headcount
10/33:
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
10/34: data_headcount
10/35:
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
#replace the NAs with 0
data_headcount.fillna(0, inplace=True)
10/36: data_headcount
10/37: data_headcount['cumsum_join'] = data_headcount.groupby(['company_id'])['join_count'].apply(lambda x: x.cumsum())
10/38:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011/01/24', end='2015/12/13')
  
#create list of unique companies
unique_companies = data['company_id'].unique()
  
#cross join so I get all combinations of dates and companies. Will need it later.
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
  
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
  
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
  
#replace the NAs with 0
data_headcount.fillna(0, inplace=True)
10/39:
#Now I need the sum by company_id. Data set is already ordered by date, so I can simply group by company_id and do cumsum
data_headcount['cumsum_join'] = data_headcount.groupby(['company_id'])['join_count'].apply(lambda x: x.cumsum())
10/40: data_headcount
10/41:
#Now I need the sum by company_id. Data set is already ordered by date, so I can simply group by company_id and do cumsum
data_headcount['cumsum_join'] = data_headcount.groupby(['company_id'])['join_count'].apply(lambda x: x.cumsum())
data_headcount['cumsum_quit'] = data_headcount.groupby(['company_id'])['quit_count'].apply(lambda x: x.cumsum())
10/42:
#finally, for each date I just take join_count - quit_count and I am done
data_headcount['count'] = data_headcount['cumsum_join'] - data_headcount['cumsum_quit']
data_headcount_table = data_headcount[["date", "company_id","count"]]
10/43:
#finally, for each date I just take join_count - quit_count and I am done
data_headcount['count'] = data_headcount['cumsum_join'] - data_headcount['cumsum_quit']
data_headcount_table = data_headcount[["date", "company_id","count"]]
print(data_headcount_table.query("company_id == 1").head(15))
10/44:
#Another way to do it would be with a for loop. 
#intialize empty vectors
loop_cumsum = []
loop_date = []
loop_company = []
#loop through all days
for i in unique_dates:
  # loop through all companies
   for j in unique_companies:
        # count joins until that day
        tmp_join = data[(data['join_date'] <= i) & (data['company_id'] == j)].shape[0]
        # count quits
        tmp_quit = data[(data['quit_date'] <= i) & (data['company_id'] == j)].shape[0]
        loop_cumsum.append(tmp_join - tmp_quit) 
        loop_date.append(i)
        loop_company.append(j)
data_headcount_table_loop = pandas.DataFrame({ 'date': loop_date, 'company_id': loop_company, 'count': loop_cumsum})
#let's check company 1
print(data_headcount_table_loop.query("company_id == 1").head(15))
10/45:
import matplotlib.pyplot as plt
  
#How many days were they employed? This should matter. People might get bored in the same place for too long
data['employment_length'] = (data['quit_date'] - data['join_date']).astype('timedelta64[D]')
  
#Let's plot employment length in days
plt.hist(data['employment_length'].dropna(), bins=100)
plt.show()
10/46:
import matplotlib.pyplot as plt
%matplot inline
#How many days were they employed? This should matter. People might get bored in the same place for too long
data['employment_length'] = (data['quit_date'] - data['join_date']).astype('timedelta64[D]')
  
#Let's plot employment length in days
plt.hist(data['employment_length'].dropna(), bins=100)
plt.show()
10/47:
import matplotlib.pyplot as plt
%matplotlib inline
#How many days were they employed? This should matter. People might get bored in the same place for too long
data['employment_length'] = (data['quit_date'] - data['join_date']).astype('timedelta64[D]')
  
#Let's plot employment length in days
plt.hist(data['employment_length'].dropna(), bins=100)
plt.show()
10/48: data['employment_length']
9/22:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()

s = Source.from_file("tree_conversion.dot")
s.view()
9/23:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()

s = Source.from_file("tree_conversion.dot")
s.view()
9/24:
s = Source.from_file("tree_conversion.dot")
s.view()
9/25:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()

s = Source.from_file("tree_conversion.dot")
s.view()
9/26:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()

s = Source.from_file("tree_conversion.dot")
s.view()
9/27:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()

s = Source.from_file("tree_conversion.dot")
s.view()
11/1:
import pandas
import numpy as np
pandas.set_option('display.max_columns', 10)
pandas.set_option('display.width', 350)
  
#read from google drive
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1_5RXUSwsvEjh0_DelLhHa8znq_Q4_7uc")
  
print(data.head())
11/2:
from datetime import datetime
  
#make them a date
data['join_date'] = pandas.to_datetime(data['join_date']) 
data['quit_date'] = pandas.to_datetime(data['quit_date']) 
  
#everything seems to make sense, some simple plots would help double check that
data.describe(include="all")
11/3:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011-01-24', end='2015-12-13')
unique_companies = data['company_id'].unique()

data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
11/4:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011/01/24', end='2015/12/13')
  
#create list of unique companies
unique_companies = data['company_id'].unique()
  
#cross join so I get all combinations of dates and companies. Will need it later.
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
  
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
  
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
  
#replace the NAs with 0
data_headcount.fillna(0, inplace=True)
  
#Now I need the sum by company_id. Data set is already ordered by date, so I can simply group by company_id and do cumsum
data_headcount['cumsum_join'] = data_headcount.groupby(['company_id'])['join_count'].apply(lambda x: x.cumsum())
data_headcount['cumsum_quit'] = data_headcount.groupby(['company_id'])['quit_count'].apply(lambda x: x.cumsum())
  
#finally, for each date I just take join_count - quit_count and I am done
data_headcount['count'] = data_headcount['cumsum_join'] - data_headcount['cumsum_quit']
data_headcount_table = data_headcount[["date", "company_id","count"]]
  
#let's check company 1
print(data_headcount_table.query("company_id == 1").head(15))
11/5:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011/01/24', end='2015/12/13')
  
#create list of unique companies
unique_companies = data['company_id'].unique()
  
#cross join so I get all combinations of dates and companies. Will need it later.
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
  
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
  
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
  
#replace the NAs with 0
data_headcount.fillna(0, inplace=True)
11/6:
#Now I need the sum by company_id. Data set is already ordered by date, so I can simply group by company_id and do cumsum
data_headcount['cumsum_join'] = data_headcount.groupby(['company_id'])['join_count'].apply(lambda x: x.cumsum())
data_headcount['cumsum_quit'] = data_headcount.groupby(['company_id'])['quit_count'].apply(lambda x: x.cumsum())
9/28:
import pandas
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1KvxyyF3QCtvIx0J7_8iWDEtFQpLgd0Yq")
  
print(data.head())
9/29: print(data.describe())
9/30: print(data.groupby(['country'])['converted'])
9/31:
import matplotlib.pyplot as plt
%matplotlib inline
#from matplotlib import rcParams
#rcParams.update({'figure.autolayout': True})
data.groupby(['country'])['converted'].mean().plot.bar()
plt.show()
9/32:
data.groupby(['total_pages_visited'])['converted'].mean().plot()
plt.show()
9/33: data_dummy = pandas.get_dummies(data, drop_first=True)
9/34:
import numpy as np
from sklearn.ensemble import RandomForestClassifier
9/35:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
np.random.seed(4684)
  
#split into train and test to avoid overfitting
train, test = train_test_split(data_dummy, test_size = 0.34)
  
#build the model
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('converted', axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
9/36: #rf.oob_decision_function_[:,0]
9/37:
#and let's print test accuracy and confusion matrix
print(
"Test accuracy is", rf.score(test.drop('converted', axis=1),test['converted']), 
"\n", 
"Test Set Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(test['converted'], rf.predict(test.drop('converted', axis=1)), labels=[0, 1]))
)
9/38:
feat_importances = pandas.Series(rf.feature_importances_, index=train.drop('converted', axis=1).columns)
feat_importances.sort_values().plot(kind='barh')
plt.show()
9/39:
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True, class_weight={0:1, 1:10})
rf.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['converted'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
9/40:
#and let's print test accuracy and confusion matrix
print(
"Test accuracy is", rf.score(test.drop(['converted', 'total_pages_visited'], axis=1),test['converted']), 
"\n", 
"Test Set Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(test['converted'], rf.predict(test.drop(['converted', 'total_pages_visited'], axis=1)), labels=[0, 1]))
)
9/41:
feat_importances = pandas.Series(rf.feature_importances_, index=train.drop(['converted', 'total_pages_visited'], axis=1).columns)
feat_importances.sort_values().plot(kind='barh')
plt.show()
9/42:
from pdpbox import pdp, info_plots
  
#country
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['country_Germany', 'country_UK', 'country_US'], 
                          num_grid_points=50)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Country')
plt.show()
9/43:
#source
pdp_iso = pdp.pdp_isolate( model=rf, 
                          dataset=train.drop(['converted', 'total_pages_visited'], axis=1),      
                          model_features=list(train.drop(['converted', 'total_pages_visited'], axis=1)), 
                          feature=['source_Direct', 'source_Seo'], 
                          num_grid_points=50)
pdp_dataset = pandas.Series(pdp_iso.pdp, index=pdp_iso.display_columns)
pdp_dataset.sort_values(ascending=False).plot(kind='bar', title='Source')
plt.show()
9/44:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()

s = Source.from_file("tree_conversion.dot")
s.view()
9/45:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
tree = DecisionTreeClassifier( max_depth=2,class_weight={0:1, 1:10}, min_impurity_decrease = 0.001)
tree.fit(train.drop(['converted', 'total_pages_visited'], axis=1), train['converted'])
  
#visualize it
export_graphviz(tree, out_file="tree_conversion.dot", feature_names=train.drop(['converted', 'total_pages_visited'], axis=1).columns, proportion=True, rotate=True)
with open("tree_conversion.dot") as f:
    dot_graph = f.read()

s = Source.from_file("tree_conversion.dot")
s.view()
9/46:
s = Source.from_file("tree_conversion.dot")
s.view()
11/7:
import matplotlib.pyplot as plt
%matplotlib inline
#How many days were they employed? This should matter. People might get bored in the same place for too long
data['employment_length'] = (data['quit_date'] - data['join_date']).astype('timedelta64[1,D]')
  
#Let's plot employment length in days
plt.hist(data['employment_length'].dropna(), bins=100)
plt.show()
11/8:
import matplotlib.pyplot as plt
%matplotlib inline
#How many days were they employed? This should matter. People might get bored in the same place for too long
data['employment_length'] = (data['quit_date'] - data['join_date']).astype('timedelta64[D]')
  
#Let's plot employment length in days
plt.hist(data['employment_length'].dropna(), bins=100)
plt.show()
11/9:
#Another way to do it would be with a for loop. 
#intialize empty vectors
loop_cumsum = []
loop_date = []
loop_company = []
#loop through all days
for i in unique_dates:
  # loop through all companies
   for j in unique_companies:
        # count joins until that day
        tmp_join = data[(data['join_date'] <= i) & (data['company_id'] == j)].shape[0]
        # count quits
        tmp_quit = data[(data['quit_date'] <= i) & (data['company_id'] == j)].shape[0]
        loop_cumsum.append(tmp_join - tmp_quit) 
        loop_date.append(i)
        loop_company.append(j)
data_headcount_table_loop = pandas.DataFrame({ 'date': loop_date, 'company_id': loop_company, 'count': loop_cumsum})
#let's check company 1
print(data_headcount_table_loop.query("company_id == 1").head(15))
12/1:
import pandas
import numpy as np
pandas.set_option('display.max_columns', 10)
pandas.set_option('display.width', 350)
  
#read from google drive
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1_5RXUSwsvEjh0_DelLhHa8znq_Q4_7uc")
  
print(data.head())
12/2:
from datetime import datetime
  
#make them a date
data['join_date'] = pandas.to_datetime(data['join_date']) 
data['quit_date'] = pandas.to_datetime(data['quit_date']) 
  
#everything seems to make sense, some simple plots would help double check that
data.describe(include="all")
12/3:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011-01-24', end='2015-12-13')
unique_companies = data['company_id'].unique()

data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
12/4:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011/01/24', end='2015/12/13')
  
#create list of unique companies
unique_companies = data['company_id'].unique()
  
#cross join so I get all combinations of dates and companies. Will need it later.
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
  
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
  
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
  
#replace the NAs with 0
data_headcount.fillna(0, inplace=True)
  
#Now I need the sum by company_id. Data set is already ordered by date, so I can simply group by company_id and do cumsum
data_headcount['cumsum_join'] = data_headcount.groupby(['company_id'])['join_count'].apply(lambda x: x.cumsum())
data_headcount['cumsum_quit'] = data_headcount.groupby(['company_id'])['quit_count'].apply(lambda x: x.cumsum())
  
#finally, for each date I just take join_count - quit_count and I am done
data_headcount['count'] = data_headcount['cumsum_join'] - data_headcount['cumsum_quit']
data_headcount_table = data_headcount[["date", "company_id","count"]]
  
#let's check company 1
print(data_headcount_table.query("company_id == 1").head(15))
12/5:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011/01/24', end='2015/12/13')
  
#create list of unique companies
unique_companies = data['company_id'].unique()
  
#cross join so I get all combinations of dates and companies. Will need it later.
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
  
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
  
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
  
#replace the NAs with 0
data_headcount.fillna(0, inplace=True)
12/6:
#Now I need the sum by company_id. Data set is already ordered by date, so I can simply group by company_id and do cumsum
data_headcount['cumsum_join'] = data_headcount.groupby(['company_id'])['join_count'].apply(lambda x: x.cumsum())
data_headcount['cumsum_quit'] = data_headcount.groupby(['company_id'])['quit_count'].apply(lambda x: x.cumsum())
12/7:
#finally, for each date I just take join_count - quit_count and I am done
data_headcount['count'] = data_headcount['cumsum_join'] - data_headcount['cumsum_quit']
data_headcount_table = data_headcount[["date", "company_id","count"]]
print(data_headcount_table.query("company_id == 1").head(15))
12/8:
#Another way to do it would be with a for loop. 
#intialize empty vectors
loop_cumsum = []
loop_date = []
loop_company = []
#loop through all days
for i in unique_dates:
  # loop through all companies
   for j in unique_companies:
        # count joins until that day
        tmp_join = data[(data['join_date'] <= i) & (data['company_id'] == j)].shape[0]
        # count quits
        tmp_quit = data[(data['quit_date'] <= i) & (data['company_id'] == j)].shape[0]
        loop_cumsum.append(tmp_join - tmp_quit) 
        loop_date.append(i)
        loop_company.append(j)
data_headcount_table_loop = pandas.DataFrame({ 'date': loop_date, 'company_id': loop_company, 'count': loop_cumsum})
#let's check company 1
print(data_headcount_table_loop.query("company_id == 1").head(15))
12/9: data_headcount_table_loop[data_headcount_table_loop['company_id'] == 1]
12/10: data_headcount_table_loop[data_headcount_table_loop['company_id'] == 1]
12/11:
import matplotlib.pyplot as plt
%matplotlib inline
#How many days were they employed? This should matter. People might get bored in the same place for too long
data['employment_length'] = (data['quit_date'] - data['join_date']).astype('timedelta64[D]')
  
#Let's plot employment length in days
plt.hist(data['employment_length'].dropna(), bins=100)
plt.show()
12/12: data['join_date']
12/13: data['join_date']
12/14: datetime.strptime("2015-12-13", "%Y/%m/%d")
12/15: datetime.strptime("2015-12-13", "%Y-%m-%d")
12/16: datetime.strptime("2015-12-13", "%Y/%m/%d")
12/17: datetime.strptime("2015/12/13", "%Y/%m/%d")
12/18: timedelta(days=(365+31)
12/19: timedelta(days=(365+31))
12/20: data['join_date'] < datetime.strptime("2015-12-13", "%Y/%m/%d")
12/21: data['join_date'] < datetime.strptime("2015-12-13", "%Y/%m/%d")- timedelta(days=(365+31))
12/22: data['join_date'] < datetime.strptime("2015-12-13", "%Y-%m-%d")
12/23: data['join_date'] < datetime.strptime("2015-12-13", "%Y-%m-%d")- timedelta(days=(365+31))
12/24:
from datetime import timedelta
#Create binary class
data=data[data['join_date'] < datetime.strptime("2015-12-13", "%Y/%m/%d") - timedelta(days=(365+31))]
data['early_quitter'] = np.where((data['employment_length']>396) | (np.isnan(data['employment_length'])), 0, 1)
12/25:
from datetime import timedelta
#Create binary class
data=data[data['join_date'] < datetime.strptime("2015/12/13", "%Y/%m/%d") - timedelta(days=(365+31))]
data['early_quitter'] = np.where((data['employment_length']>396) | (np.isnan(data['employment_length'])), 0, 1)
12/26: data['join_date'] < datetime.strptime("2015/12/13", "%Y/%m/%d")- timedelta(days=(365+31))
12/27: data['join_date'] < datetime.strptime("2015-12-13", "%Y-%m-%d")- timedelta(days=(365+31))
12/28: data['join_date'] - timedelta(days=(365+31))
12/29: data=data[data['join_date'] < datetime.strptime("2015/12/13", "%Y/%m/%d") - timedelta(days=(365+31))]
12/30: data
12/31: a = np.array([1,2,3,4])
12/32: np.where(a>2,0,1)
12/33: a = np.array([1,2,3,4,0,NA])
12/34: a = np.array([1,2,3,4,0,NAN])
12/35: a = np.array([1,2,3,4,0,nan])
12/36: a = np.array([1,2,3,4,0,'nan'])
12/37: a
12/38: a = np.array([1,2,3,4,0,np.nan])
12/39: a
12/40: np.where(a>2|np.isnan(a),0,1)
12/41: np.where(a>2,0,1)
12/42: a = np.array([1,2,3,4,0])
12/43: np.where(a>2,0,1)
12/44:
from datetime import timedelta
#Create binary class
data=data[data['join_date'] < datetime.strptime("2015/12/13", "%Y/%m/%d") - timedelta(days=(365+31))]
data['early_quitter'] = np.where((data['employment_length']>396) | (np.isnan(data['employment_length'])), 0, 1)
12/45: data
13/1: a = np.array([0,1,2,3,4])
13/2:
import pandas
import numpy as np
pandas.set_option('display.max_columns', 10)
pandas.set_option('display.width', 350)
  
#read from google drive
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1_5RXUSwsvEjh0_DelLhHa8znq_Q4_7uc")
  
print(data.head())
13/3: a = np.array([0,1,2,3,4])
13/4: np.where(a>2|np.isnan(a),0,1)
13/5: a = np.array([np.nan, 0,1,2,3,4])
13/6: np.where(a>2|np.isnan(a),0,1)
13/7: np.where(np.isnan(a),0,1)
13/8: np.where(a>2,0,1)
13/9: a = np.array([0,1,2,3,4])
13/10: np.where(a>2,0,1)
13/11:
from datetime import datetime
  
#make them a date
data['join_date'] = pandas.to_datetime(data['join_date']) 
data['quit_date'] = pandas.to_datetime(data['quit_date']) 
  
#everything seems to make sense, some simple plots would help double check that
data.describe(include="all")
13/12:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011-01-24', end='2015-12-13')
unique_companies = data['company_id'].unique()

data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
13/13:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011/01/24', end='2015/12/13')
  
#create list of unique companies
unique_companies = data['company_id'].unique()
  
#cross join so I get all combinations of dates and companies. Will need it later.
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
  
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
  
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
  
#replace the NAs with 0
data_headcount.fillna(0, inplace=True)
  
#Now I need the sum by company_id. Data set is already ordered by date, so I can simply group by company_id and do cumsum
data_headcount['cumsum_join'] = data_headcount.groupby(['company_id'])['join_count'].apply(lambda x: x.cumsum())
data_headcount['cumsum_quit'] = data_headcount.groupby(['company_id'])['quit_count'].apply(lambda x: x.cumsum())
  
#finally, for each date I just take join_count - quit_count and I am done
data_headcount['count'] = data_headcount['cumsum_join'] - data_headcount['cumsum_quit']
data_headcount_table = data_headcount[["date", "company_id","count"]]
  
#let's check company 1
print(data_headcount_table.query("company_id == 1").head(15))
13/14:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011/01/24', end='2015/12/13')
  
#create list of unique companies
unique_companies = data['company_id'].unique()
  
#cross join so I get all combinations of dates and companies. Will need it later.
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
  
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
  
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
  
#replace the NAs with 0
data_headcount.fillna(0, inplace=True)
13/15:
#Now I need the sum by company_id. Data set is already ordered by date, so I can simply group by company_id and do cumsum
data_headcount['cumsum_join'] = data_headcount.groupby(['company_id'])['join_count'].apply(lambda x: x.cumsum())
data_headcount['cumsum_quit'] = data_headcount.groupby(['company_id'])['quit_count'].apply(lambda x: x.cumsum())
13/16:
#finally, for each date I just take join_count - quit_count and I am done
data_headcount['count'] = data_headcount['cumsum_join'] - data_headcount['cumsum_quit']
data_headcount_table = data_headcount[["date", "company_id","count"]]
print(data_headcount_table.query("company_id == 1").head(15))
13/17:
#Another way to do it would be with a for loop. 
#intialize empty vectors
loop_cumsum = []
loop_date = []
loop_company = []
#loop through all days
for i in unique_dates:
  # loop through all companies
   for j in unique_companies:
        # count joins until that day
        tmp_join = data[(data['join_date'] <= i) & (data['company_id'] == j)].shape[0]
        # count quits
        tmp_quit = data[(data['quit_date'] <= i) & (data['company_id'] == j)].shape[0]
        loop_cumsum.append(tmp_join - tmp_quit) 
        loop_date.append(i)
        loop_company.append(j)
data_headcount_table_loop = pandas.DataFrame({ 'date': loop_date, 'company_id': loop_company, 'count': loop_cumsum})
#let's check company 1
print(data_headcount_table_loop.query("company_id == 1").head(15))
13/18:
import matplotlib.pyplot as plt
%matplotlib inline
#How many days were they employed? This should matter. People might get bored in the same place for too long
data['employment_length'] = (data['quit_date'] - data['join_date']).astype('timedelta64[D]')
  
#Let's plot employment length in days
plt.hist(data['employment_length'].dropna(), bins=100)
plt.show()
13/19:
from datetime import timedelta
#Create binary class
data=data[data['join_date'] < datetime.strptime("2015/12/13", "%Y/%m/%d") - timedelta(days=(365+31))]
data['early_quitter'] = np.where((data['employment_length']>396) | (np.isnan(data['employment_length'])), 0, 1)
13/20: data
13/21:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
data_dummy = pandas.get_dummies(data[['company_id', 'dept', 'seniority', 'salary', 'early_quitter']], drop_first=True)           
  
#we are not too interested in predictive power, we are mainly using the tree as a descriptive stat tool
tree = DecisionTreeClassifier( max_depth=4, min_samples_leaf = 30, class_weight="balanced", min_impurity_decrease = 0.001)
tree.fit(data_dummy.drop('early_quitter', axis=1), data['early_quitter'])
  
#visualize it
export_graphviz(tree, out_file="tree_employee.dot", feature_names=data_dummy.drop('early_quitter', axis=1).columns, proportion=True, rotate=True)
with open("tree_employee.dot") as f:
    dot_graph = f.read()
s = Source.from_file("tree_employee.dot")
s.view()
13/22:
import os
os.environ["PATH"] += os.pathsep + 'D:/Program Files (x86)/Graphviz2.38/bin/'
13/23:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
data_dummy = pandas.get_dummies(data[['company_id', 'dept', 'seniority', 'salary', 'early_quitter']], drop_first=True)           
  
#we are not too interested in predictive power, we are mainly using the tree as a descriptive stat tool
tree = DecisionTreeClassifier( max_depth=4, min_samples_leaf = 30, class_weight="balanced", min_impurity_decrease = 0.001)
tree.fit(data_dummy.drop('early_quitter', axis=1), data['early_quitter'])
  
#visualize it
export_graphviz(tree, out_file="tree_employee.dot", feature_names=data_dummy.drop('early_quitter', axis=1).columns, proportion=True, rotate=True)
with open("tree_employee.dot") as f:
    dot_graph = f.read()
s = Source.from_file("tree_employee.dot")
s.view()
13/24:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
data_dummy = pandas.get_dummies(data[['company_id', 'dept', 'seniority', 'salary', 'early_quitter']], drop_first=True)           
  
#we are not too interested in predictive power, we are mainly using the tree as a descriptive stat tool
tree = DecisionTreeClassifier( max_depth=4, min_samples_leaf = 30, class_weight="balanced", min_impurity_decrease = 0.001)
tree.fit(data_dummy.drop('early_quitter', axis=1), data['early_quitter'])
  
#visualize it
export_graphviz(tree, out_file="tree_employee.dot", feature_names=data_dummy.drop('early_quitter', axis=1).columns, proportion=True, rotate=True)
with open("tree_employee.dot") as f:
    dot_graph = f.read()
s = Source.from_file("tree_employee.dot")
s.view()
13/25: s.view()
13/26:
import pandas
import numpy as np
pandas.set_option('display.max_columns', 10)
pandas.set_option('display.width', 350)
  
#read from google drive
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1_5RXUSwsvEjh0_DelLhHa8znq_Q4_7uc")
  
print(data.head())
13/27:
from datetime import datetime
  
#make them a date
data['join_date'] = pandas.to_datetime(data['join_date']) 
data['quit_date'] = pandas.to_datetime(data['quit_date']) 
  
#everything seems to make sense, some simple plots would help double check that
data.describe(include="all")
13/28:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011-01-24', end='2015-12-13')
unique_companies = data['company_id'].unique()

data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
13/29:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011/01/24', end='2015/12/13')
  
#create list of unique companies
unique_companies = data['company_id'].unique()
  
#cross join so I get all combinations of dates and companies. Will need it later.
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
  
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
  
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
  
#replace the NAs with 0
data_headcount.fillna(0, inplace=True)
  
#Now I need the sum by company_id. Data set is already ordered by date, so I can simply group by company_id and do cumsum
data_headcount['cumsum_join'] = data_headcount.groupby(['company_id'])['join_count'].apply(lambda x: x.cumsum())
data_headcount['cumsum_quit'] = data_headcount.groupby(['company_id'])['quit_count'].apply(lambda x: x.cumsum())
  
#finally, for each date I just take join_count - quit_count and I am done
data_headcount['count'] = data_headcount['cumsum_join'] - data_headcount['cumsum_quit']
data_headcount_table = data_headcount[["date", "company_id","count"]]
  
#let's check company 1
print(data_headcount_table.query("company_id == 1").head(15))
13/30:
#create list of unique dates for the tables
unique_dates = pandas.date_range(start='2011/01/24', end='2015/12/13')
  
#create list of unique companies
unique_companies = data['company_id'].unique()
  
#cross join so I get all combinations of dates and companies. Will need it later.
data_headcount = pandas.MultiIndex.from_product([unique_dates, unique_companies], names = ["date", "company_id"])
data_headcount = pandas.DataFrame(index = data_headcount).reset_index()
  
#now I get for each day/company, how many people quit/got hired on that day
data_join = data.groupby(['join_date', 'company_id']).size().reset_index(name='join_count')
data_join.columns.values[0]='date'
data_quit = data.groupby(['quit_date', 'company_id']).size().reset_index(name='quit_count')
data_quit.columns.values[0]='date'
  
#Now I left outer join with data_headcount. 
#NA means no people were hired/quit on that day cause there is no match.
data_headcount = pandas.merge(data_headcount, data_join, on=["date", "company_id"], how='left')
data_headcount = pandas.merge(data_headcount, data_quit, on=["date", "company_id"], how='left')
  
#replace the NAs with 0
data_headcount.fillna(0, inplace=True)
13/31:
#Now I need the sum by company_id. Data set is already ordered by date, so I can simply group by company_id and do cumsum
data_headcount['cumsum_join'] = data_headcount.groupby(['company_id'])['join_count'].apply(lambda x: x.cumsum())
data_headcount['cumsum_quit'] = data_headcount.groupby(['company_id'])['quit_count'].apply(lambda x: x.cumsum())
13/32:
#finally, for each date I just take join_count - quit_count and I am done
data_headcount['count'] = data_headcount['cumsum_join'] - data_headcount['cumsum_quit']
data_headcount_table = data_headcount[["date", "company_id","count"]]
print(data_headcount_table.query("company_id == 1").head(15))
13/33:
#Another way to do it would be with a for loop. 
#intialize empty vectors
loop_cumsum = []
loop_date = []
loop_company = []
#loop through all days
for i in unique_dates:
  # loop through all companies
   for j in unique_companies:
        # count joins until that day
        tmp_join = data[(data['join_date'] <= i) & (data['company_id'] == j)].shape[0]
        # count quits
        tmp_quit = data[(data['quit_date'] <= i) & (data['company_id'] == j)].shape[0]
        loop_cumsum.append(tmp_join - tmp_quit) 
        loop_date.append(i)
        loop_company.append(j)
data_headcount_table_loop = pandas.DataFrame({ 'date': loop_date, 'company_id': loop_company, 'count': loop_cumsum})
#let's check company 1
print(data_headcount_table_loop.query("company_id == 1").head(15))
13/34:
import matplotlib.pyplot as plt
%matplotlib inline
#How many days were they employed? This should matter. People might get bored in the same place for too long
data['employment_length'] = (data['quit_date'] - data['join_date']).astype('timedelta64[D]')
  
#Let's plot employment length in days
plt.hist(data['employment_length'].dropna(), bins=100)
plt.show()
13/35:
from datetime import timedelta
#Create binary class
data=data[data['join_date'] < datetime.strptime("2015/12/13", "%Y/%m/%d") - timedelta(days=(365+31))]
data['early_quitter'] = np.where((data['employment_length']>396) | (np.isnan(data['employment_length'])), 0, 1)
13/36: data
13/37:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
  
data_dummy = pandas.get_dummies(data[['company_id', 'dept', 'seniority', 'salary', 'early_quitter']], drop_first=True)           
  
#we are not too interested in predictive power, we are mainly using the tree as a descriptive stat tool
tree = DecisionTreeClassifier( max_depth=4, min_samples_leaf = 30, class_weight="balanced", min_impurity_decrease = 0.001)
tree.fit(data_dummy.drop('early_quitter', axis=1), data['early_quitter'])
  
#visualize it
export_graphviz(tree, out_file="tree_employee.dot", feature_names=data_dummy.drop('early_quitter', axis=1).columns, proportion=True, rotate=True)
with open("tree_employee.dot") as f:
    dot_graph = f.read()
s = Source.from_file("tree_employee.dot")
s.view()
13/38: pandas.qcut(np.arange(10),q=10)
13/39: pandas.qcut(np.arange(10),q=9)
13/40: pandas.qcut(np.arange(10),q=8)
13/41: pandas.qcut(np.arange(0,10),q=8)
13/42: pandas.qcut(np.arange(0,10),q=6)
13/43:
#convert salary to its percentiles. We break it into 50 bins.
data['salary_percentile'] = pandas.qcut(data['salary'],q=50, labels=False)
#take proportion of early quitters for each percentile value and plot it
data.groupby('salary_percentile')['early_quitter'].mean().plot(title="Proportion of early quitters")
plt.show()
13/44: data['salary_percentile']
13/45: a = pandas.qcut(np.arange(0,10),q=6)
13/46: a
13/47: a = pandas.qcut([1,2,3,4,5],q=2)
13/48: a
13/49: pandas.qcut(data['salary'],q=50, labels=False)
13/50: d['salary']
13/51: data['salary']
13/52: a = pandas.qcut([100,1000,100,1000,100,100],q=2)
13/53: a = pandas.qcut([100,1001,101,1020,120,110],q=2)
13/54: a
13/55: pandas.qcut(data['salary'],q=50)
13/56: pandas.qcut(data['salary'],q=50, labels=False)
13/57: a = pandas.qcut([100,1000,200,1002,102,2001],q=2,label=False)
13/58: a = pandas.qcut([100,1000,200,1002,102,2001],q=2,labels=False)
13/59: a
14/1:
import pandas
import numpy as np
pandas.set_option('display.max_columns', 20)
pandas.set_option('display.width', 350)
  
#read from google drive
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=18RLruiMU8rM-IQPLdwL6wNEc8Kks2JZQ")
#fix ip address
data['ip_address']=round(data['ip_address'])
  
data.head()
14/2:
#Read from google drive
ip_addresses=pandas.read_csv("https://drive.google.com/uc?export=download&id=1wbKys6YI-IvE-b-C0_4xR4zz2YnpOL1d")
  
ip_addresses.head()
14/3:
data_country = [0]*data.shapep[0]

for i in range(data.shape[0]):
14/4:
data_country = [None]*data.shapep[0]

for i in range(data.shape[0]):
    for j in range(ip_addresses.shape[0]):
        if data['ip_addresses'][i] in [ip_addresses['lower_bound_ip_address'][j], ip_addresses['upper_bound_ip_address'][j]]:
            data_country[i] = ip_addresses['country']
            break
data['country'] = data_country
print(data.groupby('country').size().nlargest(10))
14/5:
data_country = [None]*data.shape[0]

for i in range(data.shape[0]):
    for j in range(ip_addresses.shape[0]):
        if data['ip_addresses'][i] in [ip_addresses['lower_bound_ip_address'][j], ip_addresses['upper_bound_ip_address'][j]]:
            data_country[i] = ip_addresses['country']
            break
data['country'] = data_country
print(data.groupby('country').size().nlargest(10))
14/6:
data_country = [None]*data.shape[0]

for i in range(data.shape[0]):
    for j in range(ip_addresses.shape[0]):
        if data['ip_address'][i] in [ip_addresses['lower_bound_ip_address'][j], ip_addresses['upper_bound_ip_address'][j]]:
            data_country[i] = ip_addresses['country']
            break
data['country'] = data_country
print(data.groupby('country').size().nlargest(10))
15/1:
import pandas
import numpy as np
pandas.set_option('display.max_columns', 20)
pandas.set_option('display.width', 350)
  
#read from google drive
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=18RLruiMU8rM-IQPLdwL6wNEc8Kks2JZQ")
#fix ip address
data['ip_address']=round(data['ip_address'])
  
data.head()
15/2:
#Read from google drive
ip_addresses=pandas.read_csv("https://drive.google.com/uc?export=download&id=1wbKys6YI-IvE-b-C0_4xR4zz2YnpOL1d")
  
ip_addresses.head()
15/3:
import pandas
import numpy as np
pandas.set_option('display.max_columns', 20)
pandas.set_option('display.width', 350)
  
#read from google drive
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=18RLruiMU8rM-IQPLdwL6wNEc8Kks2JZQ")
#fix ip address
data['ip_address']=round(data['ip_address'])
  
data.head()
15/4:
data_country = [None]*data.shape[0]

for i in range(data.shape[0]):
    for j in range(ip_addresses.shape[0]):
        if data['ip_address'][i] in [ip_addresses['lower_bound_ip_address'][j], ip_addresses['upper_bound_ip_address'][j]]:
            data_country[i] = ip_addresses['country']
            print(data_country)
            break
data['country'] = data_country
print(data.groupby('country').size().nlargest(10))
15/5:
data_country = [None] * data.shape[0]
  
for i in range(data.shape[0]):
    tmp = ip_addresses[(data['ip_address'][i] >= ip_addresses['lower_bound_ip_address']) & 
                       (data['ip_address'][i] <= ip_addresses['upper_bound_ip_address'])
                      ]['country'].values
    if (len(tmp) == 1):  
        data_country[i] = tmp

data['country'] = data_country
data['country'] = data['country'].str.get(0)
  
print(data.groupby('country').size().nlargest(10))
15/6: from sklearn.ensemble import RandomForestClassifier
15/7:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
15/8: data_rg
15/9: data
15/10:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')size().transform('count')
data['ip_address_count'] = data.groupby('ip_address')size().transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country').size()
15/11:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id').size().transform('count')
data['ip_address_count'] = data.groupby('ip_address').size().transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country').size()
15/12:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country').size()
15/13: bottom_countries
15/14:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country').['country']
15/15:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country')['country']
15/16: bottom_countries
15/17: bottom_countries.head()
15/18:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country').size().sort_values(ascending=False)[50:].index
15/19: bottom_countries
15/20:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country').size()
15/21: bottom_countries
15/22:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country')['country'].transform('count')
15/23: bottom_countries
15/24:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id').size()
data['ip_address_count'] = data.groupby('ip_address').size()
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country')['country'].transform('count')
15/25: bottom_countries
15/26: data_rf
15/27:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country')['country'].transform('count')
15/28: data_rf
15/29: bottom_countries
15/30:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country').size()
15/31: bottom_countries
15/32:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country')
15/33: bottom_countries
15/34: bottom_countries.head()
15/35:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country').size()
15/36: bottom_countries.head()
15/37:
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from datetime import datetime
np.random.seed(10)

data['signup_time'] = pandas.to_datetime(data['signup_time'])
data['purchase_time'] = pandas.to_datetime(data['purchase_time'])

data['purchase_signup_diff'] = (data['purchase_time'] - data['signup_time']).dt.total_seconds()
data['device_id_count'] = data.groupby('device_id')['device_id'].transform('count')
data['ip_address_count'] = data.groupby('ip_address')['ip_address'].transform('count')
data_rf = data.drop(['user_id','signup_time','purchase_time','device_id'], axis= 1)
data_rf['country'].replace([None],'Not_found', inplace = True)

bottom_countries = data_rf.groupby('country').size().sort_values(ascending = False)[50:].index
x = dict.fromkeys(bottom_countries,'Other')
data_rf['country'] = data_rf['country'].replace(x)
15/38:
data_rf = pandas.get_dummies(data_rf, drop_first= True)
train,test = train_test_split(data_rf,test_size = 0.34)
15/39:
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('class', axis =1), train['class'])
15/40: data_rf.head()
15/41:
rf = RandomForestClassifier(n_estimators=100, max_features=3, oob_score=True)
rf.fit(train.drop('class', axis =1), train['class'])
#let's print OOB accuracy and confusion matrix
print(
"OOB accuracy is", 
rf.oob_score_, 
"\n", 
"OOB Confusion Matrix", 
"\n",
pandas.DataFrame(confusion_matrix(train['class'], rf.oob_decision_function_[:,1].round(), labels=[0, 1]))
)
15/42:
print(
    "Test accuracy is", rf.score(test.drop('class', axis =1), test['class']),
"\n",
"Test Set Confusion Matrix",
"\n",
pandas.DataFrame(confusion_matrix(test['class'], rf.predict(test.drop('class, axis=1')), labels = [0,1])))
15/43:
print(
    "Test accuracy is", rf.score(test.drop('class', axis =1), test['class']),
    "\n",
    "Test Set Confusion Matrix",
    "\n",
    pandas.DataFrame(confusion_matrix(test['class'], rf.predict(test.drop('class', axis=1)), labels = [0,1]))
)
15/44:
pred_prob = rf.predict_proba(test.drop('class', axis=1))[:,1]
pred = rf.preduct(test.drop('class'), axis =1)
opprint(np.array_equal(pred_prob.round,pred))
15/45:
pred_prob = rf.predict_proba(test.drop('class', axis=1))[:,1]
pred = rf.predict(test.drop('class'), axis =1)
opprint(np.array_equal(pred_prob.round,pred))
15/46:
pred_prob = rf.predict_proba(test.drop('class', axis=1))[:,1]
pred = rf.predict(test.drop('class'), axis =1)
print(np.array_equal(pred_prob.round,pred))
15/47:
pred_prob = rf.predict_proba(test.drop('class', axis=1))[:,1]
pred = rf.predict(test.drop('class', axis =1))
print(np.array_equal(pred_prob.round,pred))
15/48:
pred_prob = rf.predict_proba(test.drop('class', axis=1))[:,1]
pred = rf.predict(test.drop('class', axis =1))
print(np.array_equal(pred_prob.round(),pred))
15/49: pred_prob
15/50:
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt
fpr, tpr, thresholds = roc_curve(test['class'], pred_prob)
plt.plot(fpr,tpr)
plt.plot([0,1],[0,1])
plt.ylim([0,1])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC')
plt.show()
15/51:
from sklearn.metrics import roc_curve
import matplotlib.pyplot as plt
%matplotlib inline
fpr, tpr, thresholds = roc_curve(test['class'], pred_prob)
plt.plot(fpr,tpr)
plt.plot([0,1],[0,1])
plt.ylim([0,1])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC')
plt.show()
15/52: thresholds
15/53:
error_cutoff = pandas.DataFrame({'cutoff':pandas.Series(thresholds), 
                                'class0_error':pandas.Series(fpr),
                                'class1_error':1-pandas.Series(tpr)})
error_cutoff['optimal_value'] = 1 - error_cutoff['class1_errror'] - error_cutoff['class0_error']
print(error_cutoff.sort_value('optimal_value', ascending = False)).head(1)
15/54:
error_cutoff = pandas.DataFrame({'cutoff':pandas.Series(thresholds), 
                                'class0_error':pandas.Series(fpr),
                                'class1_error':1-pandas.Series(tpr)})
error_cutoff['optimal_value'] = 1 - error_cutoff['class1_error'] - error_cutoff['class0_error']
print(error_cutoff.sort_value('optimal_value', ascending = False)).head(1)
15/55:
error_cutoff = pandas.DataFrame({'cutoff':pandas.Series(thresholds), 
                                'class0_error':pandas.Series(fpr),
                                'class1_error':1-pandas.Series(tpr)})
error_cutoff['optimal_value'] = 1 - error_cutoff['class1_error'] - error_cutoff['class0_error']
print(error_cutoff.sort_value('optimal_value', ascending = False).head(1))
15/56:
error_cutoff = pandas.DataFrame({'cutoff':pandas.Series(thresholds), 
                                'class0_error':pandas.Series(fpr),
                                'class1_error':1-pandas.Series(tpr)})
error_cutoff['optimal_value'] = 1 - error_cutoff['class1_error'] - error_cutoff['class0_error']
print(error_cutoff.sort_values('optimal_value', ascending = False).head(1))
16/1:
import pandas
pandas.set_option('display.max_columns', 20)
pandas.set_option('display.width', 350)
  
#read data
user = pandas.read_csv("Translation_Test/user_table.csv")
test = pandas.read_csv("Translation_Test/test_table.csv")
  
print(user.shape)
16/2:
import pandas
pandas.set_option('display.max_columns', 20)
pandas.set_option('display.width', 350)
  
#read data
user = pandas.read_csv("Translation_Test/user_table.csv")
test = pandas.read_csv("Translation_Test/test_table.csv")
  
print(user.shape)
16/3: print(test.shape)
16/4: print(test['user_id']).nunique() == len(test('user_id'))
16/5: print(test['user_id'].nunique() == len(test['user_id']'user_id')
16/6: print(test['user_id'].nunique() == len(test['user_id'])
16/7: print(test['user_id'].nunique() == len(test['user_id']))
16/8: print(user['user_id'].nunique() == len(user['user_id']))
16/9: print(len(user[user_id]) - len(test['user_id']))
16/10: print(len(user[user_id]) - len(test['user_id']))
16/11: print(len(user['user_id']) - len(test['user_id']))
16/12: user.shape[0] - test.shape[0]
16/13: data = test.merge(user, on = ['user_id'])
16/14: data
16/15: data.head()
16/16:
data['date']  = pandas.to_datetime(data['date'])
print(data.describes(include = 'all'))
16/17:
data['date']  = pandas.to_datetime(data['date'])
print(data.describe(include = 'all'))
16/18: data = test.merge(user, on = ['user_id'])
16/19:
data['date']  = pandas.to_datetime(data['date'])
print(data.describe(include = 'all'))
16/20:
data['date']  = pandas.to_datetime(data['date'])
data.describe(include = 'all')
16/21: test.head()
16/22: user.head()
16/23:
country_conversion = data.query('test == 0').groupby('country')['conversion'].mean()
print(country_conversion.sort_values(ascending = False))
16/24:
country_conversion = data.query('test == 1').groupby('country')['conversion'].mean()
print(country_conversion.sort_values(ascending = False))
16/25:
country_conversion = data.query('test == 0').groupby('country')['conversion'].mean()
print(country_conversion.sort_values(ascending = False))
16/26:
from scipy import stats
data = data.query('country != "Spain"')
16/27:
test = stats.ttest_ind(data[data['test'] == 1]['conversion'],
                      data[data['test'] == 0]['conversion'],
                      equal_var = False)
print(data.goupby('test')['conversion'].mean())
16/28:
test = stats.ttest_ind(data[data['test'] == 1]['conversion'],
                      data[data['test'] == 0]['conversion'],
                      equal_var = False)
test
16/29:
test = stats.ttest_ind(data[data['test'] == 1]['conversion'],
                      data[data['test'] == 0]['conversion'],
                      equal_var = False)
print(data.groupby('test')['conversion'].mean())
16/30: print(test.statisitic)
16/31: print(test.statistic)
16/32: data.head()
16/33: print(data.groupby('test')['conversion'].mean())
16/34:
import pandas
pandas.set_option('display.max_columns', 20)
pandas.set_option('display.width', 350)
  
#read data
user = pandas.read_csv("Translation_Test/user_table.csv")
test = pandas.read_csv("Translation_Test/test_table.csv")
  
print(user.shape)
16/35: print(test.shape)
16/36: print(test['user_id'].nunique() == len(test['user_id']))
16/37: print(user['user_id'].nunique() == len(user['user_id']))
16/38: print(len(user['user_id']) - len(test['user_id']))
16/39: data = test.merge(user, on = ['user_id'])
16/40:
data['date']  = pandas.to_datetime(data['date'])
data.describe(include = 'all')
16/41:
country_conversion = data.query('test == 0').groupby('country')['conversion'].mean()
print(country_conversion.sort_values(ascending = False))
16/42:
from scipy import stats
data = data.query('country != "Spain"')
16/43: print(data.groupby('test')['conversion'].mean())
16/44:
test = stats.ttest_ind(data[data['test'] == 1]['conversion'],
                      data[data['test'] == 0]['conversion'],
                      equal_var = False)
print(data.groupby('test')['conversion'].mea)
16/45:
test = stats.ttest_ind(data[data['test'] == 1]['conversion'],
                      data[data['test'] == 0]['conversion'],
                      equal_var = False)
print(data.groupby('test')['conversion'].mean())
16/46: print(test.statistic)
16/47: print(test.pvalue)
16/48:
import matplotlib.pyplot as plt
data_test_by_day = data.groupby('date')['conversion'].agg({'test_vs_control': lambda x: x[data['test'] == 1].mean()/
                                                          x[data['test'] == 0].mean}).plot()
16/49:
import matplotlib.pyplot as plt
data_test_by_day = data.groupby('date')['conversion'].agg({'test_vs_control': lambda x: x[data['test'] == 1].mean()/
                                                          x[data['test'] == 0].mean()}).plot()
16/50:
import matplotlib.pyplot as plt
data_test_by_day = data.groupby('date')['conversion'].agg({'test_vs_control': lambda x: x[data['test'] == 1].mean()/
                                                          x[data['test'] == 0].mean()}).plot()
plt.show()
16/51:
data_grouped_source = data.groupby('source')['test'].agg({'frequency_test_0' : lambda x: len(x[x==0]), 
                                                          'frequency_test_1': lambda x:len(x[x==1])})
16/52: print(data_grouped_source/data_group_source.sum())
16/53: print(data_grouped_source/data_grouped_source.sum())
16/54: data_grouped_source
16/55: data_grouped_source.sum()
16/56: data_grouped_source
16/57:
import graphviz
from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import export_graphviz
from graphviz import Source
16/58:
data['date'] = date['date'].apply(str)
data_dummy = pandas.get_dummies(data)
train_cols = data.dummy.drop(['test', 'conversion'],axis = 1)
16/59:
data['date'] = data['date'].apply(str)
data_dummy = pandas.get_dummies(data)
train_cols = data.dummy.drop(['test', 'conversion'],axis = 1)
16/60:
data['date'] = data['date'].apply(str)
data_dummy = pandas.get_dummies(data)
train_cols = data_dummy.drop(['test', 'conversion'],axis = 1)
16/61:
tree = DecisionTreeClassifier(class_weight='balanced', min_impurity_decrease = 0.001)
tree.fit(train_cols, data_dummy['test'])
16/62:
tree = DecisionTreeClassifier(class_weight='balanced', min_impurity_decrease = 0.001)
tree.fit(train_cols, data_dummy['test'])
export_graphviz(tree, out_file = 'tree_test.dot', feature_names = train_cols.columns, proprotion = True, rotation = True)
with open('tree_test.dot') as f:
16/63:
tree = DecisionTreeClassifier(class_weight='balanced', min_impurity_decrease = 0.001)
tree.fit(train_cols, data_dummy['test'])
export_graphviz(tree, out_file = 'tree_test.dot', feature_names = train_cols.columns, proprotion = True, rotation = True)
with open('tree_test.dot') as f:
    dot_graph = f.read()
s = Source.from_file('tree_test.dot')
s.view()
16/64:
tree = DecisionTreeClassifier(class_weight='balanced', min_impurity_decrease = 0.001)
tree.fit(train_cols, data_dummy['test'])
export_graphviz(tree, out_file = 'tree_test.dot', feature_names = train_cols.columns, proportion = True, rotation = True)
with open('tree_test.dot') as f:
    dot_graph = f.read()
s = Source.from_file('tree_test.dot')
s.view()
16/65:
tree = DecisionTreeClassifier(class_weight='balanced', min_impurity_decrease = 0.001)
tree.fit(train_cols, data_dummy['test'])
export_graphviz(tree, out_file = 'tree_test.dot', feature_names = train_cols.columns, proportion = True, rotate = True)
with open('tree_test.dot') as f:
    dot_graph = f.read()
s = Source.from_file('tree_test.dot')
s.view()
16/66: print(data_dummy.groupby('test')[['country_Argentina', 'country_Uruguay']].mean())
16/67:
orginial_data = stats.ttest_ind(data_dummy[data['test'] == 1]['conversion'],
                               data_dummy[data['test'] == 0]['conversion'],
                               equal_val = False)
data_no_AR_UR = stats.ttest_ind(data_dummy[(data['test'] == 1) & 
                                           (data_dummy['country_Argentina'] == 0) &
                                           (data_dummy['country_Uruguay'] == 0)]['conversion'],
                               data_dummy[(data['test'] == 0) & 
                                           (data_dummy['country_Argentina'] == 0) &
                                           (data_dummy['country_Uruguay'] == 0)]['conversion'],
                               equal_val = False)
print(pandas.DataFrame({'data_type':['Full', 'Removed_Argentina_Uruguay'],
                       'p_value':[orginial_data.pvalue, data_no_AR_UR.pvalue]
                       't_statisitc':[orginial_data.statistic, data_no_AR_UR.statistic]}))
16/68:
orginial_data = stats.ttest_ind(data_dummy[data['test'] == 1]['conversion'],
                               data_dummy[data['test'] == 0]['conversion'],
                               equal_val = False)
data_no_AR_UR = stats.ttest_ind(data_dummy[(data['test'] == 1) & 
                                           (data_dummy['country_Argentina'] == 0) &
                                           (data_dummy['country_Uruguay'] == 0)]['conversion'],
                               data_dummy[(data['test'] == 0) & 
                                           (data_dummy['country_Argentina'] == 0) &
                                           (data_dummy['country_Uruguay'] == 0)]['conversion'],
                               equal_val = False)
print(pandas.DataFrame({'data_type':['Full', 'Removed_Argentina_Uruguay'],
                       'p_value':[orginial_data.pvalue, data_no_AR_UR.pvalue]
                       't_statisit':[orginial_data.statistic, data_no_AR_UR.statistic]}))
16/69:
orginial_data = stats.ttest_ind(data_dummy[data['test'] == 1]['conversion'],
                               data_dummy[data['test'] == 0]['conversion'],
                               equal_val = False)
data_no_AR_UR = stats.ttest_ind(data_dummy[(data['test'] == 1) & 
                                           (data_dummy['country_Argentina'] == 0) &
                                           (data_dummy['country_Uruguay'] == 0)]['conversion'],
                               data_dummy[(data['test'] == 0) & 
                                           (data_dummy['country_Argentina'] == 0) &
                                           (data_dummy['country_Uruguay'] == 0)]['conversion'],
                               equal_val = False)
print(pandas.DataFrame({'data_type':['Full', 'Removed_Argentina_Uruguay'],
                       'p_value':[orginial_data.pvalue, data_no_AR_UR.pvalue],
                       't_statisit':[orginial_data.statistic, data_no_AR_UR.statistic]}))
16/70:
orginial_data = stats.ttest_ind(data_dummy[data['test'] == 1]['conversion'],
                               data_dummy[data['test'] == 0]['conversion'],
                               equal_var = False)
data_no_AR_UR = stats.ttest_ind(data_dummy[(data['test'] == 1) & 
                                           (data_dummy['country_Argentina'] == 0) &
                                           (data_dummy['country_Uruguay'] == 0)]['conversion'],
                               data_dummy[(data['test'] == 0) & 
                                           (data_dummy['country_Argentina'] == 0) &
                                           (data_dummy['country_Uruguay'] == 0)]['conversion'],
                               equal_var = False)
print(pandas.DataFrame({'data_type':['Full', 'Removed_Argentina_Uruguay'],
                       'p_value':[orginial_data.pvalue, data_no_AR_UR.pvalue],
                       't_statisit':[orginial_data.statistic, data_no_AR_UR.statistic]}))
16/71:
data_test_country = data.groupby('country')['conversion'].agg({
    "p_value": lambda x: stats.ttest_ind(x[data['test'] == 1], x[data['test'] == 0], equal_var = False).pvalue,
    'conversion_test': lambda x: x[data['test'] == 1]['conversion'].mean(),
    'conversion_control': lambda x: x[data['test'] == 0]['conversion'].mean()})
16/72:
data_test_country = data.groupby('country')['conversion'].agg({
    "p_value": lambda x: stats.ttest_ind(x[data['test'] == 1], x[data['test'] == 0], equal_var = False).pvalue,
    'conversion_test': lambda x: x[data['test'] == 1]['conversion'].mean(),
    'conversion_control': lambda x: x[data['test'] == 0]['conversion'].mean()}).reindex(['p_value','conversion_test','conversion_control'], axis = 1)
16/73:
data_test_country = data.groupby('country')['conversion'].agg({
    "p_value": lambda x: stats.ttest_ind(x[data['test'] == 1], x[data['test'] == 0], equal_var = False).pvalue,
    'conversion_test': lambda x: x[data['test'] == 1].mean(),
    'conversion_control': lambda x: x[data['test'] == 0].mean()})
16/74: data_test_country.head()
16/75:
data_test_country = data.groupby('country')['conversion'].agg({
    "p_value": lambda x: stats.ttest_ind(x[data['test'] == 1], x[data['test'] == 0], equal_var = False).pvalue,
    'conversion_test': lambda x: x[data['test'] == 1].mean(),
    'conversion_control': lambda x: x[data['test'] == 0].mean()}).reindex(['p_value','conversion_test','conversion_control'], axis = 1)
16/76: data_test_country.head()
16/77: print(data_test_country.sort_values('p_values'))
16/78: print(data_test_country.sort_values(by ='p_values'))
16/79: print(data_test_country.sort_values(by =['p_values']))
16/80: print(data_test_country.sort_values(by =['p_value']))
17/1:
from __future__ import print_function
import torch
24/1:
import pandas as pd
df = pd.DataFrame({'col1':[1,2,3,4],'col2':[444,555,666,444],'col3':['abc','def','ghi','xyz']})
df.head()
24/2: df['col2'].unique()
24/3: df['col2'].nunique()
24/4: df['col2'].value_counts()
24/5: df['col2'].counts()
24/6: df['col2'].count()
24/7: df.groupby('col2').count()
24/8:
def times2(x):
    return x*2
24/9: df['col1'].apply(times2)
24/10: df['col3'].apply(len)
24/11: df['col1'].sum()
24/12: del df['col1']
24/13: df
24/14: df.sort_values(by='col2') #inplace=False by default
24/15: df.isnull()
24/16:
# Drop rows with NaN Values
df.dropna()
24/17: import numpy as np
24/18:
df = pd.DataFrame({'col1':[1,2,3,np.nan],
                   'col2':[np.nan,555,666,444],
                   'col3':['abc','def','ghi','xyz']})
df.head()
24/19: df.fillna('FILL')
24/20:
data = {'A':['foo','foo','foo','bar','bar','bar'],
     'B':['one','one','two','two','one','one'],
       'C':['x','y','x','y','x','y'],
       'D':[1,3,2,5,4,1]}

df = pd.DataFrame(data)
24/21: df
28/1: import pands as pd
28/2: import pandas as pd
28/3: sal = pd.read_csv('Salaries.csv')
28/4: sal.head()
28/5: sal.info()
28/6: sal['BasePay'].mean()
28/7: sal['OvertimePay'].max()
28/8: sal[sal['EmployeeName'] == 'JOSEPH DRISCOLL ']['JobTitle']
28/9: sal[sal['EmployeeName'] == 'JOSEPH DRISCOLL']['JobTitle']
28/10: sal[sal['EmployeeName'] == 'JOSEPH DRISCOLL']['TotalPayBenefits']
28/11: sal['TotalPayBenefits'].max().index()
28/12: sal['TotalPayBenefits'].max()
28/13: sal['TotalPayBenefits'].idxmax()
28/14: sal.iloc(sal['TotalPayBenefits'].idxmax())
28/15: sal.iloc[sal['TotalPayBenefits'].idxmax()]
28/16: sal.loc[sal['TotalPayBenefits'].idxmax()]
28/17: sal.loc[sal['TotalPayBenefits'].idxmax()]
28/18: sal.loc[sal['TotalPayBenefits'].idxmin()]
28/19: sal[(sal['Year'] >= 2011) & (sal['Year'] <= 2014)].groupby('Year').mean()
28/20: sal[(sal['Year'] >= 2011) & (sal['Year'] <= 2014)].mean()
30/1: import pandas as pd
30/2: sal = pd.read_csv('Salaries.csv')
30/3: sal.head()
30/4: sal.info()
30/5: sal['BasePay'].mean()
30/6: sal['OvertimePay'].max()
30/7: sal[sal['EmployeeName'] == 'JOSEPH DRISCOLL']['JobTitle']
30/8: sal[sal['EmployeeName'] == 'JOSEPH DRISCOLL']['TotalPayBenefits']
30/9: sal.loc[sal['TotalPayBenefits'].idxmax()]
30/10: sal.loc[sal['TotalPayBenefits'].idxmin()]
30/11: sal[(sal['Year'] >= 2011) & (sal['Year'] <= 2014)].groupby('Year').mean()['Year']
30/12: sal.groupby('Year').mean()['Year']
30/13: sal[(sal['Year'] >= 2011) & (sal['Year'] <= 2014)].groupby('Year').mean()['BasePay']
30/14: sal[(sal['Year'] >= 2011) & (sal['Year'] <= 2014)].groupby('Year').mean()
30/15: sal[(sal['Year'] >= 2011) & (sal['Year'] <= 2014)].groupby('Year').mean()['BasePay']
30/16: sal['JobTitle'].nunique()
30/17: sal['JobTitle'].value_count()
30/18: sal['JobTitle'].values_count()
30/19: sal['JobTitle'].value_counts()
30/20: sal['JobTitle'].value_counts().mean()
30/21: sal['JobTitle'].value_counts()
30/22: sum(sal['JobTitle'].value_counts() == 1)
30/23: sum(sal[sal["Year"] == 2013]['JobTitle'].value_counts() == 1)
30/24:
def chief_string(title):
    if 'chief' in title.lower():
        return True
    else:
        return False


sum(sal['JobTitle'].apply(lambda x: chife_string(x)))
30/25:
def chief_string(title):
    if 'chief' in title.lower():
        return True
    else:
        return False


sum(sal['JobTitle'].apply(lambda x: chief_string(x)))
30/26: sum(sal['JobTitle'].apply(lambda x: chief_string(x)))
30/27:
def chief_string(title):
    if 'chief' in title.lower():
        return True
    else:
        return False
30/28: sum(sal['JobTitle'].apply(lambda x: chief_string(x)))
30/29: sal['title_len'] = sal['JobTitle'].apply(len)
30/30: sal[['title_len','TotalPayBenefits']].corr()
30/31: sal.corr()
31/1: ecom = pd.reas_csv('Ecommerce Purchases')
31/2: import pandas as pd
31/3: ecom = pd.reas_csv('Ecommerce Purchases')
31/4: ecom = pd.read_csv('Ecommerce Purchases')
31/5: ecom.head()
31/6: ecom.info()
31/7: ecom['Purchase Price'].mean()
31/8: ecom['Purchase Price'].max()
31/9: ecom['Purchase Price'].min()
31/10: sum(ecom['Language'] == 'en')
31/11: sum(ecom['Job'] == 'Lawyer')
31/12: ecom.groupby('AM or PM').value_counts()
31/13: ecom['AM or PM'].value_counts()
31/14: ecom['Job'].value_counts()
31/15: ecom['Job'].value_counts().head(5)
31/16: ecom['Job'].value_counts(ascending = False).head(5)
31/17: ecom['Job'].value_counts(ascending = Truee).head(5)
31/18: ecom['Job'].value_counts(ascending = True).head(5)
31/19: ecom['Job'].value_counts().head(5)
31/20: ecom[ecom['Lot'] = '90 WT']['Purchase Price']
31/21: ecom[ecom['Lot'] == '90 WT']['Purchase Price']
31/22: ecom[ecom['Credit Card'].apply(lambda x: x[:16] == '4926535242672853')]
31/23: ecom[ecom['Credit Card'].apply(lambda x: x[:16].str() =='4926535242672853')]
31/24: ecom[ecom['Credit Card'].apply(lambda x.str(): x[:16] =='4926535242672853')]
31/25: ecom[ecom['Credit Card'].apply(lambda x: x[:16] == 4926535242672853)]
31/26: ecom[ecom['Credit Card'].apply(lambda x: x == 4926535242672853)]
31/27: ecom[ecom['Credit Card'] == 4926535242672853]['Email']
31/28: sum(ecom[(ecom['CC Provider'] == 'American Express') & (ecom['Purchase Price '] > 95)] )
31/29: sum(ecom[(ecom['CC Provider'] == 'American Express') & (ecom['Purchase Price'] > 95)] )
31/30: sum(ecom[(ecom['CC Provider'] == 'American Express') & (ecom['Purchase Price'].int() > 95)] )
31/31: sum(ecom[(ecom['CC Provider'] == 'American Express') & (ecom['Purchase Price']>95)] )
31/32: ecom[(ecom['CC Provider'] == 'American Express') & (ecom['Purchase Price']>95)].count()
31/33: sum(ecom[(ecom['CC Provider'] == 'American Express') & (ecom['Purchase Price']>95)])
31/34: ecom[(ecom['CC Provider'] == 'American Express') & (ecom['Purchase Price']>95)].count()
31/35: sum(ecom[ecom['CC Exp Date'] == 2025])
31/36: sum(ecom['CC Exp Date'].apply(lambda x: x[3:]) == '25')
31/37: sum(ecom['CC Provider'] == 'American Express') & (ecom['Purchase Price']>95))
31/38: sum((ecom['CC Provider'] == 'American Express') & (ecom['Purchase Price']>95))
31/39: ecom['Email'].value_counts()
31/40: ecom[email].apply(lambda x: x.split('@')[-1]).value_counts()
31/41: ecom['Email'].apply(lambda x: x.split('@')[-1]).value_counts()
31/42: ecom['Email'].apply(lambda x: x.split('@')[-1]).value_counts().head(5)
33/1:
# plt.subplot(nrows, ncols, plot_number)
plt.subplot([1,1])
plt.plot(x, y, 'r--') # More on color options later
plt.subplot([1,2])
plt.plot(y, x, 'g*-');
33/2:
# plt.subplot(nrows, ncols, plot_number)
plt.subplot(1,2,1)
plt.plot(x, y, 'r--') # More on color options later
plt.subplot(1,2,2)
plt.plot(y, x, 'g*-');
33/3: import matplotlib.pyplot as plt
33/4: %matplotlib inline
33/5:
import numpy as np
x = np.linspace(0, 5, 11)
y = x ** 2
33/6: x
33/7: y
33/8:
plt.plot(x, y, 'r') # 'r' is the color red
plt.xlabel('X Axis Title Here')
plt.ylabel('Y Axis Title Here')
plt.title('String Title Here')
plt.show()
33/9:
# plt.subplot(nrows, ncols, plot_number)
plt.subplot([0,0])
plt.plot(x, y, 'r--') # More on color options later
plt.subplot([0,1])
plt.plot(y, x, 'g*-');
33/10:
# plt.subplot(nrows, ncols, plot_number)
plt.subplot(1,2,1)
plt.plot(x, y, 'r--') # More on color options later
plt.subplot(1,2,2)
plt.plot(y, x, 'g*-');
33/11:
# Empty canvas of 1 by 2 subplots
fig, axes = plt.subplots(nrows=2, ncols=2)
33/12:
# Axes is an array of axes to plot on
axes
33/13:
for ax in axes:
    ax.plot(x, y, 'b')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title('title')

# Display the figure object    
fig
33/14:
# Empty canvas of 1 by 2 subplots
fig, axes = plt.subplots(nrows=1, ncols=2)
33/15:
# Axes is an array of axes to plot on
axes
33/16:
for ax in axes:
    ax.plot(x, y, 'b')
    ax.set_xlabel('x')
    ax.set_ylabel('y')
    ax.set_title('title')

# Display the figure object    
fig
33/17:
for ax_row in axes:
    for ax in ax_row:
        ax.plot(x, y, 'b')
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title('title')

# Display the figure object    
fig
33/18:
# Empty canvas of 1 by 2 subplots
fig, axes = plt.subplots(nrows=2, ncols=2)
33/19:
# Axes is an array of axes to plot on
axes
33/20:
for ax_row in axes:
    for ax in ax_row:
        ax.plot(x, y, 'b')
        ax.set_xlabel('x')
        ax.set_ylabel('y')
        ax.set_title('title')

# Display the figure object    
fig
33/21: data
33/22:
data = [np.random.normal(0, std, 100) for std in range(1, 4)]

# rectangular box plot
plt.boxplot(data,vert=True,patch_artist=True);
33/23: data
34/1:
import numpy as np
x = np.arange(0,100)
y = x*2
z = x**2
34/2:
import matplotlib.pyplot as plt
%matplotlib inline
34/3:
fig, ax = plt.subplot([0,0,1,1])
plt.plot(x,y)
fig
34/4:
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.plot(x,y)
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_title('title')
34/5:
fig = plt.figure()

ax1 = fig.add_axes([0,0,1,1])
ax2 = fig.add_axes([0.2,0.5,.2,.2]
34/6:
fig = plt.figure()

ax1 = fig.add_axes([0,0,1,1])
ax2 = fig.add_axes([0.2,0.5,.2,.2])
34/7:
ax1.plot(x,y)
ax2.plot(x,y)
34/8:
ax1.plot(x,y)
ax2.plot(x,y)
fig
34/9:
fig = plt.figure()
ax1 = fig.add_axes([0,0,1,1])
ax2 = fig.add_axes([0.2,0.5,0.4,0.4])
34/10:
ax1.plot(x,y)
ax2.plot(x,z)
fig
34/11: fig, axes = plt.subplots(nrows=1, ncols=2)
34/12:
axes[0].plot(x,y)
axes[1].plot(x,z)
34/13:
axes[0].plot(x,y)
axes[1].plot(x,z)
fig
34/14:
fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12,2))

axes[0].plot(x,y,color="blue", lw=5)
axes[0].set_xlabel('x')
axes[0].set_ylabel('y')

axes[1].plot(x,z,color="red", lw=3, ls='--')
axes[1].set_xlabel('x')
axes[1].set_ylabel('z')
35/1:
# distance between x and y axis and the numbers on the axes
#matplotlib.rcParams['xtick.major.pad'] = 5
#matplotlib.rcParams['ytick.major.pad'] = 5

fig, ax = plt.subplots(1, 1)
      
ax.plot(x, x**2, x, np.exp(x))
ax.set_yticks([0, 50, 100, 150])

ax.set_title("label and axis spacing")

# padding between axis label and axis numbers
ax.xaxis.labelpad = 5
ax.yaxis.labelpad = 5

ax.set_xlabel("x")
ax.set_ylabel("y");
35/2:
# distance between x and y axis and the numbers on the axes
matplotlib.rcParams['xtick.major.pad'] = 5
matplotlib.rcParams['ytick.major.pad'] = 5

fig, ax = plt.subplots(1, 1)
      
ax.plot(x, x**2, x, np.exp(x))
ax.set_yticks([0, 50, 100, 150])

ax.set_title("label and axis spacing")

# padding between axis label and axis numbers
ax.xaxis.labelpad = 5
ax.yaxis.labelpad = 5

ax.set_xlabel("x")
ax.set_ylabel("y");
35/3:
fig, axes = plt.subplots(1, 2, figsize=(10,4))
      
axes[0].plot(x, x**2, x, np.exp(x))
axes[0].set_title("Normal scale")

axes[1].plot(x, x**2, x, np.exp(x))
axes[1].set_yscale("log")
axes[1].set_title("Logarithmic scale (y)");
35/4:
import matplotlib.pyplot as plt
%matplotlib inline
35/5:
fig, axes = plt.subplots(1, 2, figsize=(10,4))
      
axes[0].plot(x, x**2, x, np.exp(x))
axes[0].set_title("Normal scale")

axes[1].plot(x, x**2, x, np.exp(x))
axes[1].set_yscale("log")
axes[1].set_title("Logarithmic scale (y)");
35/6:
fig, ax = plt.subplots(figsize=(10, 4))

ax.plot(x, x**2, x, x**3, lw=2)

ax.set_xticks([1, 2, 3, 4, 5])
ax.set_xticklabels([r'$\alpha$', r'$\beta$', r'$\gamma$', r'$\delta$', r'$\epsilon$'], fontsize=18)

yticks = [0, 50, 100, 150]
ax.set_yticks(yticks)
ax.set_yticklabels(["$%.1f$" % y for y in yticks], fontsize=18); # use LaTeX formatted labels
35/7:
# distance between x and y axis and the numbers on the axes
matplotlib.rcParams['xtick.major.pad'] = 5
matplotlib.rcParams['ytick.major.pad'] = 5

fig, ax = plt.subplots(1, 1)
      
ax.plot(x, x**2, x, np.exp(x))
ax.set_yticks([0, 50, 100, 150])

ax.set_title("label and axis spacing")

# padding between axis label and axis numbers
ax.xaxis.labelpad = 5
ax.yaxis.labelpad = 5

ax.set_xlabel("x")
ax.set_ylabel("y");
41/1:
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
41/2: sns.set_style('whitegrid')
41/3: titanic = sns.load_dataset('titanic')
41/4: titanic.head()
41/5: sns.pairplot(x='fare',y='age')
41/6: sns.pairplot(x='fare',y='age',data = 'titanic')
41/7: sns.jointplot(x='fare',y='age',data = 'titanic')
41/8: sns.jointplot(x='fare',y='age',data = titanic)
41/9: sns.distplot('fare')
41/10: sns.distplot(titanic['fare'],bins=30, kde=False, color = 'red')
41/11: sns.boxplot(titanic['age'])
41/12: sns.boxplot(x='class', y = 'age', data = titanic)
41/13: sns.boxplot(x='class', y = 'age', data = titanic,palette = 'rainbow')
41/14: sns.swarmplot(x='class', y = 'age', data = titanic,palette = 'rainbow')
41/15: sns.swarmplot(x='class', y = 'age', data = titanic,palette = 'set2')
41/16: sns.swarmplot(x='class', y = 'age', data = titanic,palette = 'Set2')
41/17: sns.countplot(titanic['sex'])
41/18: sns.corr.plot()
41/19:
sns.heatmap(titanic.corr(), cmap = 'coolwarm')
plt.title('titanic.corr()')
41/20:
g = sns.FacetGrid(data=titanic,col='sex')
g.map(plt.hist,'age')
43/1:
import numpy as np
import pandas as pd
%matplotlib inline
43/2:
df1 = pd.read_csv('df1',index_col=0)
df2 = pd.read_csv('df2')
43/3: df1['A'].hist()
43/4:
import matplotlib.pyplot as plt
plt.style.use('ggplot')
43/5: df1['A'].hist()
43/6:
plt.style.use('bmh')
df1['A'].hist()
43/7:
plt.style.use('dark_background')
df1['A'].hist()
43/8:
plt.style.use('fivethirtyeight')
df1['A'].hist()
43/9: plt.style.use('ggplot')
43/10: df2.plot.area(alpha=0.4)
43/11: df2.head()
43/12: df2.head()
43/13: df2.plot.bar()
43/14: df2.plot.bar(stacked=True)
43/15: df1['A'].plot.hist(bins=50)
43/16: df1.plot.line(x=df1.index,y='B',figsize=(12,3),lw=1)
43/17: df1.plot.line(x=df1.index,y='B',figsize=(12,3),lw=1)
43/18: df1.plot.scatter(x='A',y='B')
43/19: df1.plot.scatter(x='A',y='B',c='C',cmap='coolwarm')
43/20: df1.plot.scatter(x='A',y='B',s=df1['C']*200)
43/21: df2.plot.box() # Can also pass a by= argument for groupby
43/22:
df = pd.DataFrame(np.random.randn(1000, 2), columns=['a', 'b'])
df.plot.hexbin(x='a',y='b',gridsize=25,cmap='Oranges')
43/23: df2['a'].plot.kde()
43/24: df2.plot.density()
44/1:
import pandas as pd
import matplotlib.pyplot as plt
df3 = pd.read_csv('df3')
%matplotlib inline
44/2: df3.info()
44/3: df3.head()
44/4: df3.plot.scatter(x='a', y='b', color='red', figsize=(12,3))
44/5: df3.plot.scatter(x='a', y='b', color='red', s = 50, figsize=(12,3))
44/6: df3.plot.scatter(x='a', y='b', color='red', size = 50 figsize=(12,3))
44/7: df3.plot.scatter(x='a', y='b', color='red', s = 50 figsize=(12,3))
44/8: df3.plot.scatter(x='a', y='b', color='red', size = 50, figsize=(12,3))
44/9: df3.plot.scatter(x='a', y='b', color='red', s = 50, figsize=(12,3))
45/1: df3.plot.scatter(x='a',y='b',c='red',s=50,figsize=(12,3))
45/2:
import pandas as pd
import matplotlib.pyplot as plt
df3 = pd.read_csv('df3')
%matplotlib inline
45/3: df3.info()
45/4: df3.head()
45/5: df3.plot.scatter(x='a',y='b',c='red',s=50,figsize=(12,3))
44/10: df3['a'].plot.hist()
44/11: df3['a'].plot.hist(bin =30)
44/12: df3['a'].plot.hist(bins =30)
44/13: plt.style.use('ggplot')
44/14: df3['a'].plot.hist(alpha =0.5, bins =30)
44/15: df3[['a','b']].plot.box()
44/16: df3['d'].plot.kde()
44/17: df3['d'].plot.kde(lw=5,ls='--')
44/18: df3.plot.area()
44/19: df3.ix[0:30].plot.area()
44/20: df3.ix[0:30]
44/21:
f = plt.figure()
df3.ix[0:30].plot.area(alpha=0.4,ax=f.gca())
plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
plt.show()
44/22:
f = plt.figure()
df3.ix[0:30].plot.area(alpha=0.4,ax=f.gca())
plt.legend(loc='center left')
plt.show()
44/23:
f = plt.figure()
df3.ix[0:30].plot.area(alpha=0.4,ax=f.gca())
plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))
plt.show()
47/1:
import numpy as np
import pandas as pd
47/2:
import matplotlib.pyplot as plt
%matplotlib inline
47/3: df = pd.read_csv('dataframe')
47/4: df = pd.read_csv('911.csv')
47/5: df.info()
47/6: df.head()
47/7: df['zip'].head()
47/8: df['zip'].head(5)
47/9: df['zip'].sount_values()
47/10: df['zip'].count_values()
47/11: df['zip'].value_counts()
47/12: df['zip'].value_counts().head(5)
47/13: df['twp'].value_counts().head(5)
47/14: df['title'].nunique()
47/15: df['Reason'] = df['title'].apply(lambda: x : x.split())[0]
47/16: df['Reason'] = df['title'].apply(lambda: x x.split())[0]
47/17:
df['Reason'] = df['title'].apply(lambda: x :x.split()[0])
#df['Reason'] = df['title'].apply(lambda title: title.split(':')[0])
47/18:
df['Reason'] = df['title'].apply(lambda: x: x.split()[0])
#df['Reason'] = df['title'].apply(lambda title: title.split(':')[0])
47/19:
#df['Reason'] = df['title'].apply(lambda: x: x.split()[0])
df['Reason'] = df['title'].apply(lambda title: title.split(':')[0])
47/20:
df['Reason'] = df['title'].apply(lambda x: x.split()[0])
#df['Reason'] = df['title'].apply(lambda title: title.split(':')[0])
47/21:
df['Reason'] = df['title'].apply(lambda x: x.split())[0]
#df['Reason'] = df['title'].apply(lambda title: title.split(':')[0])
47/22:
df['Reason'] = df['title'].apply(lambda x: x.split()[0])
#df['Reason'] = df['title'].apply(lambda title: title.split(':')[0])
47/23: df.head()
47/24:
df['Reason'] = df['title'].apply(lambda x: x.split(":")[0])
#df['Reason'] = df['title'].apply(lambda title: title.split(':')[0])
47/25: df.head()
47/26: df['Reason'].value_counts()
47/27: df['Reason'].countplot()
47/28: df['Reason'].count.plot()
47/29: sns
47/30:
import seaborn as sns
sns.countplot(x = 'Reason', data = df)
47/31: df['timeStamp'].dtype()
47/32: type(df['timeStamp'][0])
47/33:
from datetime import datetime
pd['timeStamp'].to_datetime()
47/34: df['timeStamp'] = pd.to_datetime(df['timeStamp'])
47/35: df['Hour'] = df['timeStamp'].apply(lambda x: x.hour)
47/36:
df['Hour'] = df['timeStamp'].apply(lambda x: x.hour)
df['Month'] = df['Month'].apply(lamda x: x.monthu)
47/37:
df['Hour'] = df['timeStamp'].apply(lambda x: x.hour)
df['Month'] = df['Month'].apply(lamda x: x.month)
47/38:
df['Hour'] = df['timeStamp'].apply(lambda x: x.hour)
df['Month'] = df['Month'].apply(lambda x: x.month)
df['Day of Week'] = df['Month'].apply(lambda x: x.dayofweek)
47/39:
df['Hour'] = df['timeStamp'].apply(lambda x: x.hour)
df['Month'] = df['timeStamp'].apply(lambda x: x.month)
df['Day of Week'] = df['timeStamp'].apply(lambda x: x.dayofweek)
47/40: dmap
47/41: dmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}
47/42: df['Day of Week'].map(dmap)
47/43: df['Day of Week'] = df['Day of Week'].map(dmap)
47/44: sns.countplot(x = 'Day of Week', hue = 'reason', data =df)
47/45: sns.countplot(x = 'Day of Week', hue = 'Reason', data =df)
47/46:
sns.countplot(x = 'Day of Week', hue = 'Reason', data =df)
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
47/47:
sns.countplot(x = 'Day of Week', hue = 'Reason', data =df)
plt.legend(bbox_to_anchor=(1.05, 1))
47/48:
sns.countplot(x = 'Day of Week', hue = 'Reason', data =df)
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
47/49:
sns.countplot(x = 'Month', hue = 'Reason', data =df)
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
47/50: df.groupby('Month').count()
47/51: df.groupby('Month').count().head()
47/52: byMonth = df.groupby('Month').count()
47/53:
byMonth = df.groupby('Month').count()
byMonth.head()
47/54: byMonth['twp'].plot()
47/55: df['Date']=df['timeStamp'].apply(lambda t: t.date())
47/56: df['Date'].count()
47/57: df.groupby('Date').count.plot()
47/58:
df.groupby('Date').count().plot()
df.groupby('Date').count()['twp'].plot()
47/59: df.groupby('Date').count()
47/60: df.groupby('Date').count()['twp'].plot()
47/61:
g = sns.FacetGrid(df, row = 'Reason')
g.map(plt.plot, 'date')
47/62:
df[df['Reason']=='Traffic'].groupby('Date').count()['twp'].plot()
df[df['Reason'] == 'Traffic'].groupby('Date').count()['twp'].plot()
plt.title('Traffic')
47/63:
df[df['Reason']=='Traffic'].groupby('Date').count()['twp'].plot()
plt.title('Traffic')
47/64:
df[df['Reason']=='Fire'].groupby('Date').count()['twp'].plot()
plt.title('Traffic')
47/65:
df[df['Reason']=='EMS'].groupby('Date').count()['twp'].plot()
plt.title('EMS')
47/66:
g = sns.FacetGrid(data=titanic,col='Reason')
g.map(plt.plot,'Date')
47/67:
g = sns.FacetGrid(data=df,col='Reason')
g.map(plt.plot,'Date')
47/68:
g = sns.FacetGrid(data=df,col='Reason')
g.map(plt.plot(x = 'Date', y =))
47/69:
g = sns.FacetGrid(data=df,col='Reason')
g.map(plt.plot(x = 'Date'))
47/70:
g = sns.FacetGrid(data=df,col='Reason')
g.map(plt.plot(),'Date')
47/71:
g = sns.FacetGrid(data=df,col='Reason')
g.map(plt.plot,'Date')
47/72: df.groupby('Day of the Week')
47/73: df
47/74: df.groupby('Day of Week')
47/75: df.groupby('Day of Week').head()
47/76: dayHour = df.groupby(by= ['Day of Week', 'Hour']).count()['Reason']
47/77:
dayHour = df.groupby(by= ['Day of Week', 'Hour']).count()['Reason']
dayHour.head()
47/78: dayHour = df.groupby(by= ['Day of Week', 'Hour']).count()['Reason'].unstack()
47/79:
dayHour = df.groupby(by= ['Day of Week', 'Hour']).count()['Reason'].unstack()
dayHour.head()
47/80: sns.heatmap(dayHour)
47/81:

plt.figure(figsize=(12,6))
sns.heatmap(dayHour)
47/82: sns.cluster(dayHour)
47/83: sns.clustermap(dayHour)
47/84: dayMonth = df.groupby(by =['Day of Week', 'Month']).count()['Reason'].unstack()
47/85:
dayMonth = df.groupby(by =['Day of Week', 'Month']).count()['Reason'].unstack()
dayMonth.head()
47/86: sns.heatmap(dayMonth)
47/87: sns.clutermap(dayMonth)
47/88: sns.clustermap(dayMonth)
51/1:
from pandas_datareader import data, wb
import pandas as pd
import numpy as np
import datetime
%matplotlib inline
51/2:
from pandas_datareader import data, wb
import pandas as pd
import numpy as np
import datetime
%matplotlib inline
51/3: BAC = data.DataReader("BAC", 'google', start, end)
51/4:
start = datetime.datetime(2006, 1, 1)
end = datetime.datetime(2016, 1, 1)
51/5: BAC = data.DataReader("BAC", 'google', start, end)
51/6:
# Bank of America
BAC = data.DataReader("BAC", 'google', start, end)

# CitiGroup
C = data.DataReader("C", 'google', start, end)

# Goldman Sachs
GS = data.DataReader("GS", 'google', start, end)

# JPMorgan Chase
JPM = data.DataReader("JPM", 'google', start, end)

# Morgan Stanley
MS = data.DataReader("MS", 'google', start, end)

# Wells Fargo
WFC = data.DataReader("WFC", 'google', start, end)
51/7:
# Bank of America
BAC = data.DataReader("BAC", 'Google', start, end)

# CitiGroup
C = data.DataReader("C", 'google', start, end)

# Goldman Sachs
GS = data.DataReader("GS", 'google', start, end)

# JPMorgan Chase
JPM = data.DataReader("JPM", 'google', start, end)

# Morgan Stanley
MS = data.DataReader("MS", 'google', start, end)

# Wells Fargo
WFC = data.DataReader("WFC", 'google', start, end)
51/8:
# Bank of America
BAC = data.DataReader("BAC", 'google', start, end)

# CitiGroup
C = data.DataReader("C", 'google', start, end)

# Goldman Sachs
GS = data.DataReader("GS", 'google', start, end)

# JPMorgan Chase
JPM = data.DataReader("JPM", 'google', start, end)

# Morgan Stanley
MS = data.DataReader("MS", 'google', start, end)

# Wells Fargo
WFC = data.DataReader("WFC", 'google', start, end)
51/9:
# Bank of America
BAC = data.DataReader("BAC", 'yahoo', start, end)

# CitiGroup
C = data.DataReader("C", 'yahoo', start, end)

# Goldman Sachs
GS = data.DataReader("GS", 'yahoo', start, end)

# JPMorgan Chase
JPM = data.DataReader("JPM", 'yahoo', start, end)

# Morgan Stanley
MS = data.DataReader("MS", 'yahoo', start, end)

# Wells Fargo
WFC = data.DataReader("WFC", 'yahoo', start, end)
51/10: df = data.DataReader(['BAC', 'C', 'GS', 'JPM', 'MS', 'WFC'],'yahoo', start, end)
51/11: tickers = ['BAC', 'C', 'GS', 'JPM', 'MS', 'WFC']
51/12: bank_stocks = pd.concat([BAC, C, GS, JPM, MS, WFC],axis=1,keys=ticker)
51/13: ticker = ['BAC', 'C', 'GS', 'JPM', 'MS', 'WFC']
51/14: bank_stocks = pd.concat([BAC, C, GS, JPM, MS, WFC],axis=1,keys=ticker)
51/15: bank_stocks.head()
51/16: bank_stocks = pd.concat([BAC, C, GS, JPM, MS, WFC],axis=1,keys=ticker)
51/17: bank_stocks.head(5)
51/18: bank_stocks.columns.names = ['Bank Ticker', 'Stock Info']
51/19: bank_stocks.columns.names = ['Bank Ticker','Stock Info']
51/20: bank_stocks.head()
51/21: bank_stocks.colums.xs('Close').max()
51/22: bank_stocks.colums.xs('Close')
51/23: bank_stocks.xs('Close')
51/24: bank_stocks.xs('Close')
51/25: bank_stocks['Close'].xs('Bank Ticker')
51/26: bank_stocks['Close'].max().xs('Bank Ticker')
51/27: bank_stocks.columns.names = ['Bank Ticker','Stock Info']
51/28: bank_stocks.head()
51/29:
bank_stocks.xs(key = 'Close', axis = 1, level = 'Stock Info').max()
bank_stocks.xs(key='Close',axis=1,level='Stock Info').max()
51/30:
bank_stocks.xs(key = 'Close', axis = 1, level = 'Stock Info').max()
bank_stocks.xs(key='Close',axis=1,level='Stock Info')
51/31:
bank_stocks.xs(key = 'Close', axis = 1, level = 'Stock Info').max()
bank_stocks.xs(key='Close',axis=1,level='Bank Sticker')
51/32:
bank_stocks.xs(key = 'Close', axis = 1, level = 'Stock Info').max()
bank_stocks.xs(key='Close',axis=1,level='Stock Info')
51/33:
bank_stocks.xs(key = 'Close', axis = 1, level = 'Stock Info').max()
bank_stocks.xs(key='Close',axis=1,level='Stock Info').max()
51/34: bank_stocks.xs(key='Close',axis=1,level='Stock Info').max()
51/35: returns = pd.DateFrame()
51/36: returns = pd.DataFrame()
51/37: df.groupby(by = ['Date', 'Bank Ticker'])
51/38: df.groupby(by = ['Date', 'Bank Ticker'])['Close']
51/39: df.groupby(by = ['Date'])['Close']
51/40: df.groupby(by = ['Date'])['Close'].head()
51/41: df.groupby('Date')
51/42: df.groupby('Date').head()
51/43: bank_stocks.xs(key = 'Close', axis = 1, level='Stock Info')
51/44:
for tick in tickers:
    returns[tick +' Return'] = bank_stocks[tick]['Close'].pct_change()
returns.head()
51/45: bank_stocks
51/46: sns.pairplot(bank_stocks)
51/47:
import seaborn as sns
sns.pairplot(bank_stocks)
53/1:
from pandas_datareader import data, wb
import pandas as pd
import numpy as np
import datetime
%matplotlib inline
53/2:
start = datetime.datetime(2006, 1, 1)
end = datetime.datetime(2016, 1, 1)
53/3:
# Bank of America
BAC = data.DataReader("BAC", 'yahoo', start, end)

# CitiGroup
C = data.DataReader("C", 'yahoo', start, end)

# Goldman Sachs
GS = data.DataReader("GS", 'yahoo', start, end)

# JPMorgan Chase
JPM = data.DataReader("JPM", 'yahoo', start, end)

# Morgan Stanley
MS = data.DataReader("MS", 'yahoo', start, end)

# Wells Fargo
WFC = data.DataReader("WFC", 'yahoo', start, end)
53/4: df = data.DataReader(['BAC', 'C', 'GS', 'JPM', 'MS', 'WFC'],'yahoo', start, end)
53/5: ticker = ['BAC', 'C', 'GS', 'JPM', 'MS', 'WFC']
53/6: bank_stocks = pd.concat([BAC, C, GS, JPM, MS, WFC],axis=1,keys=ticker)
53/7: bank_stocks.head(5)
53/8: bank_stocks.columns.names = ['Bank Ticker', 'Stock Info']
53/9: bank_stocks.columns.names = ['Bank Ticker','Stock Info']
53/10: bank_stocks.head()
53/11: bank_stocks.xs(key='Close',axis=1,level='Stock Info').max()
53/12: returns = pd.DataFrame()
53/13:
for tick in tickers:
    returns[tick +' Return'] = bank_stocks[tick]['Close'].pct_change()
returns.head()
53/14: returns = pd.DataFrame()
53/15:
for tick in tickers:
    returns[tick +' Return'] = bank_stocks[tick]['Close'].pct_change()
returns.head()
53/16:
for tick in ticker:
    returns[tick +' Return'] = bank_stocks[tick]['Close'].pct_change()
returns.head()
53/17:
import seaborn as sns
sns.pairplot(returns[1:])
53/18: returns.idxmin()
53/19: returns.idxmax()
53/20: returns.std()
53/21: returns.ix['2015-01-01':'2015-12-31'].std()
53/22: returns['MS Return'].ix['2015-01-01':'2015-12-31'].displot()
53/23: sns.displot(returns['MS Return'].ix['2015-01-01':'2015-12-31'])
53/24: returns['MS Return'].ix['2015-01-01':'2015-12-31'].displot()
53/25: sns.distplot(returns.ix['2015-01-01':'2015-12-31']['MS Return'],color='green',bins=100)
53/26: sns.distplot(returns['MS Return'].ix['2015-01-01':'2015-12-31'],color='green',bins=100)
53/27: sns.distplot(returns.ix['2015-01-01':'2015-12-31']['MS Return'],color='green',bins=100)
53/28: returns.head()
53/29: returns['Date']
53/30: returns.ix()
53/31: returns.ix().head()
53/32: print(returns.ix())
53/33: returns.loc()
53/34: returns.loc().head()
53/35: returns.loc()
53/36: np.list(returns.loc())
53/37: list(returns.loc())
53/38: sns.distplot(returns.ix['2015-01-01':'2015-12-31']['MS Return'],color='green',bins=100)
53/39:
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
%matplotlib inline

# Optional Plotly Method Imports
import plotly
import cufflinks as cf
cf.go_offline()
54/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
54/2: USAhousing = pd.read_csv('USA_Housing.csv')
54/3: USAhousing.head()
54/4: USAhousing.columns
54/5: sns.pairplot(USAhousing)
54/6: sns.distplot(USAhousing['Price'])
54/7: sns.heatmap(USAhousing.corr())
54/8:
X = USAhousing[['Avg. Area Income', 'Avg. Area House Age', 'Avg. Area Number of Rooms',
               'Avg. Area Number of Bedrooms', 'Area Population']]
y = USAhousing['Price']
54/9: from sklearn.model_selection import train_test_split
54/10: sns.distplot(USAhousing['Price'])
54/11: sns.heatmap(USAhousing.corr())
54/12:
# print the intercept
print(lm.intercept_)
54/13: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)
54/14: from sklearn.linear_model import LinearRegression
54/15: lm = LinearRegression()
54/16: lm.fit(X_train,y_train)
54/17:
# print the intercept
print(lm.intercept_)
54/18:
coeff_df = pd.DataFrame(lm.coef_,X.columns,columns=['Coefficient'])
coeff_df
54/19: predictions = lm.predict(X_test)
54/20: plt.scatter(y_test,predictions)
54/21: sns.distplot((y_test-predictions),bins=50);
54/22: from sklearn import metrics
54/23:
print('MAE:', metrics.mean_absolute_error(y_test, predictions))
print('MSE:', metrics.mean_squared_error(y_test, predictions))
print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))
55/1:
import pandas as pd
import numpy as np
import seaborn as sns
55/2: customers = pd.read_csv('Ecommerce Customers')
55/3: customers.head()
55/4: sns.jointplot(x= 'Time on Website', y = 'Yearly Amount Spent', data = customers)
55/5:
sns.set_palette("GnBu_d")
sns.set_style('whitegrid')
sns.jointplot(x= 'Time on Website', y = 'Yearly Amount Spent', data = customers)
55/6: sns.jointplot(x='Time on App',y='Yearly Amount Spent',data=customers)
55/7: sns.jointplot(x='Time on Website',y='Yearly Amount Spent',data=customers, kind ='hex')
55/8: sns.jointplot(x='Time on App',y='Yearly Amount Spent',data=customers, kind ='hex')
55/9: sns.pairplot(customers)
55/10: sns.lmplot(x='Length of Membership', y ='Yearly Amount Spent')
55/11: sns.lmplot(x='Length of Membership', y ='Yearly Amount Spent', data = customers)
55/12: X = customers[['Avg. Session Length', 'Time on App','Time on Website', 'Length of Membership']]
55/13: Y = customers['Yearly Amount Spent']
55/14: from sklearn.model_selection import train_test_split
55/15: train, test = train_test_split(X,y, test_size =0.3, random_state = 101)
55/16: from sklearn.model_selection import train_test_split
55/17: train, test = train_test_split(X,y, test_size =0.3, random_state = 101)
55/18: X_train, X_test, y_train, y_test = train_test_split(X,y, test_size =0.3, random_state = 101)
55/19: y = customers['Yearly Amount Spent']
55/20: from sklearn.model_selection import train_test_split
55/21: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.3, random_state = 101)
55/22: from sklearn.linear_model import LinearRegression
55/23: lm = LinearRegression()
55/24: lm.fit(X_train, y_train)
55/25: lm.coef_
55/26: lm.predict(X_test)
55/27: y_est = lm.predict(X_test)
55/28: plt.scatter(y_test, y_est)
55/29:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
55/30: plt.scatter(y_test, y_est)
55/31:
from sklearn import metrics
print(metrics.mean_absolute_error(y_test, y_est))
print(metrics.mean_squared_error(y_test, y_est))
55/32: plt.displot(y_est - y_test)
55/33: plt.distplot(y_est - y_test)
55/34: sns.distplot(y_est - y_test)
55/35: pd.Dataframe({'Coeffecient': lm.coef_}, label = ['Avg. Session Length', 'Time on App','Time on Website', 'Length of Membership'])
55/36: co =pd.Dateframe({'Coeffecient': lm.coef_}, label = ['Avg. Session Length', 'Time on App','Time on Website', 'Length of Membership'])
55/37: co =pd.DataFrame({'Coeffecient': lm.coef_}, label = ['Avg. Session Length', 'Time on App','Time on Website', 'Length of Membership'])
55/38: co =pd.DataFrame({'Coeffecient': lm.coef_}, index = ['Avg. Session Length', 'Time on App','Time on Website', 'Length of Membership'])
55/39: co.head()
55/40: co =pd.DataFrame({'Coeffecient': lm.coef_}, X.columns)
55/41: co.head()
57/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
57/2: train = pd.read_csv('titanic_train.csv')
57/3: train.head()
57/4:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
57/5: train = pd.read_csv('titanic_train.csv')
57/6: train.head()
57/7: sns.heatmap(train.isnull())
57/8: sns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap ='viridis')
57/9: sns.heatmap(train.isnull(), cbar = False, cmap ='viridis')
57/10: sns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap ='viridis')
57/11: sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')
57/12:
sns.set_style('whitegrid')
sns.countplot(x='Survived',data=train,palette='RdBu_r')
57/13:
sns.set_style('whitegrid')
sns.countplot(x='Survived',hue='Sex',data=train,palette='RdBu_r')
57/14:
sns.set_style('whitegrid')
sns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')
57/15: sns.distplot(train['Age'].dropna(),kde=False,color='darkred',bins=30)
57/16: sns.countplot(x='SibSp',data=train)
57/17: train['Fare'].hist(color='green',bins=40,figsize=(8,4))
59/1:
import numpy as np
import panads as pd
import seaborn as sns
import matplotlib.pyplot as plt 
%matplotlib inline
59/2:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt 
%matplotlib inline
59/3: ad_data = pd.read_csv('advertising.csv')
59/4: ad_data.head()
59/5: ad_data['Age'].hist()
59/6: ad_data['Age'].hist(bin = 30)
59/7: ad_data['Age'].hist(bins = 30)
59/8: sns.jointplot(x = 'Age', y = 'Area Income', data = ad_data, )
59/9: sns.jointplot(x = 'Age', y = 'Area Income', data = ad_data )
59/10: sns.jointplot(x = 'Age', y = 'Area Income', data = ad_data, kind = 'kde' )
59/11: sns.pairplot(ad_data, hue = 'Clicked on Ad')
59/12: from sklearn.model_selection import train_test_split
59/13:
X = ad_data[['Daily Time Spent on Site', 'Age', 'Area Income','Daily Internet Usage', 'Male']]
y = ad_data['Clicked on Ad']
59/14: from sklearn.model_selection import train_test_split
59/15: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 42)
59/16: from sklearn.linear_model import LogisticRegression
59/17: lr = LogisticRegression()
59/18: lr.fit(X_train, y_train)
59/19: y_est = lr.predict(X_test)
59/20: print(classification_report(y_test,predictions))
59/21: from sklearn.metrics import classification_report
59/22: print(classification_report(y_test,predictions))
59/23: print(classification_report(y_test, y_est))
59/24:
from sklearn.metrics import confusion_matrix
confusion_matrix(y_true, y_pred)
59/25:
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_pred)
59/26:
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test, y_est)
59/27: 156/(156+6)
59/28: 156/(156+ 24)
61/1:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
61/2: df = pd.read_csv("Classified Data",index_col=0)
61/3: df.head()
61/4: from sklearn.preprocessing import StandardScaler
61/5: scaler = StandardScaler()
61/6: scaler.fit(df.drop('TARGET CLASS',axis=1))
61/7: scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))
61/8:
df_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])
df_feat.head()
61/9: from sklearn.model_selection import train_test_split
61/10:
X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['TARGET CLASS'],
                                                    test_size=0.30)
61/11: from sklearn.neighbors import KNeighborsClassifier
61/12: knn = KNeighborsClassifier(n_neighbors=1)
61/13: knn.fit(X_train,y_train)
61/14: pred = knn.predict(X_test)
61/15: from sklearn.metrics import classification_report,confusion_matrix
61/16: print(confusion_matrix(y_test,pred))
61/17: print(classification_report(y_test,pred))
61/18:
error_rate = []

# Will take some time
for i in range(1,40):
    
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
61/19:
error_rate = []

# Will take some time
for i in range(1,40):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
61/20:
plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
61/21:
error_rate = []
# Will take some time
for i in range(1,40):
    knn = KNeighborsClassifier(n_neighbors=i)
    knn.fit(X_train,y_train)
    pred_i = knn.predict(X_test)
    error_rate.append(np.mean(pred_i != y_test))
61/22:
# FIRST A QUICK COMPARISON TO OUR ORIGINAL K=1
knn = KNeighborsClassifier(n_neighbors=1)

knn.fit(X_train,y_train)
pred = knn.predict(X_test)

print('WITH K=1')
print('\n')
print(confusion_matrix(y_test,pred))
print('\n')
print(classification_report(y_test,pred))
61/23:
# NOW WITH K=23
knn = KNeighborsClassifier(n_neighbors=23)

knn.fit(X_train,y_train)
pred = knn.predict(X_test)

print('WITH K=23')
print('\n')
print(confusion_matrix(y_test,pred))
print('\n')
print(classification_report(y_test,pred))
62/1:
import pandas as pd
import seaborn as sns
62/2: df = pd.read_csv('KNN_Project_Data')
62/3: df.head()
62/4: sns.pairplot(df, hue = 'TARGET CLASS')
62/5: from sklearn.preprocessing import StandardScaler
62/6: scaler = StandardScaler(df)
62/7: scaler.fit(df.drop('TARGET CLASS', axis = 1))
62/8: scaler.fit(df.drop('TARGET CLASS',axis=1))
62/9: scaler = StandardScaler()
62/10: scaler.fit(df.drop('TARGET CLASS',axis=1))
62/11: scaled_features = scaler.transform(df.drop('TARGET CLASS',axis=1))
62/12:
scale_features = pd.DataFrame(scaled_features)
scale_features.head()
62/13:
scale_features = pd.DataFrame(scaled_features, columns = df.columns[:-1])
scale_features.head()
62/14: from sklearn.model_selection import train_test_split
62/15: X_train, X_test, y_train, y_test = train_test_split( scale_features, df['TARGET CLASS'], test_size = 0.3)
62/16: from sklearn.neighbors import KNeighborsClassifier
62/17: kn = KNeighborsClassifier(n_neighbors=1)
62/18: kn.fit(X_train, y_train)
62/19: y_est = kn.predict(X_test)
62/20: confusion_matrix(y_test, y_est)
62/21: from sklearn.metrics import confusion_matrix
62/22: confusion_matrix(y_test, y_est)
62/23:
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
62/24: classification_report(y_test, y_est)
62/25: print(classification_report(y_test, y_est))
62/26:
error_rate = []
for i in range(1,40):
    kn = KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train, X_test)
    y_est = kn.predict(y_est)
    error_rate.append(np.mean(y_est != y_test))
62/27:
error_rate = []
for i in range(1,40):
    kn = KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train, y_train)
    y_est = kn.predict(y_est)
    error_rate.append(np.mean(y_est != y_test))
    
# error_rate = []

# # Will take some time
# for i in range(1,40):
    
#     knn = KNeighborsClassifier(n_neighbors=i)
#     knn.fit(X_train,y_train)
#     pred_i = knn.predict(X_test)
#     error_rate.append(np.mean(pred_i != y_test))
62/28:
error_rate = []
for i in range(1,40):
    kn = KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train, y_train)
    y_est = kn.predict(X_test)
    error_rate.append(np.mean(y_est != y_test))
    
# error_rate = []

# # Will take some time
# for i in range(1,40):
    
#     knn = KNeighborsClassifier(n_neighbors=i)
#     knn.fit(X_train,y_train)
#     pred_i = knn.predict(X_test)
#     error_rate.append(np.mean(pred_i != y_test))
62/29:
import pandas as pd
import seaborn as sns
import numpy as np
62/30:
error_rate = []
for i in range(1,40):
    kn = KNeighborsClassifier(n_neighbors=i)
    kn.fit(X_train, y_train)
    y_est = kn.predict(X_test)
    error_rate.append(np.mean(y_est != y_test))    
# error_rate = []

# # Will take some time
# for i in range(1,40):
    
#     knn = KNeighborsClassifier(n_neighbors=i)
#     knn.fit(X_train,y_train)
#     pred_i = knn.predict(X_test)
#     error_rate.append(np.mean(pred_i != y_test))
62/31:
plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
62/32:
import matplotlib.pyplot as plt
%matplotlib inline
plt.figure(figsize=(10,6))
plt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', marker='o',
         markerfacecolor='red', markersize=10)
plt.title('Error Rate vs. K Value')
plt.xlabel('K')
plt.ylabel('Error Rate')
62/33:
kn = KNeighborsClassifier(n_neighbors=30)
y_est = kn.predict(X_test)
from sklearn.metrics import confusion_matrix
62/34:
kn = KNeighborsClassifier(n_neighbors=30)
kn.fit(X_train, y_train)
y_est = kn.predict(X_test)
from sklearn.metrics import confusion_matrix
62/35:
kn = KNeighborsClassifier(n_neighbors=30)
kn.fit(X_train, y_train)
y_est = kn.predict(X_test)
from sklearn.metrics import confusion_matrix
62/36:
kn = KNeighborsClassifier(n_neighbors=30)
kn.fit(X_train, y_train)
y_est = kn.predict(X_test)
from sklearn.metrics import confusion_matrix
confusion_matrix(y_est)
62/37:
kn = KNeighborsClassifier(n_neighbors=30)
kn.fit(X_train, y_train)
y_est = kn.predict(X_test)
from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,y_est)
64/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
64/2:
from IPython.display import Image  
from sklearn.externals.six import StringIO  
from sklearn.tree import export_graphviz
import pydot 

features = list(df.columns[1:])
64/3:
from IPython.display import Image  
from sklearn.externals.six import StringIO  
from sklearn.tree import export_graphviz
import pydot 

features = list(df.columns[1:])
features
64/4:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
64/5: df = pd.read_csv('kyphosis.csv')
64/6: df.head()
64/7: sns.pairplot(df,hue='Kyphosis',palette='Set1')
64/8: from sklearn.model_selection import train_test_split
64/9:
X = df.drop('Kyphosis',axis=1)
y = df['Kyphosis']
64/10: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
64/11: from sklearn.tree import DecisionTreeClassifier
64/12: dtree = DecisionTreeClassifier()
64/13: dtree.fit(X_train,y_train)
64/14: predictions = dtree.predict(X_test)
64/15: from sklearn.metrics import classification_report,confusion_matrix
64/16: print(classification_report(y_test,predictions))
64/17: print(confusion_matrix(y_test,predictions))
64/18:
from IPython.display import Image  
from sklearn.externals.six import StringIO  
from sklearn.tree import export_graphviz
import pydot 

features = list(df.columns[1:])
features
64/19:
dot_data = StringIO()  
export_graphviz(dtree, out_file=dot_data,feature_names=features,filled=True,rounded=True)

graph = pydot.graph_from_dot_data(dot_data.getvalue())  
Image(graph[0].create_png())
65/1:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline
65/2: from sklearn.datasets import load_breast_cancer
65/3: cancer = load_breast_cancer()
65/4: cancer.keys()
65/5: cancer['feature_names']
65/6:
df_feat = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])
df_feat.info()
65/7: cancer['target']
65/8: df_target = pd.DataFrame(cancer['target'],columns=['Cancer'])
65/9: df.head()
65/10: df_target.head()
65/11: df.head()
65/12: df_feat.head()
65/13: from sklearn.model_selection import train_test_split
65/14: X_train, X_test, y_train, y_test = train_test_split(df_feat, np.ravel(df_target), test_size=0.30, random_state=101)
65/15: df_target.head()
65/16: np.ravel(df_target.head())
65/17: np.ravel(df_target.head)
65/18: X_train, X_test, y_train, y_test = train_test_split(df_feat, df_target, test_size=0.30, random_state=101)
65/19: from sklearn.svm import SVC
65/20: model = SVC()
65/21: model.fit(X_train,y_train)
65/22: predictions = model.predict(X_test)
65/23: from sklearn.metrics import classification_report,confusion_matrix
65/24: print(confusion_matrix(y_test,predictions))
65/25: print(classification_report(y_test,predictions))
65/26: param_grid = {'C': [0.1,1, 10, 100, 1000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}
65/27: from sklearn.model_selection import GridSearchCV
65/28: grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=3)
65/29:
# May take awhile!
grid.fit(X_train,y_train)
65/30: grid.best_params_
65/31: grid.best_estimator_
65/32: grid_predictions = grid.predict(X_test)
65/33: print(confusion_matrix(y_test,grid_predictions))
65/34: print(classification_report(y_test,grid_predictions))
66/1:
# The Iris Setosa
from IPython.display import Image
url = 'http://upload.wikimedia.org/wikipedia/commons/5/56/Kosaciec_szczecinkowaty_Iris_setosa.jpg'
Image(url,width=300, height=300)
66/2:
# The Iris Versicolor
from IPython.display import Image
url = 'http://upload.wikimedia.org/wikipedia/commons/4/41/Iris_versicolor_3.jpg'
Image(url,width=300, height=300)
66/3:
# The Iris Virginica
from IPython.display import Image
url = 'http://upload.wikimedia.org/wikipedia/commons/9/9f/Iris_virginica.jpg'
Image(url,width=300, height=300)
66/4: import seaborn as sns
66/5:
import seaborn as sns
iris = sns.load_dataset('iris')
66/6: sns.pairplot(iris)
66/7: sns.pairplot(iris, hue = 'species')
66/8: sns.pairplot(iris, hue = 'species', kbind = 'bin')
66/9: sns.pairplot(iris, hue = 'species', kbind = hist)
66/10: sns.pairplot(iris, hue = 'species', kind = hist)
66/11: sns.pairplot(iris, hue = 'species')
66/12: sns.kdeplot(x='sepal_width', y='sepal_length')
66/13: sns.kdeplot(x='sepal_width', y='sepal_length',data = iris)
66/14:
setosa = iris[iris['species']=='setosa']
sns.kdeplot( setosa['sepal_width'], setosa['sepal_length'],
                 cmap="plasma", shade=True, shade_lowest=False)
66/15: from sklearn.model_selection import train_test_split
66/16: X_train, X_test, y_train, y_test = train_test_split(iris.drop('species', axis =1), iris['species'])
66/17: from sklearn.svm import svm
66/18: X_train, X_test, y_train, y_test = train_test_split(iris.drop('species', axis =1), iris['species'], test_size = 0.3)
66/19: from sklearn.svm import SVC
66/20: svc_model = SVC()
66/21: svc_model.fit(X_train, y_train)
66/22: y_est = svc_model.predict(X_test)
66/23: from sklearn.metrics import classification_report, confusion_matrix
66/24: confusion_matix(y_test, y_est)
66/25: confusion_matrix(y_test, y_est)
66/26: print(confusion_matrix(y_test, y_est))
66/27: print(classification_report(y_test, y_est))
66/28: from sklearn.model_selection import GridSearchCV
66/29: param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001]}
66/30:
grid = GridSearchCV(SVC(),param_grid,refit=True)
grid.fit(X_train,y_train)
66/31:
grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)
grid.fit(X_train,y_train)
66/32:
grid = GridSearchCV(SVC(),param_grid,refit=True)
grid.fit(X_train,y_train)
66/33:
grid = GridSearchCV(SVC(),param_grid,refit=True,verbose=2)
grid.fit(X_train,y_train)
66/34:
grid = GridSearchCV(SVC(),param_grid,cv = 5,refit=True,verbose=2)
grid.fit(X_train,y_train)
66/35:
grid = GridSearchCV(SVC(),param_grid,cv = 10,refit=True,verbose=2)
grid.fit(X_train,y_train)
66/36: grid_predictions = grid.predict(X_test)
66/37: confusion_matrix(y_test, grid_predictions)
66/38: print(classification_report(y_test, grid_predictions))
68/1:
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
68/2: from sklearn.datasets import make_blobs
68/3:
# Create Data
data = make_blobs(n_samples=200, n_features=2, 
                           centers=4, cluster_std=1.8,random_state=101)
68/4: data
68/5: data[0]
68/6: data[1]
68/7: data[2]
68/8: data[0]
68/9: data[0].shape()
68/10: shape(data[0])
68/11: data[0].size()
68/12: data[0].size
68/13: data[0].shape
68/14: plt.scatter(data[0][:,0],data[0][:,1],c=data[1],cmap='rainbow')
68/15: from sklearn.cluster import KMeans
68/16: kmeans = KMeans(n_clusters=4)
68/17: kmeans.fit(data[0])
68/18: kmeans.fit(data[0])
68/19: kmeans.cluster_centers_
68/20: kmeans.labels_
70/1:
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns
%matplotlib inline
70/2: from sklearn.datasets import load_breast_cancer
70/3: cancer = load_breast_cancer()
70/4: cancer.keys()
70/5: print(cancer['DESCR'])
70/6:
df = pd.DataFrame(cancer['data'],columns=cancer['feature_names'])
#(['DESCR', 'data', 'feature_names', 'target_names', 'target'])
70/7: df.head()
70/8: from sklearn.preprocessing import StandardScaler
70/9:
scaler = StandardScaler()
scaler.fit(df)
70/10: scaled_data = scaler.transform(df)
70/11: from sklearn.decomposition import PCA
70/12: pca = PCA(n_components=2)
70/13: pca.fit(scaled_data)
70/14: x_pca = pca.transform(scaled_data)
70/15: scaled_data.shape
70/16: x_pca.shape
70/17:
plt.figure(figsize=(8,6))
plt.scatter(x_pca[:,0],x_pca[:,1],c=cancer['target'],cmap='plasma')
plt.xlabel('First principal component')
plt.ylabel('Second Principal Component')
70/18: pca.components_
70/19: df_comp = pd.DataFrame(pca.components_,columns=cancer['feature_names'])
70/20:
plt.figure(figsize=(12,6))
sns.heatmap(df_comp,cmap='plasma',)
70/21: df_comp.head()
71/1:
import numpy as np
import pandas as pd
71/2:
column_names = ['user_id', 'item_id', 'rating', 'timestamp']
df = pd.read_csv('u.data', sep='\t', names=column_names)
71/3: df.head()
71/4:
column_names = ['user_id', 'item_id', 'rating', 'timestamp']
df = pd.read_csv('u.data', sep='\t', names=column_names)
71/5: df.head()
71/6:
movie_titles = pd.read_csv("Movie_Id_Titles")
movie_titles.head()
71/7:
df = pd.merge(df,movie_titles,on='item_id')
df.head()
71/8:
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('white')
%matplotlib inline
71/9: df.groupby('title')['rating'].mean().sort_values(ascending=False).head()
71/10:
plt.figure(figsize=(10,4))
ratings['num of ratings'].hist(bins=70)
71/11:
plt.figure(figsize=(10,4))
ratings['rating'].hist(bins=70)
71/12:
import numpy as np
import pandas as pd
71/13:
column_names = ['user_id', 'item_id', 'rating', 'timestamp']
df = pd.read_csv('u.data', sep='\t', names=column_names)
71/14: df.head()
71/15:
movie_titles = pd.read_csv("Movie_Id_Titles")
movie_titles.head()
71/16:
df = pd.merge(df,movie_titles,on='item_id')
df.head()
71/17:
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('white')
%matplotlib inline
71/18: df.groupby('title')['rating'].mean().sort_values(ascending=False).head()
71/19: df.groupby('title')['rating'].count().sort_values(ascending=False).head()
71/20:
ratings = pd.DataFrame(df.groupby('title')['rating'].mean())
ratings.head()
71/21:
ratings['num of ratings'] = pd.DataFrame(df.groupby('title')['rating'].count())
ratings.head()
71/22:
plt.figure(figsize=(10,4))
ratings['num of ratings'].hist(bins=70)
71/23:
plt.figure(figsize=(10,4))
ratings['rating'].hist(bins=70)
71/24: sns.jointplot(x='rating',y='num of ratings',data=ratings,alpha=0.5)
71/25:
moviemat = df.pivot_table(index='user_id',columns='title',values='rating')
moviemat.head()
71/26: ratings.sort_values('num of ratings',ascending=False).head(10)
71/27: ratings.head()
71/28: ratings.head()
71/29:
starwars_user_ratings = moviemat['Star Wars (1977)']
liarliar_user_ratings = moviemat['Liar Liar (1997)']
starwars_user_ratings.head()
71/30:
starwars_user_ratings = moviemat['Star Wars (1977)']
liarliar_user_ratings = moviemat['Liar Liar (1997)']
starwars_user_ratings.head()
71/31:
similar_to_starwars = moviemat.corrwith(starwars_user_ratings)
similar_to_liarliar = moviemat.corrwith(liarliar_user_ratings)
71/32:
corr_starwars = pd.DataFrame(similar_to_starwars,columns=['Correlation'])
corr_starwars.dropna(inplace=True)
corr_starwars.head()
71/33:
corr_starwars = pd.DataFrame(similar_to_starwars,columns=['Correlation'])
corr_starwars.dropna(inplace=True)
corr_starwars.head()
71/34: moviemat.head()
71/35: similar_to_liarliar.head()
71/36: similar_to_liarliar.head(10)
71/37:
starwars_user_ratings = moviemat['Star Wars (1977)']
liarliar_user_ratings = moviemat['Liar Liar (1997)']
starwars_user_ratings.head(10)
73/1: import tensorflow as tf
73/2: print(tf.__version__)
73/3: hello = tf.constant('Hello World')
73/4: type(hello)
73/5: x = tf.constant(100)
73/6: type(x)
73/7: sess = tf.Session()
76/1:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
76/2:
dataset = pd.read_csv('Churn_Modelling.csv')
X = dataset.iloc[:, 3:13].values
y = dataset.iloc[:, -1].values
# Encoding the Independent Variable
X_dummy = pd.get_dummies(X, drop_first=True)
76/3:
dataset = pd.read_csv('Churn_Modelling.csv')
data_dummy = pd.get_dummies(dataset, drop_first = False)
76/4:
dataset = pd.read_csv('Churn_Modelling.csv')
data_dummy = pd.get_dummies(dataset, drop_first = True)
76/5: data_dummy.head()
76/6:
dataset = pd.read_csv('Churn_Modelling.csv')
dataset = dataset.drop(['RowNumber', 'CustomerId'])
data_dummy = pd.get_dummies(dataset, drop_first = True)
76/7:
dataset = pd.read_csv('Churn_Modelling.csv')
dataset = dataset.drop(['RowNumber', 'CustomerId'], axis = 1)
data_dummy = pd.get_dummies(dataset, drop_first = True)
76/8: data_dummy.head()
76/9:
dataset = pd.read_csv('Churn_Modelling.csv')
dataset = dataset.drop(['RowNumber', 'CustomerId', 'Surname'], axis = 1)
data_dummy = pd.get_dummies(dataset, drop_first = True)
76/10: data_dummy.head()
76/11:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data_dummy.drop('Exited', axis = 1), data_dummy['Exited'], test_size = 0.34, random_state = 1)
76/12:
from sklearn.preprocessing import StandardScaler
sc = StandardScaler)_
76/13:
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit_transform(X_train)
X_test = sc.transform(X_test)
76/14:
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
sc.fit(data_dummy.drop('Exited', axis = 1))
data_scaled = sc.transform(data_dummy.drop('Exited', axis = 1))
76/15:
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data_scaled, data_dummy['Exited'], test_size = 0.34, random_state = 1)
76/16: import keras
76/17:
import keras
from keras.model import Sequential
from keras.layers import Dense
76/18:
import keras
from keras.models import Sequential
from keras.layers import Dense
76/19:
classifier = Sequential()
classifier.add(Dense(output_dim = 6,init = 'uniform', activation = 'relu', input_dim = 11))
76/20:
classifier = Sequential()
classifier.add(Dense(output_dim = 6,init = 'uniform', activation = 'relu', input_dim = 11))
classifier.add(Dense(output_dim = 6,init = 'uniform', activation = 'relu'))
76/21:
classifier = Sequential()
classifier.add(Dense(output_dim = 6,init = 'uniform', activation = 'relu', input_dim = 11))
classifier.add(Dense(output_dim = 6,init = 'uniform', activation = 'relu'))
classifier.add(Dense(output_dim = 1,init = 'uniform', activation = 'sigmoid'))
76/22:
classifier = Sequential()
classifier.add(Dense(output_dim = 6,init = 'uniform', activation = 'relu', input_dim = 11))
classifier.add(Dense(output_dim = 6,init = 'uniform', activation = 'relu'))
classifier.add(Dense(output_dim = 1,init = 'uniform', activation = 'sigmoid'))
classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])
76/23: classifier.fit(X_train, y_train, batch_size= 10, np_epoch = 100)
76/24: classifier.fit(X_train, y_train, batch_size= 10, epochs = 100)
76/25:
y_est = classifier.predict(X_test)
from sklearn.metrics import confusion_matrix
print(confusion_matrix)
76/26:
y_est = classifier.predict(X_test)
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, y_est))
76/27:
y_est = classifier.predict(X_test)
y_est = (y_est > 0.5)
from sklearn.metrics import confusion_matrix
print(confusion_matrix(y_test, y_est))
76/28:
y_est = classifier.predict(X_test)
y_est = (y_est > 0.5)
from sklearn.metrics import confusion_matrix, classification_report
print(confusion_matrix(y_test, y_est))
print("\n")
print(classification_report(y_test, y_est))
76/29:
new_pred = classifier(sc.transform([0,0,600,1, 40,3, 60000,2, 1,1,50000]))
new_pred = (new_pred > 0.5)
76/30:
new_pred = classifier(sc.transform(np.array[[0,0,600,1, 40,3, 60000,2, 1,1,50000]]))
new_pred = (new_pred > 0.5)
76/31: test_employee = np.array([[0,0,600,1,40,3,60000,2,1,1,50000]])
76/32: test_employee
76/33:
new_pred = classifier(sc.transform(test_employee))
new_pred = (new_pred > 0.5)
76/34: test_employee = [0,0,600,1,40,3,60000,2,1,1,50000]
76/35: test_employee
76/36:
new_pred = classifier(sc.transform(test_employee))
new_pred = (new_pred > 0.5)
76/37: new_pred = classifier(sc.transform(test_employee))
76/38: new_pred = classifier.predict(sc.transform(test_employee))
76/39: test_employee.shape
76/40: test_employee.size
76/41: test_employee.size()
76/42: X_test.head()
76/43: X_test.head
76/44: X_test
76/45: shape(X_test)
76/46: size(X_test)
76/47: X_test.size
76/48: X_test.shape
76/49: test_employee.shape
76/50: test_employee = np.array([[0,0,600,1,40,3,60000,2,1,1,50000]])
76/51: test_employee.shape
76/52: sc.transform
76/53: sc.transform.shape
76/54: sc.transform(test_employee)
76/55: te = sc.transform(test_employee)
76/56: new_pred = classifier.predict(te)
76/57: new_pred = classifier.predict(sc.transform(test_employee))
76/58:
new_pred = classifier.predict(sc.transform(test_employee))
new_pred = (new_pred > 0.5)
76/59: print(new_pred)
80/1:
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.parallelrallel
80/2:
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.parallel
import torch.optim as optim 
import torch.utils.data
from torch.autograd import Variable
80/3: movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header =None, engine = 'python', encoding = 'latin-1')
80/4: movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header =None, engine = 'python', encoding = 'latin-1')
80/5: users = pd.read_csv('ml-1m/users.dat', sep = '::', header =None, engine = 'python', encoding = 'latin-1')
80/6: ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header =None, engine = 'python', encoding = 'latin-1')
80/7: training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\t')
80/8:
training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\t')
training_set = np.array(training_set, dtype = 'int')
80/9:
training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\t')
training_set = np.array(training_set, dtype = 'int')
test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\t')
test_set = np.array(test_set, dtype = 'int')
80/10:
nb_users = int(max(max(training_set[:,0], max(test_set[:,0]))))
nb_movies = int(max(max(training_set[:,1], max(test_set[:,1]))))
80/11:
nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))
nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))
80/12:
def convert(data):
    new_data = []
    for id_users in range(1, nb_users+1):
80/13:
def convert(data):
    new_data = []
    for id_users in range(1, nb_users+1):
        id_movies = data[:,1][data[:,0] == id_users]
        id_ratings = data[:,2][data[:,0] == id_users]
        ratings = np.zeros(nb_movies)
        ratings[id_movies - 1] = id_ratings
        new_data.append(list(ratings))
    return new_data
training_set = convert(training_set)
test_set = convert(test_set)
80/14:
training_set = torch.FloatTensor(training_set)
test_set = torch.FloatTensor(test_set)
80/15:
class SAE(nn.Module):
    def __init__(self,):
        super(SAE, self).__init__()
        self.fc1 = nn.Linear(nb_movies, 20)
        self.fc2 = nn.Linear(20, 10)
        self.fc3 = nn.Linear(10, 20)
        self.fc4 = nn.Linear(20, nb_movies)
        self.activation = nn.Sigmoid()
        
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        x = self.fc4(x)
        return x 
sae = SAE()
80/16:
class SAE(nn.Module):
    def __init__(self,):
        super(SAE, self).__init__()
        self.fc1 = nn.Linear(nb_movies, 20)
        self.fc2 = nn.Linear(20, 10)
        self.fc3 = nn.Linear(10, 20)
        self.fc4 = nn.Linear(20, nb_movies)
        self.activation = nn.Sigmoid()
        
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        x = self.fc4(x)
        return x 
sae = SAE() 
criterion = MSELoss()
optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay =0.5)
80/17:
class SAE(nn.Module):
    def __init__(self,):
        super(SAE, self).__init__()
        self.fc1 = nn.Linear(nb_movies, 20)
        self.fc2 = nn.Linear(20, 10)
        self.fc3 = nn.Linear(10, 20)
        self.fc4 = nn.Linear(20, nb_movies)
        self.activation = nn.Sigmoid()
        
    def forward(self, x):
        x = self.activation(self.fc1(x))
        x = self.activation(self.fc2(x))
        x = self.activation(self.fc3(x))
        x = self.fc4(x)
        return x 
sae = SAE() 
criterion = nn.MSELoss()
optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay =0.5)
82/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
82/2:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
82/3:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
82/4: irs.head()
82/5: irs.head(5)
82/6: asc.head(5)
82/7: acs.head(5)
82/8: irs.describe()
82/9: print(irs.shape)
82/10: irs.info()
82/11: irs[agi_stub][0]
82/12: irs[agi_stub]
82/13: irs['agi_stub'][0]
82/14: irs['agi_stub'][0] == 1
82/15: irs['agi_stub'].shape
82/16: irs['agi_stub'].shape[0]
82/17:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1:[0,25000], 2:[25000,50000], 3: [50000,75000],
        4:[75000,100000], 5:[100000,200000], 6:[200000, float(inf)]}
    for i in range(agi_stub.shape[0]):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res
check_agi(irs['amount_agi'], irs['agi_stub'])
82/18:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1:[0,25000], 2:[25000,50000], 3: [50000,75000],
        4:[75000,100000], 5:[100000,200000], 6:[200000, 4.439737e+08]}
    for i in range(agi_stub.shape[0]):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res
check_agi(irs['amount_agi'], irs['agi_stub'])
82/19:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1:[0,25000], 2:[25000,50000], 3: [50000,75000],
        4:[75000,100000], 5:[100000,200000], 6:[200000, 4.439737e+08]}
    for i in range(agi_stub.shape[0]):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
82/20:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1:[0,25000], 2:[25000,50000], 3: [50000,75000],
        4:[75000,100000], 5:[100000,200000], 6:[200000, 4.439737e+08]}
    for i in range(agi_stub.shape[0]):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
82/21:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1:[0,25000], 2:[25000,50000], 3: [50000,75000],
        4:[75000,100000], 5:[100000,200000], 6:[200000, 4.439737e+08]}
    for i in range(10):
        print(amount_agi[i], agi_stub[i])
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
82/22:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1:[0,25000], 2:[25000,50000], 3: [50000,75000],
        4:[75000,100000], 5:[100000,200000], 6:[200000, 4.439737e+08]}
    for i in range(10):
        print(amount_agi[i], agi_stub[i], dic[agi_stub[i]])
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
82/23:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1:[0,25000], 2:[25000,50000], 3: [50000,75000],
        4:[75000,100000], 5:[100000,200000], 6:[200000, 4.439737e+08]}
    for i in range(10):
        print(amount_agi[i], agi_stub[i], dic[agi_stub[i]])
        if int(amount_agi[i]) not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
82/24:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1:[0,25000], 2:[25000,50000], 3: [50000,75000],
        4:[75000,100000], 5:[100000,200000], 6:[200000, 4.439737e+08]}
    for i in range(10):
        print(amount_agi[i], agi_stub[i], dic[agi_stub[i]])
        if int(amount_agi[i]) not in dic[agi_stub[i]]:
            print(1)
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
82/25:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1:[0,25000], 2:[25000,50000], 3: [50000,75000],
        4:[75000,100000], 5:[100000,200000], 6:[200000, 4.439737e+08]}
    for i in range(10):
        print(amount_agi[i], agi_stub[i], dic[agi_stub[i]])
        if (amount_agi[i]) in dic[agi_stub[i]]:
            print(1)
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
82/26:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0,25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 4.439737e+08)}
    for i in range(10):
        print(amount_agi[i], agi_stub[i], dic[agi_stub[i]])
        if amount_agi[i] not in dic[agi_stub[i]]:
            print(1)
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
82/27:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0,25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, float(inf))}
    for i in range(10):
        print(amount_agi[i], agi_stub[i], dic[agi_stub[i]])
        if amount_agi[i] not in dic[agi_stub[i]]:
            print(1)
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
82/28:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0,25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, float('inf'))}
    for i in range(10):
        print(amount_agi[i], agi_stub[i], dic[agi_stub[i]])
        if amount_agi[i] not in dic[agi_stub[i]]:
            print(1)
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
82/29:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0,25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        print(amount_agi[i], agi_stub[i], dic[agi_stub[i]])
        if amount_agi[i] not in dic[agi_stub[i]]:
            print(1)
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
82/30:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0,25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        print(amount_agi[i], agi_stub[i], dic[agi_stub[i]])
        if amount_agi[i] not in dic[agi_stub[i]]:
            print(1)
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
82/31:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0,25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(amount_agi.shape[0]):
        print(amount_agi[i], agi_stub[i], dic[agi_stub[i]])
        if amount_agi[i] not in dic[agi_stub[i]]:
            print(1)
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
83/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
83/2:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
83/3: irs.head(5)
83/4: acs.head(5)
83/5: print(irs.shape)
83/6: irs.info()
83/7: irs.describe()
83/8: irs['agi_stub'].shape[0]
83/9:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0,25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(amount_agi.shape[0]):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
84/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
84/2:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
84/3: irs.head(5)
84/4: acs.head(5)
84/5: print(irs.shape)
84/6: irs.info()
84/7: irs.describe()
84/8: irs['agi_stub'].shape[0]
84/9:
def check_agi(amount_agi, agi_stub):
    res = []
    new = []
    dic = {range(0,25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
            new.append()
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
84/10:
def check_agi(amount_agi, agi_stub):
    res = []
    new = []
    dic = {range(0,25000):1, 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
            new.append()
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
84/11:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {range(0, 25000):1, 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
84/12:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
84/13:
list_ = [25000, 50000, 75000, 100000, 200000,2**31]
names = [1, 2, 3, 4, 5, 6]
df['new_agi_stub'] = pd.cut(df['amount_agi'], list_, names)
84/14:
list_ = [25000, 50000, 75000, 100000, 200000,2**31]
names = [1, 2, 3, 4, 5, 6]
df['new_agi_stub'] = pd.cut(irs['amount_agi'], list_, names)
84/15:
list_ = [25000, 50000, 75000, 100000, 200000,2**31]
names = [1, 2, 3, 4, 5, 6]
irs['new_agi_stub'] = pd.cut(irs['amount_agi'], list_, names)
84/16: irs['new_agi_stub']
84/17: irs.head()
84/18:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs['new_agi_stub'] = pd.cut(irs['amount_agi'], list_, names)
84/19: irs.head()
84/20:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs['new_agi_stub'] = pd.cut(irs['amount_agi'], list_, labels = names)
84/21: irs.head()
84/22: irs.head(10)
84/23:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs['new_agi_stub'] = pd.cut(irs['amount_agi'], list_, labels = names)
84/24: irs.head()
84/25: irs = irs.drop('zip', axis =1)
84/26: irs = irs.drop('zipcode', axis =1)
84/27:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
84/28:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
84/29:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs['new_agi_stub'] = pd.cut(irs['amount_agi'], list_, labels = names)
84/30: irs_new = irs.drop(['agi_stub', 'amount_agi', axis =1)
84/31: irs_new = irs.drop(['agi_stub', 'amount_agi'], axis =1)
84/32: irs_new.head()
84/33: irs_new.head(10)
84/34: print(irs.groupby(['state']).size())
84/35: print(irs.groupby('State').size())
84/36: print(irs.groupby('STATE').size())
84/37: print(irs['STATE'].value_counts())
84/38: acs.describe()
84/39:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
84/40:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
84/41: irs.head(5)
84/42: acs.head(5)
84/43: print(irs.shape)
84/44: irs.info()
84/45: irs.describe()
84/46: irs2 = irs.drop('zipcode', axis =1)
84/47: irs2.head()
84/48:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
84/49:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names)
84/50: irs_new = irs2.drop(['agi_stub', 'amount_agi'], axis =1)
84/51: irs_new.head(10)
84/52: joint_data = pd.merge(irs, acs, how='left', on=['zipcode'])
84/53: joint_data = pd.merge(irs, acs, how='left', left_on=['zipcode'],  right_on=['ZIP'])
84/54: joint_data.head()
84/55:
joint_data = pd.merge(irs, acs, how='inner', 
                      left_on=['zipcode'],  right_on=['ZIP'])
84/56: joint_data.head()
84/57: joint_data.groupby('STATE').value_counts()
84/58: joint_data['STATE'].value_counts()
84/59: irs_state = irs['STATE'].value_counts()
84/60: print(irs_state)
84/61: joint_zip = joint_data['STATE'].value_counts()
84/62: zip_check = pd.merge(irs_state, joint_zip, how = 'left')
84/63:
zip_check = pd.merge(irs_state, joint_zip, how = 'left')
print(zip_checl)
84/64:
zip_check = pd.merge(irs_state, joint_zip, how = 'left')
print(zip_check)
84/65:
zip_check = pd.merge(irs_state), joint_zip, how = 'left', on = 'zipcode')
print(zip_check)
84/66:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', on = 'zipcode')
print(zip_check)
84/67:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', on = 'zipcode')
zip_check.head()
84/68:
joint_zip = joint_data['STATE'].value_counts()
print(joint_zip)
84/69:
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
joint_zip
84/70:
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts(), columns=['state', 'cnt'])
joint_zip
84/71:
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts(), colums =['cnt']).resetindex()
joint_zip =
84/72: joint_zip = pd.DataFrame(joint_data['STATE'].value_counts(), colums =['cnt']).resetindex()
84/73: joint_zip = pd.DataFrame(joint_data['STATE'].value_counts(), columns =['cnt']).resetindex()
84/74: joint_zip = pd.DataFrame(joint_data['STATE'].value_counts(), columns =['cnt']).set_index('state')
84/75: joint_zip = pd.DataFrame(joint_data['STATE'].value_counts()).set_index('state')
84/76: joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
84/77: joint_zip = pd.DataFrame(joint_data['STATE'].value_counts()).reset_index()
84/78: joint_zip = pd.DataFrame(joint_data['STATE'].value_counts(), columns =['cnt']).reset_index()
84/79:
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts(), columns =['cnt']).reset_index()
joint_zip.head()
84/80:
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts(), columns =['cnt']).reset_index()
joint_zip.head(10)
84/81:
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts(),).reset_index()
joint_zip.head(10)
84/82: joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
84/83: irs_state = pd.DataFrame(irs['STATE'].value_counts())
84/84: irs_state.head()
84/85:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
print(zip_check)
84/86:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
print(zip_check)
84/87:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
print(zip_check)
84/88:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
84/89:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
84/90: irs.head(5)
84/91: acs.head(5)
84/92: print(irs.shape)
84/93: irs.info()
84/94: irs.describe()
84/95: irs2 = irs.drop('zipcode', axis =1)
84/96: irs2.head()
84/97:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
84/98:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names)
84/99: irs_new = irs2.drop(['agi_stub', 'amount_agi'], axis =1)
84/100: irs_new.head(10)
84/101: irs_state = pd.DataFrame(irs['STATE'].value_counts())
84/102: irs_state.head()
84/103: acs.describe()
84/104:
joint_data = pd.merge(irs, acs, how='inner', 
                      left_on=['zipcode'],  right_on=['ZIP'])
84/105: joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
84/106:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
print(zip_check)
84/107:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = zip_check['STATE_y']/ zip_check['STATE_x']
zip_check.head()
84/108:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
zip_check.head(all)
84/109:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
84/110:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(mean(zip_check))
84/111:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
84/112:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
print(min(zip_check['percent']))
84/113:
joint_data = pd.merge(irs, acs, how='inner', 
                      left_on=['zipcode'],  right_on=['ZIP'])
84/114:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
print(min(zip_check['percent']))
84/115:
joint_data = pd.merge(irs, acs, how='inner', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
84/116:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
print(min(zip_check['percent']))
87/1:
import plotly.plotly as py
import plotly.graph_objs as go 
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
87/2:
import plotly.plotly as py
import plotly.graph_objs as go 
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
87/3:
import plotly.plotly as py
import plotly.graph_objs as go 
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
87/4:
import chart_studiot.plotly as py
import chart_studiot.graph_objs as go 
from chart_studiot.offline import download_plotlyjs, init_notebook_mode, plot, iplot
87/5:
import chart_studio.plotly as py
import chart_studio.graph_objs as go 
from chart_studio.offline import download_plotlyjs, init_notebook_mode, plot, iplot
87/6:
import chart_studio.plotly as py
import plotly.graph_objs as go 
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
87/7: init_notebook_mode(connected=True)
87/8: import pandas as pd
87/9:
data = dict(type = 'choropleth',
            locations = ['AZ','CA','NY'],
            locationmode = 'USA-states',
            colorscale= 'Portland',
            text= ['text1','text2','text3'],
            z=[1.0,2.0,3.0],
            colorbar = {'title':'Colorbar Title'})
87/10: layout = dict(geo = {'scope':'usa'})
87/11: choromap = go.Figure(data = [data],layout = layout)
87/12: iplot(choromap)
87/13:
df = pd.read_csv('2011_US_AGRI_Exports')
df.head()
87/14:
data = dict(type='choropleth',
            colorscale = 'YIOrRd',
            locations = df['code'],
            z = df['total exports'],
            locationmode = 'USA-states',
            text = df['text'],
            marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),
            colorbar = {'title':"Millions USD"}
            )
87/15:
layout = dict(title = '2011 US Agriculture Exports by State',
              geo = dict(scope='usa',
                         showlakes = True,
                         lakecolor = 'rgb(85,173,240)')
             )
87/16: choromap = go.Figure(data = [data],layout = layout)
87/17:
data = dict(type='choropleth',
            colorscale = 'YIOrRd',
            locations = df['code'],
            z = df['total exports'],
            locationmode = 'USA-states',
            text = df['text'],
            marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),
            colorbar = {'title':"Millions USD"}
            )
87/18:
layout = dict(title = '2011 US Agriculture Exports by State',
              geo = dict(scope='usa',
                         showlakes = True,
                         lakecolor = 'rgb(85,173,240)')
             )
87/19: choromap = go.Figure(data = [data],layout = layout)
87/20: iplot(choromap)
87/21:
df = pd.read_csv('2014_World_GDP')
df.head()
87/22:
data = dict(
        type = 'choropleth',
        locations = df['CODE'],
        z = df['GDP (BILLIONS)'],
        text = df['COUNTRY'],
        colorbar = {'title' : 'GDP Billions US'},
      )
87/23:
layout = dict(
    title = '2014 Global GDP',
    geo = dict(
        showframe = False,
        projection = {'type':'Mercator'}
    )
)
87/24:
choromap = go.Figure(data = [data],layout = layout)
iplot(choromap)
87/25:
layout = dict(
    title = '2014 Global GDP',
    geo = dict(
        showframe = False,
        projection = {'type':'Mercator'}
    )
)
87/26:
choromap = go.Figure(data = [data],layout = layout)
iplot(choromap)
87/27:
layout = dict(title = '2011 US Agriculture Exports by State',
              geo = dict(scope='usa',
                         showlakes = True,)
             )
87/28:
layout = dict(title = '2011 US Agriculture Exports by State',
              geo = dict(scope='usa',
                         showlakes = True)
             )
87/29:
data = dict(type='choropleth',
            colorscale = 'YIOrRd',
            locations = df['code'],
            z = df['total exports'],
            locationmode = 'USA-states',
            text = df['text'],
            marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),
            colorbar = {'title':"Millions USD"}
            )
87/30:
df = pd.read_csv('2011_US_AGRI_Exports')
df.head()
87/31:
data = dict(type='choropleth',
            colorscale = 'YIOrRd',
            locations = df['code'],
            z = df['total exports'],
            locationmode = 'USA-states',
            text = df['text'],
            marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),
            colorbar = {'title':"Millions USD"}
            )
87/32:
layout = dict(title = '2011 US Agriculture Exports by State',
              geo = dict(scope='usa',
                         showlakes = True,
                         lakecolor = 'rgb(85,173,240)')
             )
87/33: choromap = go.Figure(data = [data],layout = layout)
87/34:
layout = dict(title = '2011 US Agriculture Exports by State',
              geo = dict(scope='usa',
                         showlakes = True,
                         lakecolor = 'aggrnyl')
             )
87/35: choromap = go.Figure(data = [data],layout = layout)
88/1:
import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode,iplot,plot
init_notebook_mode(connected=True)
88/2: import pandas as pd
88/3: df = pd.read_csv('2014_World_Power_Consumption')
88/4: df.head()
88/5:
data = dict(
        type = 'choropleth',
        colorscale = 'Viridis',
        reversescale = True,
        locations = df['Country'],
        locationmode = "country names",
        z = df['Power Consumption KWH'],
        text = df['Country'],
        colorbar = {'title' : 'Power Consumption KWH'},
      ) 

layout = dict(title = '2014 Power Consumption KWH',
                geo = dict(showframe = False,projection = {'type':'Mercator'})
             )
88/6:
choromap = go.Figure(data = [data],layout = layout)
plot(choromap,validate=False)
88/7: import pandas as pd
88/8: df = pd.read_csv('2014_World_Power_Consumption')
88/9: df.head()
88/10:
data = dict(
        type = 'choropleth',
        colorscale = 'Viridis',
        reversescale = True,
        locations = df['Country'],
        locationmode = "country names",
        z = df['Power Consumption KWH'],
        text = df['Country'],
        colorbar = {'title' : 'Power Consumption KWH'},
      ) 

layout = dict(title = '2014 Power Consumption KWH',
                geo = dict(showframe = False,projection = {'type':'Mercator'})
             )
88/11:
choromap = go.Figure(data = [data],layout = layout)
plot(choromap,validate=False)
88/12:
import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode,iplot,plot
init_notebook_mode(connected=True)
88/13: import pandas as pd
88/14: df = pd.read_csv('2014_World_Power_Consumption')
88/15: df.head()
88/16:
data = dict(
        type = 'choropleth',
        colorscale = 'Viridis',
        reversescale = True,
        locations = df['Country'],
        locationmode = "country names",
        z = df['Power Consumption KWH'],
        text = df['Country'],
        colorbar = {'title' : 'Power Consumption KWH'},
      ) 

layout = dict(title = '2014 Power Consumption KWH',
                geo = dict(showframe = False,projection = {'type':'Mercator'})
             )
88/17:
choromap = go.Figure(data = [data],layout = layout)
plot(choromap,validate=False)
88/18:
import plotly.graph_objs as go 
from plotly.offline import init_notebook_mode,iplot,plot
init_notebook_mode(connected=True)
88/19: import pandas as pd
88/20: df = pd.read_csv('2014_World_Power_Consumption')
88/21: df.head()
88/22:
data = dict(
        type = 'choropleth',
        colorscale = 'Viridis',
        reversescale = True,
        locations = df['Country'],
        locationmode = "country names",
        z = df['Power Consumption KWH'],
        text = df['Country'],
        colorbar = {'title' : 'Power Consumption KWH'},
      ) 

layout = dict(title = '2014 Power Consumption KWH',
                geo = dict(showframe = False,projection = {'type':'Mercator'})
             )
88/23:
choromap = go.Figure(data = [data],layout = layout)
plot(choromap,validate=False)
88/24:
choromap = go.Figure(data = [data],layout = layout)
plot(choromap,validate=False)
88/25:
data = dict(
        type = 'choropleth',
        colorscale = 'Viridis',
        reversescale = True,
        locations = df['Country'],
        locationmode = "country names",
        z = df['Power Consumption KWH'],
        text = df['Country'],
        colorbar = {'title' : 'Power Consumption KWH'},
      ) 

layout = dict(title = '2014 Power Consumption KWH',
                geo = dict(showframe = False,projection = {'type':'mercator'})
             )
88/26:
choromap = go.Figure(data = [data],layout = layout)
plot(choromap,validate=False)
88/27:
choromap = go.Figure(data = [data])
plot(choromap,validate=False)
87/36:
import chart_studio.plotly as py
import plotly.graph_objs as go 
from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot
87/37: init_notebook_mode(connected=True)
87/38: import pandas as pd
87/39:
data = dict(type = 'choropleth',
            locations = ['AZ','CA','NY'],
            locationmode = 'USA-states',
            colorscale= 'Portland',
            text= ['text1','text2','text3'],
            z=[1.0,2.0,3.0],
            colorbar = {'title':'Colorbar Title'})
87/40: layout = dict(geo = {'scope':'usa'})
87/41: choromap = go.Figure(data = [data],layout = layout)
87/42: iplot(choromap)
87/43:
df = pd.read_csv('2011_US_AGRI_Exports')
df.head()
87/44:
data = dict(type='choropleth',
            colorscale = 'YIOrRd',
            locations = df['code'],
            z = df['total exports'],
            locationmode = 'USA-states',
            text = df['text'],
            marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),
            colorbar = {'title':"Millions USD"}
            )
87/45:
data = dict(type='choropleth',
            colorscale = 'YIOrRd',
            locations = df['code'],
            z = df['total exports'],
            locationmode = 'USA-states',
            text = df['text'],
            marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),
            colorbar = {'title':"Millions USD"}
            )
87/46:
layout = dict(title = '2011 US Agriculture Exports by State',
              geo = dict(scope='usa',
                         showlakes = True,
                         lakecolor = 'rgb(85,173,240)')
             )
87/47: choromap = go.Figure(data = [data],layout = layout)
87/48:
data = dict(type='choropleth',
            colorscale = 'Portland',
            locations = df['code'],
            z = df['total exports'],
            locationmode = 'USA-states',
            text = df['text'],
            marker = dict(line = dict(color = 'rgb(255,255,255)',width = 2)),
            colorbar = {'title':"Millions USD"}
            )
87/49:
layout = dict(title = '2011 US Agriculture Exports by State',
              geo = dict(scope='usa',
                         showlakes = True,
                         lakecolor = 'rgb(85,173,240)')
             )
87/50: choromap = go.Figure(data = [data],layout = layout)
87/51: iplot(choromap)
84/117: irs_new.groupby('STATE')['new']
84/118: irs_new.groupby('STATE')['new_agi_stub']
84/119: irs_new.groupby('STATE')['new_agi_stub'].boxplot()
84/120: sns.boxplot(irs_new.groupby('STATE')['new_agi_stub'])
84/121: irs_new.groupby('STATE')['new_agi_stub'].head()
84/122: irs_new.groupby('STATE')
84/123:
irs_new.groupby('State')['new_agi_sub'].mean().plot.bar()
plt.show()
84/124:
irs_new.groupby('STATE')['new_agi_sub'].mean().plot.bar()
plt.show()
84/125: irs_new.head(10)
84/126:
irs_new.groupby('STATE')['new_agi_sTub'].mean().plot.bar()
plt.show()
84/127:
irs_new.groupby('STATE')['new_agi_stub'].mean().plot.bar()
plt.show()
84/128: irs_new.groupby(['STATE'])['new_agi_stub'].mean()
84/129: irs_new.groupby(['STATE'])
84/130: irs_new.groupby(['STATE']).head()
84/131: irs_new.groupby(['STATE'])['new_agi_stub']head()
84/132: irs_new.groupby(['STATE'])['new_agi_stub'].head()
84/133: irs_new.groupby(['STATE'])['new_agi_stub']
84/134: irs_new.groupby(['STATE'])['new_agi_stub'].mean()
89/1:
import pandas
data=pandas.read_csv("https://drive.google.com/uc?export=download&id=1KvxyyF3QCtvIx0J7_8iWDEtFQpLgd0Yq")
  
print(data.head())
89/2: print(data.describe())
89/3: print(data.groupby(['country'])['converted'])
89/4: print(data.groupby(['country'])['converted'].head())
89/5:
import matplotlib.pyplot as plt
%matplotlib inline
#from matplotlib import rcParams
#rcParams.update({'figure.autolayout': True})
data.groupby(['country'])['converted'].mean().plot.bar()
plt.show()
84/135: irs_new.groupby(['STATE'])['new_agi_stub'].mean()
84/136: irs_new.groupby(['STATE'])['new_agi_stub'].head()
84/137: irs_new.groupby(['new_agi_stub'])['STATE']
84/138: irs_new.groupby(['new_agi_stub'])['STATE'].head()
84/139: irs_new.groupby(['new_agi_stub'])['STATE'].plot()
84/140: irs_new.groupby(['STATE'])
84/141: irs_new.groupby(['STATE']).head()
84/142: irs_new.groupby(['STATE','new_agi_stub']).head()
84/143: irs_new.groupby(['STATE','new_agi_stub'])['new_agi_stub'].value_counts()
84/144: irs_new.groupby(['STATE'])['new_agi_stub'].value_counts()
84/145: state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts())
84/146:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts())
state_agi.head()
84/147: state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts(), columns=['cnt'])
84/148:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts(), 
                         columns=['cnt'])
state_agi.head()
84/149:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts(), 
                         columns=['state','agi','cnt'])
84/150:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts(), 
                         columns=['state','agi','cnt'])
state_agi
84/151:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts(), 
                         columns=['state','agi','cnt'])
state_agi.head()
84/152:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts(), 
                         columns=['state','agi','cnt'])
state_agi.set_index(['state','agi']).valie
84/153:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts(), 
                         columns=['state','agi','cnt'])
state_agi.set_index(['state','agi']).value
state_agi
84/154:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts(), 
                         columns=['state','agi','cnt'])
state_agi.set_index(['state','agi']).value
state_agi.head()
84/155:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts())
state_agi.set_index(['state','agi']).value
state_agi.head()
84/156:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts())
state_agi.head()
84/157:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts())
state_agi.index()
84/158:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts())
state_agi.mutliindex()
84/159:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts())
state_agi.reset_idex(name = 'count')
84/160:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts())
state_agi.reset_index(name = 'count')
84/161:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts())
state_agi.reset_index(name = 'counts')
84/162:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts()).reset_index(name = 'counts')
state_agi.reset_index(name = 'counts')
84/163:
state_agi = pd.DataFrame(irs_new.groupby(['STATE'])['new_agi_stub'].value_counts())
state_agi.rename('count')
84/164: state_agi = pd.DataFrame(irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index)_
84/165: state_agi = pd.DataFrame(irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
84/166: irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
84/167:
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
state_agi = state_agi.set_index(['STATE', 'new_agi_stub']).value
84/168:
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
state_agi = state_agi.set_index(['STATE', 'new_agi_stub']).value()
84/169:
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
84/170:
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
pivot_sa.plot.bar(stack=True)
84/171:
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
pivot_sa.head()
84/172:
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
irs_new.groupby('new_agi_stub')['STATE'].value_count().unstack(level =1).plot.bar(stacked = True)
84/173:
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
irs_new.groupby('new_agi_stub')['STATE'].value_counts().unstack(level =1).plot.bar(stacked = True)
84/174:
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.bar(stacked = True)
84/175:
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.bar(stacked = True, figsize(6,12))
84/176:
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')

irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/177:
from plt import rcParams
rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/178:

plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/179:
plt.rcParams['figure.figsize'] = 10, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/180:
plt.rcParams['figure.figsize'] = 10, 5
irs_new.groupby('new_agi_stub')['STATE'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/181: state_agi.head()
84/182: sum(state_agi['state'][count])
84/183: sum(state_agi['STATE']['count'])
84/184: state_agi.groupby('STATE')['count'].apply(sum)
84/185: state_agi.sum()
84/186: state_agi['STATE'].sum()
84/187:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
state_agi_sum = pd.DataFrame(state_agi.groupby('STATE')['count'].apply(sum)).reset_index()
state_agi.merge(state_agi_sum, left_on = 'STATE')
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/188:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
state_agi_sum = pd.DataFrame(state_agi.groupby('STATE')['count'].apply(sum)).reset_index()
state_agi.merge(state_agi_sum, left_on = 'STATE', right_on = 'STATE')
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/189: state_agi.head()
84/190:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
state_agi_sum = pd.DataFrame(state_agi.groupby('STATE')['count'].apply(sum)).reset_index()
state_agi_percent = state_agi.merge(state_agi, state_agi_sum, how = 'left', on = 'STATE')
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/191:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
state_agi_sum = pd.DataFrame(state_agi.groupby('STATE')['count'].apply(sum)).reset_index()
state_agi_percent = state_agi.merge(state_agi, state_agi_sum, how = 'left', left_on = 'STATE', right_on = 'STATE')
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/192:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
state_agi_sum = pd.DataFrame(state_agi.groupby('STATE')['count'].apply(sum)).reset_index()
state_agi_percent = df.merge(state_agi, state_agi_sum, how = 'left', left_on = 'STATE', right_on = 'STATE')
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/193:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
state_agi_sum = pd.DataFrame(state_agi.groupby('STATE')['count'].apply(sum)).reset_index()
state_agi_percent = pd.merge(state_agi, state_agi_sum, how = 'left', left_on = 'STATE', right_on = 'STATE')
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/194: state_agi_percent.head()
84/195:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/196: sns.heatmap(irs_new.corr())
84/197: sns.heatmap(irs2.corr())
84/198: sns.heatmap(irs2.drop('agi_stub',axis = 1).corr())
84/199: sns.heatmap(irs2.drop('agi_stub',axis = 1).corr(), cmap = 'coolwarm')
84/200:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
# pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_pct = irs_new.groupby(['STATE', 'new_agi_stub'])['count'].count()/irs_new.groupby(['STATE'])['count'].count()
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/201: state_agi.head()
84/202:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
# pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_pct = irs_new.groupby(['STATE', 'new_agi_stub'])['count'].count()/irs_new.groupby(['STATE'])['count'].count()
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/203: irs_new.groupby(['STATE'])['count'].count()
84/204: irs_new.groupby(['STATE', 'new_agi_stub'])['count'].count()
84/205: state_agi.groupby(['STATE', 'new_agi_stub'])['count']
84/206: state_agi.groupby(['STATE', 'new_agi_stub'])['count'].head()
84/207: state_agi.groupby(['STATE', 'new_agi_stub'])['count'].count()
84/208:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = irs_new.groupby(['STATE'])['new_agi_stub'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_pct = state_agi.groupby(['STATE', 'new_agi_stub'])['count']/irs_new.groupby(['STATE'])['count'].count()
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/209: tate_agi_sum = irs_new.groupby(['STATE'])['new_agi_stub'].apply(sum).reset_index()
84/210: tate_agi_sum = irs_new.groupby(['STATE']).apply(sum).reset_index()
84/211: tate_agi_sum = irs_new.groupby(['STATE']).apply(sum)
84/212:
tate_agi_sum = irs_new.groupby(['STATE']).apply(sum)
tate_agi_sum.head()
84/213:
tate_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum)
tate_agi_sum.head()
84/214: tate_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum)
84/215:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_pct = state_agi.groupby(['STATE', 'new_agi_stub'])['count']/irs_new.groupby(['STATE'])['count'].count()
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/216: state_agi_total
84/217:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total.groupby(['STATE']).apply(lambda x: x['count_x']/ x['count_y'])
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/218: state_agi_total.groupby(['STATE'])
84/219: state_agi_total.groupby(['STATE']).head()
84/220:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y'])
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/221:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/222: state_agi_total.head()
84/223:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
state_agi_total.groupby(['STATE', 'new_agi_stub'])['pct'].unstack(level=1).plot.barh(stacked = True)
84/224:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
state_agi_total.groupby(['STATE', 'new_agi_stub'])['pct'].plot.barh(stacked = True)
84/225: state_agi_total.groupby(['STATE', 'new_agi_stub'])['pct']
84/226: state_agi_total.groupby(['STATE', 'new_agi_stub'])['pct'].head()
84/227:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
df = state_agi_total.groupby(['STATE', 'new_agi_stub'])['pct']
df.unstack().plot(kind = 'bar', stacked = True)
84/228:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
84/229: pivot_sa1.head()
84/230: pivot_sa.head()
84/231:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
84/232: pivot_sa1.head()
84/233: pivot_sa.head()
84/234:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot.ploth(kind = 'bar', stacked = True)
84/235:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(kind = 'bar', stacked = True)
84/236:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(stacked = True)
84/237: irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
84/238: sns.heatmap(irs2.drop(['agi_stub', 'num_single_returns',axis = 1).corr(), cmap = 'coolwarm')
84/239: sns.heatmap(irs2.drop(['agi_stub', 'num_single_returns'],axis = 1).corr(), cmap = 'coolwarm')
84/240: sns.heatmap(irs2.drop(['agi_stub', 'num_joint_returns'],axis = 1).corr(), cmap = 'coolwarm')
84/241: sns.heatmap(irs2.drop(['agi_stub', 'amount_agi'],axis = 1).corr(), cmap = 'coolwarm')
84/242: irs = irs[irs['zipcode'] != 0]
84/243: acs.head(5)
84/244: print(irs.shape)
84/245: irs.info()
84/246: irs.describe()
84/247: irs2 = irs.drop('zipcode', axis =1)
84/248: irs2.head()
84/249:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
84/250:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
92/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
92/2:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
92/3: irs.head(5)
92/4: irs = irs[irs['zipcode'] != 0]
92/5: acs.head(5)
92/6: print(irs.shape)
92/7: irs.info()
92/8: irs.describe()
92/9: irs2 = irs.drop('zipcode', axis =1)
92/10: irs2.head()
92/11:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
92/12:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        print(amount_agi[i], dic[agi_stub[i])
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
92/13:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        print(amount_agi[i], dic[agi_stub[i]ï¼‰
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
92/14:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        print(amount_agi[i])
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
92/15: irs.head()
92/16: irs = irs[irs['zipcode'] != 0].reset_index()
92/17: acs.head(5)
92/18: print(irs.shape)
92/19: irs.info()
92/20: irs.describe()
92/21: irs2 = irs.drop('zipcode', axis =1)
92/22: irs2.head()
92/23: irs.head()
92/24:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
92/25: irs.head(5)
92/26: irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
92/27: irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
92/28: acs.head(5)
92/29:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
92/30: irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
92/31: acs.head(5)
92/32: irs.head(5)
92/33: irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
92/34: acs.head(5)
92/35: print(irs.shape)
92/36: irs.info()
92/37: irs.describe()
92/38:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
92/39: irs.head(5)
92/40: irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
92/41: irs.head(5)
92/42: print(irs.shape)
92/43: irs.info()
92/44: irs.describe()
92/45: irs2 = irs.drop('zipcode', axis =1)
92/46: irs2.head()
92/47: irs.head()
92/48:
def check_agi(amount_agi, agi_stub):
    res = []
    dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
        4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
    for i in range(10):
        if amount_agi[i] not in dic[agi_stub[i]]:
            res.append(i)
    return res

res = check_agi(irs['amount_agi'], irs['agi_stub'])
print(res)
93/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
93/2:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
93/3: irs.head(5)
93/4: irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
93/5: irs.head(5)
93/6: acs.head(5)
93/7: print(irs.shape)
93/8: irs.info()
93/9: irs.describe()
93/10: irs2 = irs.drop('zipcode', axis =1)
93/11: irs2.head()
93/12: irs.head()
93/13:
# def check_agi(amount_agi, agi_stub):
#     res = []
#     dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
#         4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
#     for i in range(10):
#         if amount_agi[i] not in dic[agi_stub[i]]:
#             res.append(i)
#     return res

# res = check_agi(irs['amount_agi'], irs['agi_stub'])
# print(res)
93/14:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names)
93/15: abs(irs2['nem_agi_stub'] - irs2['agi_stub']).sum()
93/16: sum(abs(irs2['nem_agi_stub'] - irs2['agi_stub']))
93/17: sum(abs(irs2['new_agi_stub'] - irs2['agi_stub']))
93/18: irs2['new_agi_stub'] - irs2['agi_stub']
93/19: irs2['new_agi_stub']
93/20: irs2['new_agi_stub'](dtype = 'int')
93/21: irs2['new_agi_stub'](dtype = 'int64')
93/22: irs2['new_agi_stub']
93/23:
irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
irs = irs[irs['amount_agi'] != 0].reset_index(drop= True)
93/24: irs.head(5)
93/25: acs.head(5)
93/26: print(irs.shape)
93/27: irs.info()
93/28: irs.describe()
93/29: irs2 = irs.drop('zipcode', axis =1)
93/30: irs2.head()
93/31: irs.head()
93/32: acs.describe()
93/33: irs.head()
93/34: irs.describe()
93/35: irs2 = irs.drop('zipcode', axis =1)
93/36: irs2.head()
93/37: irs.head()
93/38:
# def check_agi(amount_agi, agi_stub):
#     res = []
#     dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
#         4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
#     for i in range(10):
#         if amount_agi[i] not in dic[agi_stub[i]]:
#             res.append(i)
#     return res

# res = check_agi(irs['amount_agi'], irs['agi_stub'])
# print(res)
93/39:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names)
93/40: irs2['new_agi_stub']
93/41:
irs2['new_agi_stub'] = irs2.cc.cat.codes
irs2['new_agi_stub']
93/42:
#irs2['new_agi_stub'] = irs2.cc.cat.codes
irs2.new_agi_stub.astype('category').cat.codes
#irs2['new_agi_stub']
93/43: irs2['new_agi_stub']
93/44:
#irs2['new_agi_stub'] = irs2.cc.cat.codes
irs2.new_agi_stub.astype('category').cat.codes + 1
#irs2['new_agi_stub']
93/45:
#irs2['new_agi_stub'] = irs2.cc.cat.codes
sum(abs(irs.agi_stub - irs2.new_agi_stub.astype('category').cat.codes + 1))
#irs2['new_agi_stub']
93/46:
# def check_agi(amount_agi, agi_stub):
#     res = []
#     dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
#         4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
#     for i in range(10):
#         if amount_agi[i] not in dic[agi_stub[i]]:
#             res.append(i)
#     return res

# res = check_agi(irs['amount_agi'], irs['agi_stub'])
# print(res)
93/47:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names)
93/48: sum(abs(irs.agi_stub - irs2.new_agi_stub.astype('category').cat.codes + 1))
93/49: irs.head()
93/50:
# def check_agi(amount_agi, agi_stub):
#     res = []
#     dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
#         4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
#     for i in range(10):
#         if amount_agi[i] not in dic[agi_stub[i]]:
#             res.append(i)
#     return res

# res = check_agi(irs['amount_agi'], irs['agi_stub'])
# print(res)
93/51:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names)
93/52: sum(abs(irs.agi_stub - irs2.new_agi_stub.astype('category').cat.codes + 1))
93/53: irs2['new_agi_stub']
93/54: sum(abs(irs.agi_stub - irs2.new_agi_stub.astype('category').cat.codes + 1))
93/55: irs_new = irs2.drop(['agi_stub', 'amount_agi'], axis =1)
93/56:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names)astype('category').cat.codes + 1
93/57:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names).astype('category').cat.codes + 1
93/58: sum(abs(irs.agi_stub - irs2.new_agi_stub)
93/59: sum(abs(irs.agi_stub - irs2.new_agi_stub))
93/60: irs_new = irs2.drop(['agi_stub', 'amount_agi'], axis =1)
93/61: irs_new.head(10)
93/62: irs_state = pd.DataFrame(irs['STATE'].value_counts())
93/63: irs_state.head()
93/64: acs.describe()
93/65:
joint_data = pd.merge(irs, acs, how='inner', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
93/66:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
print(min(zip_check['percent']))
93/67:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
ax
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(stacked = True)
93/68:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(stacked = True)
93/69: irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
93/70: sns.heatmap(irs2.drop('agi_stub',axis = 1).corr(), cmap = 'coolwarm')
93/71:
plt.figure(figsize=(10,6))
sns.pairplot(data = 'irs_new')
93/72:
plt.figure(figsize=(10,6))
sns.pairplot(irs_new)
94/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
94/2:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
94/3: irs.head(5)
94/4:
irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
irs = irs[irs['amount_agi'] != 0].reset_index(drop= True)
94/5: irs.head(5)
94/6: acs.head(5)
94/7: print(irs.shape)
94/8: irs.info()
94/9: irs.describe()
94/10: irs2 = irs.drop('zipcode', axis =1)
94/11: irs2.head()
94/12: irs.head()
94/13:
# def check_agi(amount_agi, agi_stub):
#     res = []
#     dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
#         4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
#     for i in range(10):
#         if amount_agi[i] not in dic[agi_stub[i]]:
#             res.append(i)
#     return res

# res = check_agi(irs['amount_agi'], irs['agi_stub'])
# print(res)
94/14:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names).astype('category').cat.codes + 1
94/15: sum(abs(irs.agi_stub - irs2.new_agi_stub))
94/16: irs_new = irs2.drop(['agi_stub', 'amount_agi'], axis =1)
94/17: irs_new.head(10)
94/18: irs_state = pd.DataFrame(irs['STATE'].value_counts())
94/19: irs_state.head()
94/20: acs.describe()
94/21:
joint_data = pd.merge(irs, acs, how='inner', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
94/22:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
print(min(zip_check['percent']))
94/23:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(stacked = True)
94/24: irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
94/25:
plt.figure(figsize=(10,6))
sns.pairplot(irs_new)
94/26:
plt.figure(figsize=(10,6))
sns.pairplot(irs_new, hue = 'new_agi_stub')
94/27: sns.heatmap(irs2.drop('agi_stub',axis = 1).corr(), cmap = 'coolwarm')
94/28: irs_dummy = pd.get_dummies(irs_new, drop_first = True)
94/29: irs_dummy.head()
94/30: irs_dummy.info()
94/31: sns.heatmap(irs_new.drop('agi_stub',axis = 1).corr(), cmap = 'coolwarm')
94/32: sns.heatmap(irs_new.corr(), cmap = 'coolwarm')
94/33: irs_new_new = irs_new[irs_new['new_agi_stub'] != 6].reset_index
94/34: irs_new_new = irs_new[irs_new['new_agi_stub'] != 6].reset_index()
94/35: sns.heatmap(irs_new_new.drop('agi_stub',axis = 1).corr(), cmap = 'coolwarm')
94/36: sns.heatmap(irs_new_new.corr(), cmap = 'coolwarm')
94/37: irs_new_new = irs_new[irs_new['new_agi_stub'] != 6].reset_index(drop = True)
94/38: sns.heatmap(irs_new_new.corr(), cmap = 'coolwarm')
94/39:
plt.figure(figsize=(10,6))
sns.pairplot(irs_new_new, hue = 'new_agi_stub')
94/40: sns.heatmap(irs2.drop(['new_agi_stub','agi_stub'],axis = 1).corr(), cmap = 'coolwarm')
94/41: sns.heatmap(irs2.drop([,'agi_stub', 'amount_agi'],axis = 1).corr(), cmap = 'coolwarm')
94/42: sns.heatmap(irs2.drop(['agi_stub', 'amount_agi'],axis = 1).corr(), cmap = 'coolwarm')
94/43: sns.heatmap(irs2.drop(['agi_stub', 'new_agi_agi''amount_agi'],axis = 1).corr(), cmap = 'coolwarm')
94/44: sns.heatmap(irs2.drop(['agi_stub', 'new_agi_stub''amount_agi'],axis = 1).corr(), cmap = 'coolwarm')
94/45: sns.heatmap(irs2.drop(['agi_stub', 'new_agi_stub','amount_agi'],axis = 1).corr(), cmap = 'coolwarm')
94/46: irs_dummy = pd.get_dummies(irs_new.drop('new_agi_stub', axis = 1), drop_first = True)
94/47:
from sklearn.model_selection import KFold
np.random.seed(42)
kf = kFolde(n_splits = 10, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elassocre, gbscore, rfscore = 0,0,0,0

# split into train and test to avoid overfitting

for train_index, test_index in kf.split
94/48:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
np.random.seed(42)
kf = kFolde(n_splits = 10, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elassocre, gbscore, rfscore = 0,0,0,0

# split into train and test to avoid overfitting

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # lasso
    las = linear_model.lasso()
    model_las = las.fit(X_train, y_train)
    pre_las[test_index] = las.predict(X_test)
    lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore+elas.score(X_test,y_test)
    
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    
    # random forest
    params = {"n_estimators": 1000,"n_jobs": 2}
    rf = ensemble.RandomForestRegressor(**params)
    model_rf = rf.fit(X_train, y_train)
    pre_rf[test_index] = rf.predict(X_test)
    rfscore = rfscore + rf.score(X_test,y_test)
94/49:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
np.random.seed(42)
kf = kFold(n_splits = 10, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elassocre, gbscore, rfscore = 0,0,0,0

# split into train and test to avoid overfitting

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # lasso
    las = linear_model.lasso()
    model_las = las.fit(X_train, y_train)
    pre_las[test_index] = las.predict(X_test)
    lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore+elas.score(X_test,y_test)
    
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    
    # random forest
    params = {"n_estimators": 1000,"n_jobs": 2}
    rf = ensemble.RandomForestRegressor(**params)
    model_rf = rf.fit(X_train, y_train)
    pre_rf[test_index] = rf.predict(X_test)
    rfscore = rfscore + rf.score(X_test,y_test)
94/50:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
np.random.seed(42)
kf = KFold(n_splits = 10, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elassocre, gbscore, rfscore = 0,0,0,0

# split into train and test to avoid overfitting

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # lasso
    las = linear_model.lasso()
    model_las = las.fit(X_train, y_train)
    pre_las[test_index] = las.predict(X_test)
    lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore+elas.score(X_test,y_test)
    
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    
    # random forest
    params = {"n_estimators": 1000,"n_jobs": 2}
    rf = ensemble.RandomForestRegressor(**params)
    model_rf = rf.fit(X_train, y_train)
    pre_rf[test_index] = rf.predict(X_test)
    rfscore = rfscore + rf.score(X_test,y_test)
94/51:
irs_dummy = pd.get_dummies(irs_new.drop('new_agi_stub', axis = 1), drop_first = True)
X, y = irs_dummy.drop('num_returns', axis = 1), irs_dummy['num_returns']
94/52:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
np.random.seed(42)
kf = KFold(n_splits = 10, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elassocre, gbscore, rfscore = 0,0,0,0

# split into train and test to avoid overfitting

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # lasso
    las = linear_model.lasso()
    model_las = las.fit(X_train, y_train)
    pre_las[test_index] = las.predict(X_test)
    lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore+elas.score(X_test,y_test)
    
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    
    # random forest
    params = {"n_estimators": 1000,"n_jobs": 2}
    rf = ensemble.RandomForestRegressor(**params)
    model_rf = rf.fit(X_train, y_train)
    pre_rf[test_index] = rf.predict(X_test)
    rfscore = rfscore + rf.score(X_test,y_test)
94/53:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
np.random.seed(42)
kf = KFold(n_splits = 10, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elassocre, gbscore, rfscore = 0,0,0,0

# split into train and test to avoid overfitting

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # lasso
    las = linear_model.Lasso()
    model_las = las.fit(X_train, y_train)
    pre_las[test_index] = las.predict(X_test)
    lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore+elas.score(X_test,y_test)
    
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    
    # random forest
    params = {"n_estimators": 1000,"n_jobs": 2}
    rf = ensemble.RandomForestRegressor(**params)
    model_rf = rf.fit(X_train, y_train)
    pre_rf[test_index] = rf.predict(X_test)
    rfscore = rfscore + rf.score(X_test,y_test)
94/54:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
np.random.seed(42)
kf = KFold(n_splits = 10, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elasscore, gbscore, rfscore = 0,0,0,0

# split into train and test to avoid overfitting

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # lasso
    las = linear_model.Lasso()
    model_las = las.fit(X_train, y_train)
    pre_las[test_index] = las.predict(X_test)
    lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore+elas.score(X_test,y_test)
    
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    
    # random forest
    params = {"n_estimators": 1000,"n_jobs": 2}
    rf = ensemble.RandomForestRegressor(**params)
    model_rf = rf.fit(X_train, y_train)
    pre_rf[test_index] = rf.predict(X_test)
    rfscore = rfscore + rf.score(X_test,y_test)
95/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
95/2:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
95/3: irs.head(5)
95/4:
irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
irs = irs[irs['amount_agi'] != 0].reset_index(drop= True)
95/5: irs.head(5)
95/6: acs.head(5)
95/7: print(irs.shape)
95/8: irs.info()
95/9: irs.describe()
95/10: irs2 = irs.drop('zipcode', axis =1)
95/11: irs2.head()
95/12: irs.head()
95/13:
# def check_agi(amount_agi, agi_stub):
#     res = []
#     dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
#         4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
#     for i in range(10):
#         if amount_agi[i] not in dic[agi_stub[i]]:
#             res.append(i)
#     return res

# res = check_agi(irs['amount_agi'], irs['agi_stub'])
# print(res)
95/14:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names).astype('category').cat.codes + 1
95/15: sum(abs(irs.agi_stub - irs2.new_agi_stub))
95/16: irs_new = irs2.drop(['agi_stub', 'amount_agi'], axis =1)
95/17: irs_new.head(10)
95/18: irs_state = pd.DataFrame(irs['STATE'].value_counts())
95/19: irs_state.head()
95/20: acs.describe()
95/21:
joint_data = pd.merge(irs, acs, how='inner', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
95/22:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
print(min(zip_check['percent']))
95/23:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(stacked = True)
95/24: irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
95/25: sns.heatmap(irs2.drop('agi_stub',axis = 1).corr(), cmap = 'coolwarm')
95/26: sns.heatmap(irs2.drop(['agi_stub', 'new_agi_stub','amount_agi'],axis = 1).corr(), cmap = 'coolwarm')
95/27: sns.heatmap(irs_new.corr(), cmap = 'coolwarm')
95/28: irs_new_new = irs_new[irs_new['new_agi_stub'] != 6].reset_index(drop = True)
95/29: sns.heatmap(irs_new_new.corr(), cmap = 'coolwarm')
95/30:
irs_dummy = pd.get_dummies(irs_new.drop('new_agi_stub', axis = 1), drop_first = True)
X, y = irs_dummy.drop('num_returns', axis = 1), irs_dummy['num_returns']
95/31:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
np.random.seed(42)
kf = KFold(n_splits = 5, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elasscore, gbscore, rfscore = 0,0,0,0

# split into train and test to avoid overfitting

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
    # lasso
    las = linear_model.Lasso()
    model_las = las.fit(X_train, y_train)
    pre_las[test_index] = las.predict(X_test)
    lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore + elas.score(X_test,y_test)
    
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    
    # random forest
    params = {"n_estimators": 100,"n_jobs": 2}
    rf = ensemble.RandomForestRegressor(**params)
    model_rf = rf.fit(X_train, y_train)
    pre_rf[test_index] = rf.predict(X_test)
    rfscore = rfscore + rf.score(X_test,y_test)
96/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
96/2:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
96/3: irs.head(5)
96/4:
irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
irs = irs[irs['amount_agi'] != 0].reset_index(drop= True)
96/5: irs.head(5)
96/6: acs.head(5)
96/7: print(irs.shape)
96/8: irs.info()
96/9: irs.describe()
96/10: irs2 = irs.drop('zipcode', axis =1)
96/11: irs2.head()
96/12: irs.head()
96/13:
# def check_agi(amount_agi, agi_stub):
#     res = []
#     dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
#         4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
#     for i in range(10):
#         if amount_agi[i] not in dic[agi_stub[i]]:
#             res.append(i)
#     return res

# res = check_agi(irs['amount_agi'], irs['agi_stub'])
# print(res)
96/14:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names).astype('category').cat.codes + 1
96/15: sum(abs(irs.agi_stub - irs2.new_agi_stub))
96/16: irs_new = irs2.drop(['agi_stub', 'amount_agi'], axis =1)
96/17: irs_new.head(10)
96/18: irs_state = pd.DataFrame(irs['STATE'].value_counts())
96/19: irs_state.head()
96/20: acs.describe()
96/21:
joint_data = pd.merge(irs, acs, how='inner', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
96/22:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
print(min(zip_check['percent']))
96/23:
irs_dummy = pd.get_dummies(irs_new.drop('new_agi_stub', axis = 1), drop_first = True)
X, y = irs_dummy.drop('num_returns', axis = 1), irs_dummy['num_returns']
96/24:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
np.random.seed(42)
kf = KFold(n_splits = 5, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elasscore, gbscore, rfscore = 0,0,0,0

# split into train and test to avoid overfitting

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    
#     # lasso
#     las = linear_model.Lasso()
#     model_las = las.fit(X_train, y_train)
#     pre_las[test_index] = las.predict(X_test)
#     lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore + elas.score(X_test,y_test)
    
    print(1)
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    print(1)
    
#     # random forest
#     params = {"n_estimators": 100,"n_jobs": 2}
#     rf = ensemble.RandomForestRegressor(**params)
#     model_rf = rf.fit(X_train, y_train)
#     pre_rf[test_index] = rf.predict(X_test)
#     rfscore = rfscore + rf.score(X_test,y_test)
96/25:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
np.random.seed(42)
kf = KFold(n_splits = 5, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elasscore, gbscore, rfscore = 0,0,0,0
96/26: kf.split(X)
96/27:
for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
96/28:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
np.random.seed(42)
kf = KFold(n_splits = 5, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elasscore, gbscore, rfscore = 0,0,0,0

# split into train and test to avoid overfitting

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    print(1)
#     # lasso
#     las = linear_model.Lasso()
#     model_las = las.fit(X_train, y_train)
#     pre_las[test_index] = las.predict(X_test)
#     lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore + elas.score(X_test,y_test)
    
    print(1)
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    print(1)
    
#     # random forest
#     params = {"n_estimators": 100,"n_jobs": 2}
#     rf = ensemble.RandomForestRegressor(**params)
#     model_rf = rf.fit(X_train, y_train)
#     pre_rf[test_index] = rf.predict(X_test)
#     rfscore = rfscore + rf.score(X_test,y_test)
96/29:
from sklearn.metrics import mean_squared_error

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10,6)


print("R^2 is(Elastic Net) :\n", elasscore/10)
print("RMSE is(Elastic Net):\n", sqrt(mean_squared_error(y, pre_elas)))
plt.scatter(x=pre_elas,y=y)
plt.xlim(10,14)
plt.ylim(10,14)
plt.ylabel('Real prices in test dataset (log-scaled)')
plt.xlabel('Predicted prices for the test dataset (log-scaled)')
plt.title("Elastic Net")
plt.show()

print("R^2 is(Gradient Boosting) :\n", gbscore/10)
print("RMSE is(Gradient Boosting):\n", sqrt(mean_squared_error(y, pre_gb)))
plt.scatter(x=pre_gb,y=y)
plt.xlim(10,14)
plt.ylim(10,14)
plt.ylabel('Real prices in test dataset (log-scaled)')
plt.xlabel('Predicted prices for the test dataset (log-scaled)')
plt.title("Gradient Boosting")
plt.show()


print("Elastic Net top 10 attributes:")
for name,weight in sorted(zip(X.columns, elas.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))

print("Boosting top 10 attributes:")
for name,weight in sorted(zip(X.columns, gra.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))
    
elasprice_test = elas.predict(test)
graprice_test = gra.predict(test)


np.savetxt("elas.csv",elasprice_test)
np.savetxt("gra.csv",graprice_test)
96/30:
from sklearn.metrics import mean_squared_error

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10,6)


print("R^2 is(Elastic Net) :\n", elasscore/10)
print("RMSE is(Elastic Net):\n", np.sqrt(mean_squared_error(y, pre_elas)))
plt.scatter(x=pre_elas,y=y)
plt.xlim(10,14)
plt.ylim(10,14)
plt.ylabel('Real prices in test dataset (log-scaled)')
plt.xlabel('Predicted prices for the test dataset (log-scaled)')
plt.title("Elastic Net")
plt.show()

print("R^2 is(Gradient Boosting) :\n", gbscore/10)
print("RMSE is(Gradient Boosting):\n", np.sqrt(mean_squared_error(y, pre_gb)))
plt.scatter(x=pre_gb,y=y)
plt.xlim(10,14)
plt.ylim(10,14)
plt.ylabel('Real prices in test dataset (log-scaled)')
plt.xlabel('Predicted prices for the test dataset (log-scaled)')
plt.title("Gradient Boosting")
plt.show()


print("Elastic Net top 10 attributes:")
for name,weight in sorted(zip(X.columns, elas.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))

print("Boosting top 10 attributes:")
for name,weight in sorted(zip(X.columns, gra.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))
    
elasprice_test = elas.predict(test)
graprice_test = gra.predict(test)


np.savetxt("elas.csv",elasprice_test)
np.savetxt("gra.csv",graprice_test)
96/31:
from sklearn.metrics import mean_squared_error

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10,6)


print("R^2 is(Elastic Net) :\n", elasscore/10)
print("RMSE is(Elastic Net):\n", np.sqrt(mean_squared_error(y, pre_elas)))
plt.scatter(x=pre_elas,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset (log-scaled)')
plt.xlabel('Predicted prices for the test dataset (log-scaled)')
plt.title("Elastic Net")
plt.show()

print("R^2 is(Gradient Boosting) :\n", gbscore/10)
print("RMSE is(Gradient Boosting):\n", np.sqrt(mean_squared_error(y, pre_gb)))
plt.scatter(x=pre_gb,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset (log-scaled)')
plt.xlabel('Predicted prices for the test dataset (log-scaled)')
plt.title("Gradient Boosting")
plt.show()


print("Elastic Net top 10 attributes:")
for name,weight in sorted(zip(X.columns, elas.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))

print("Boosting top 10 attributes:")
for name,weight in sorted(zip(X.columns, gra.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))
    
elasprice_test = elas.predict(test)
graprice_test = gra.predict(test)


np.savetxt("elas.csv",elasprice_test)
np.savetxt("gra.csv",graprice_test)
96/32:
from sklearn.metrics import mean_squared_error

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10,6)


print("R^2 is(Elastic Net) :\n", elasscore/10)
print("RMSE is(Elastic Net):\n", np.sqrt(mean_squared_error(y, pre_elas)))
plt.scatter(x=pre_elas,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Elastic Net")
plt.show()

print("R^2 is(Gradient Boosting) :\n", gbscore/10)
print("RMSE is(Gradient Boosting):\n", np.sqrt(mean_squared_error(y, pre_gb)))
plt.scatter(x=pre_gb,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Gradient Boosting")
plt.show()


print("Elastic Net top 10 attributes:")
for name,weight in sorted(zip(X.columns, elas.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))

print("Boosting top 10 attributes:")
for name,weight in sorted(zip(X.columns, gra.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))
    
elasprice_test = elas.predict(test)
graprice_test = gra.predict(test)


np.savetxt("elas.csv",elasprice_test)
np.savetxt("gra.csv",graprice_test)
96/33:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
np.random.seed(42)
kf = KFold(n_splits = 10, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elasscore, gbscore, rfscore = 0,0,0,0

# split into train and test to avoid overfitting

for train_index, test_index in kf.split(X):
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
#     # lasso
#     las = linear_model.Lasso()
#     model_las = las.fit(X_train, y_train)
#     pre_las[test_index] = las.predict(X_test)
#     lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore + elas.score(X_test,y_test)
    
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    
#     # random forest
#     params = {"n_estimators": 100,"n_jobs": 2}
#     rf = ensemble.RandomForestRegressor(**params)
#     model_rf = rf.fit(X_train, y_train)
#     pre_rf[test_index] = rf.predict(X_test)
#     rfscore = rfscore + rf.score(X_test,y_test)
96/34:
from sklearn.metrics import mean_squared_error

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10,6)


print("R^2 is(Elastic Net) :\n", elasscore/10)
print("RMSE is(Elastic Net):\n", np.sqrt(mean_squared_error(y, pre_elas)))
plt.scatter(x=pre_elas,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Elastic Net")
plt.show()
plt.savefig('../Figure/elasnewt.png')

print("R^2 is(Gradient Boosting) :\n", gbscore/10)
print("RMSE is(Gradient Boosting):\n", np.sqrt(mean_squared_error(y, pre_gb)))
plt.scatter(x=pre_gb,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Gradient Boosting")
plt.show()
plt.savefig('../Figure/gra.png')

print("Elastic Net top 10 attributes:")
for name,weight in sorted(zip(X.columns, elas.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))


    
elasprice_test = elas.predict(test)
graprice_test = gra.predict(test)


np.savetxt("elas.csv",elasprice_test)
np.savetxt("gra.csv",graprice_test)
96/35:
from sklearn.metrics import mean_squared_error

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10,6)


print("R^2 is(Elastic Net) :\n", elasscore/10)
print("RMSE is(Elastic Net):\n", np.sqrt(mean_squared_error(y, pre_elas)))
plt.scatter(x=pre_elas,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Elastic Net")
plt.show()
plt.savefig('elasnewt.png')

print("R^2 is(Gradient Boosting) :\n", gbscore/10)
print("RMSE is(Gradient Boosting):\n", np.sqrt(mean_squared_error(y, pre_gb)))
plt.scatter(x=pre_gb,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Gradient Boosting")
plt.show()
plt.savefig('gra.png')

print("Elastic Net top 10 attributes:")
for name,weight in sorted(zip(X.columns, elas.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))


    
elasprice_test = elas.predict(test)
graprice_test = gra.predict(test)


np.savetxt("elas.csv",elasprice_test)
np.savetxt("gra.csv",graprice_test)
96/36:
from sklearn.metrics import mean_squared_error

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10,6)


print("R^2 is(Elastic Net) :\n", elasscore/10)
print("RMSE is(Elastic Net):\n", np.sqrt(mean_squared_error(y, pre_elas)))
plt.scatter(x=pre_elas,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Elastic Net")
plt.show()
plt.savefig('elasnewt.png')

print("R^2 is(Gradient Boosting) :\n", gbscore/10)
print("RMSE is(Gradient Boosting):\n", np.sqrt(mean_squared_error(y, pre_gb)))
plt.scatter(x=pre_gb,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Gradient Boosting")
plt.show()
plt.savefig('gra.png')

print("Elastic Net top 10 attributes:")
for name,weight in sorted(zip(X.columns, elas.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))


    
elasprice_test = elas.predict(X_test)
graprice_test = gra.predict(X_test)


np.savetxt("elas.csv",elasprice_test)
np.savetxt("gra.csv",graprice_test)
96/37: pre_elas,size()
96/38: pre_elas.shape
96/39:
from sklearn.metrics import mean_squared_error

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10,6)


print("R^2 is(Elastic Net) :\n", elasscore/10)
print("RMSE is(Elastic Net):\n", np.sqrt(mean_squared_error(y, pre_elas)))
plt.scatter(x=pre_elas,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Elastic Net")
plt.show()
plt.savefig('elasnewt.png')

print("R^2 is(Gradient Boosting) :\n", gbscore/10)
print("RMSE is(Gradient Boosting):\n", np.sqrt(mean_squared_error(y, pre_gb)))
plt.scatter(x=pre_gb,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Gradient Boosting")
plt.show()
plt.savefig('gra.png')

print("Elastic Net top 10 attributes:")
for name,weight in sorted(zip(X.columns, elas.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))


    
elasprice_test = elas.predict(X_test)
graprice_test = gra.predict(X_test)
96/40:
plt.figure(figsize=(10,6))
sns.pairplot(irs_new[irs_new_new['STATE'] = 'OH'])
96/41:
plt.figure(figsize=(10,6))
sns.pairplot(irs_new[irs_new_new['STATE'] == 'OH'])
96/42:
plt.figure(figsize=(10,6))
sns.pairplot(irs_new[irs_new['STATE'] == 'OH'])
96/43:
plt.figure(figsize=(10,6))
plt.subplot(1,2,1)
pivot_sa1.plot.barh(stacked = True)
x_label('Percent of each agi class')
plt.subplot(1,2,2)
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
x_label('Count of each agi class')
plt.savefig('STATE_AGI.png')
96/44:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(stacked = True)
plt.subplots_adjust(wspace=.5,hspace=.5)
plt.savefig()
96/45:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(stacked = True)
96/46: irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
96/47:
plt.figure(figsize=(10,6))
plt.subplot(1,2,1)
pivot_sa1.plot.barh(stacked = True)
x_label('Percent of each agi class')
plt.subplot(1,2,2)
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
x_label('Count of each agi class')
plt.savefig('STATE_AGI.png')
96/48:
plt.figure(figsize=(10,6))
plt.subplot(1,2,1)
ax = pivot_sa1.plot.barh(stacked = True)
ax.x_label('Percent of each agi class')
plt.subplot(1,2,2)
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
x_label('Count of each agi class')
plt.savefig('STATE_AGI.png')
96/49:
plt.figure(figsize=(10,6))
plt.subplot(1,2,1)
pivot_sa1.plot.barh(stacked = True)
x_label('Percent of each agi class')
plt.subplot(1,2,2)
irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
x_label('Count of each agi class')
plt.savefig('STATE_AGI.png')
96/50:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(stacked = True)
title('Percent of each agi class')
96/51:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0
pivot_sa1.plot.barh(stacked = True, x = 'Ratio', title = 'Count of each agi class)
96/52:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0
pivot_sa1.plot.barh(stacked = True, x = 'Ratio', title = 'Count of each agi class')
96/53:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0
pivot_sa1.plot.barh(stacked = True, x = 'Ratio', title = 'Count of each agi class')
96/54:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0
pivot_sa1.plot.barh(stacked = True, x = 'Ratio')
96/55:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0
pivot_sa1.plot.barh(stacked = True)
102/1:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
102/2:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
102/3: irs.head(5)
102/4:
irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
irs = irs[irs['amount_agi'] != 0].reset_index(drop= True)
102/5: irs.head(5)
102/6: acs.head(5)
102/7: print(irs.shape)
102/8: irs.info()
102/9: irs.describe()
102/10: irs2 = irs.drop('zipcode', axis =1)
102/11: irs2.head()
102/12:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names).astype('category').cat.codes + 1
102/13: sum(abs(irs.agi_stub - irs2.new_agi_stub))
102/14: irs_new = irs2.drop(['agi_stub', 'amount_agi'], axis =1)
102/15: irs_new.head(10)
102/16: irs_state = pd.DataFrame(irs['STATE'].value_counts())
102/17: irs_state.head()
102/18: acs.describe()
102/19:
joint_data = pd.merge(irs, acs, how='inner', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
102/20:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
print(min(zip_check['percent']))
102/21:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0
pivot_sa1.plot.barh(stacked = True, x = 'Ratio', title = 'Count of each agi class')
102/22: irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
102/23:
plt.rcParams['figure.figsize'] = 10, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0
pivot_sa1.plot.barh(stacked = True, x = 'Ratio', title = 'Count of each agi class')
102/24:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0
pivot_sa1.plot.barh(stacked = True, x = 'Ratio', title = 'Count of each agi class')
102/25:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0
pivot_sa1.plot.barh(stacked = True)
102/26:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(stacked = True, x = 'Ratio', title = 'Count of each agi class')
102/27:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(stacked = True)
102/28: irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
102/29: sns.heatmap(irs2.drop('agi_stub',axis = 1).corr(), cmap = 'coolwarm')
102/30: sns.heatmap(irs2.drop(['agi_stub', 'new_agi_stub','amount_agi'],axis = 1).corr(), cmap = 'coolwarm')
102/31: irs_new_new = irs_new[irs_new['new_agi_stub'] != 6].reset_index(drop = True)
102/32:
# split into train and test to avoid overfitting

for train_index, test_index in kf.split(X):
    sc = StandardScaler()
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)
    
#     # lasso
#     las = linear_model.Lasso()
#     model_las = las.fit(X_train, y_train)
#     pre_las[test_index] = las.predict(X_test)
#     lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore + elas.score(X_test,y_test)
    
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    
#     # random forest
#     params = {"n_estimators": 100,"n_jobs": 2}
#     rf = ensemble.RandomForestRegressor(**params)
#     model_rf = rf.fit(X_train, y_train)
#     pre_rf[test_index] = rf.predict(X_test)
#     rfscore = rfscore + rf.score(X_test,y_test)
102/33:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
from sklearn.preprocessing import StandardScaler
np.random.seed(42)
kf = KFold(n_splits = 5, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elasscore, gbscore, rfscore = 0,0,0,0
102/34:
irs_dummy = pd.get_dummies(irs_new.drop('new_agi_stub', axis = 1), drop_first = True)
X, y = irs_dummy.drop('num_returns', axis = 1), irs_dummy['num_returns']
102/35:
from sklearn.model_selection import KFold
from sklearn import linear_model
from sklearn import ensemble
from sklearn.preprocessing import StandardScaler
np.random.seed(42)
kf = KFold(n_splits = 5, shuffle = True)
pre_las, pre_elas, pre_gb, pre_rf = np.zeros((X.shape[0],)), np.zeros((X.shape[0],)), np.zeros((X.shape[0],)),np.zeros((X.shape[0],))
lasscore, elasscore, gbscore, rfscore = 0,0,0,0
102/36:
# split into train and test to avoid overfitting
for train_index, test_index in kf.split(X):
    sc = StandardScaler()
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)
    
#     # lasso
#     las = linear_model.Lasso()
#     model_las = las.fit(X_train, y_train)
#     pre_las[test_index] = las.predict(X_test)
#     lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore + elas.score(X_test,y_test)
    
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    
#     # random forest
#     params = {"n_estimators": 100,"n_jobs": 2}
#     rf = ensemble.RandomForestRegressor(**params)
#     model_rf = rf.fit(X_train, y_train)
#     pre_rf[test_index] = rf.predict(X_test)
#     rfscore = rfscore + rf.score(X_test,y_test)
102/37:
# split into train and test to avoid overfitting
for train_index, test_index in kf.split(X):
    sc = StandardScaler()
    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]
    X_train = sc.fit_transform(X_train)
    X_test = sc.transform(X_test)
    
#     # lasso
#     las = linear_model.Lasso()
#     model_las = las.fit(X_train, y_train)
#     pre_las[test_index] = las.predict(X_test)
#     lasscore = lasscore + las.score(X_test, y_test)
    
    # elastic net
    elas = linear_model.ElasticNet()
    model_elas = elas.fit(X_train, y_train)
    pre_elas[test_index] = elas.predict(X_test)
    elasscore = elasscore + elas.score(X_test,y_test)
    
    # gradient boosting
    gra = ensemble.GradientBoostingRegressor()
    model_gra = gra.fit(X_train,y_train)
    pre_gb[test_index] = gra.predict(X_test)
    gbscore = gbscore + gra.score(X_test,y_test)
    
#     # random forest
#     params = {"n_estimators": 100,"n_jobs": 2}
#     rf = ensemble.RandomForestRegressor(**params)
#     model_rf = rf.fit(X_train, y_train)
#     pre_rf[test_index] = rf.predict(X_test)
#     rfscore = rfscore + rf.score(X_test,y_test)
102/38:
from sklearn.metrics import mean_squared_error

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10,6)


print("R^2 is(Elastic Net) :\n", elasscore/10)
print("RMSE is(Elastic Net):\n", np.sqrt(mean_squared_error(y, pre_elas)))
plt.scatter(x=pre_elas,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Elastic Net")
plt.show()
plt.savefig('elasnewt.png')

print("R^2 is(Gradient Boosting) :\n", gbscore/10)
print("RMSE is(Gradient Boosting):\n", np.sqrt(mean_squared_error(y, pre_gb)))
plt.scatter(x=pre_gb,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Gradient Boosting")
plt.show()
plt.savefig('gra.png')

print("Elastic Net top 10 attributes:")
for name,weight in sorted(zip(X.columns, elas.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))


    
elasprice_test = elas.predict(X_test)
graprice_test = gra.predict(X_test)
102/39:
plt.figure(figsize=(10,6))
sns.pairplot(irs_new[irs_new['STATE'] == 'FL'])
102/40: print(count(irs[irs['zipcode'] == 0]))
102/41: print((irs[irs['zipcode'] == 0]).count())
102/42: irs[irs['zipcode'] == 0])
102/43:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
102/44:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
102/45: irs.head(5)
102/46: irs[irs['zipcode'] == 0].count()
102/47: print(irs[irs['zipcode'] == 0].count())
102/48:
print(irs[irs['zipcode'] == 0].count())
print(irs[irs['amount_agi'] == 0].count())
102/49: acs.info()
102/50:
plt.rcParams['figure.figsize'] = 5, 10
state_agi= irs_new.groupby(['STATE', 'new_agi_stub']).size().rename('count').reset_index()
pivot_sa = state_agi.pivot(index = 'STATE', columns ='new_agi_stub', values = 'count')
state_agi_sum = state_agi.groupby(['STATE'])['count'].apply(sum).reset_index()
state_agi_total = pd.merge(state_agi, state_agi_sum, how ='left', left_on = 'STATE', right_on = 'STATE')
state_agi_total['pct'] = state_agi_total['count_x']/ state_agi_total['count_y']
#irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
pivot_sa1 = pivot_sa.div(pivot_sa.sum(1), axis = 0)
pivot_sa1.plot.barh(stacked = True)
102/51: irs_new.groupby('STATE')['new_agi_stub'].value_counts().unstack(level =1).plot.barh(stacked = True)
102/52: acs[acs['Total; Estimate; Total population'] == 0].count()
102/53:
acs[acs['Total; Estimate; Total population'] == 0].count()
acs = acs[acs['Total; Estimate; Total population'] != 0].reset_index(drop= True)
102/54: acs[acs['Total; Estimate; Total population'] == 0].count()
102/55: irs_new = irs2.drop(['agi_stub', 'amount_agi'], axis =1)
102/56: acs[acs['Total; Estimate; Total population'] == 0].count()
102/57: acs = acs[acs['Total; Estimate; Total population'] != 0].reset_index(drop= True)
102/58:
joint_data = pd.merge(irs, acs, how='inner', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
102/59:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
print(min(zip_check['percent']))
102/60:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_y']/ zip_check['STATE_x']) * 100
print(zip_check)
print(min(zip_check['percent']))
102/61:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
print(zip_check)
print(min(zip_check['percent']))
102/62:
plt.figure(figsize=(10,6))
sns.pairplot(irs_new[irs_new['STATE'] == 'WA'])
102/63:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
file = open('zip_check.txt','w')
file.write(print(zip_check))
file.close()
print(min(zip_check['percent']))
102/64:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
print(zip_check) >> f
print(min(zip_check['percent']))
102/65:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
print(zip_check) >> f, 'waht ever  %d', i
print(min(zip_check['percent']))
102/66:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
file = open('zip_check.txt','w')
print(zip_check, file = file)
print(min(zip_check['percent']))
102/67:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
print(zip_check)
print(min(zip_check['percent']))
102/68:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
print(zip_check)
print(min(zip_check['percent']))
pd.DataFrame.to_csv(zip_check)
102/69:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
print(zip_check)
print(min(zip_check['percent']))
pd.DataFrame.to_csv('zip_check')
102/70:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
print(zip_check)
print(min(zip_check['percent']))
102/71: acs[acs['Total; Estimate; Total population'] == 0].count()
102/72:
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
102/73:
irs = pd.read_csv('irs_public_data.csv')
acs = pd.read_csv('acs_population_demographics.csv')
102/74: irs.head(5)
102/75:
print(irs[irs['zipcode'] == 0].count())
print(irs[irs['amount_agi'] == 0].count())
102/76:
irs = irs[irs['zipcode'] != 0].reset_index(drop= True)
irs = irs[irs['amount_agi'] != 0].reset_index(drop= True)
102/77: irs.head(5)
102/78: acs.head(5)
102/79: print(irs.shape)
102/80: irs.info()
102/81: irs.describe()
102/82: irs2 = irs.drop('zipcode', axis =1)
102/83: irs2.head()
102/84: irs.head()
102/85:
# def check_agi(amount_agi, agi_stub):
#     res = []
#     dic = {1: range(0, 25000), 2: range(25000,50000), 3: range(50000,75000),
#         4: range(75000,100000), 5: range(100000,200000), 6: range(200000, 2**31)}
#     for i in range(10):
#         if amount_agi[i] not in dic[agi_stub[i]]:
#             res.append(i)
#     return res

# res = check_agi(irs['amount_agi'], irs['agi_stub'])
# print(res)
102/86:
list_ = [0, 25000, 50000, 75000, 100000, 200000, np.inf]
names = [1, 2, 3, 4, 5, 6]
irs2['new_agi_stub'] = pd.cut(irs2['amount_agi'], list_, labels = names).astype('category').cat.codes + 1
102/87: sum(abs(irs.agi_stub - irs2.new_agi_stub))
102/88: irs_new = irs2.drop(['agi_stub', 'amount_agi'], axis =1)
102/89: irs_new.head(10)
102/90: irs_state = pd.DataFrame(irs['STATE'].value_counts())
102/91: irs_state.head()
102/92: acs.describe()
102/93: acs[acs['Total; Estimate; Total population'] == 0].count()
102/94: acs = acs[acs['Total; Estimate; Total population'] != 0].reset_index(drop= True)
102/95:
joint_data = pd.merge(irs, acs, how='inner', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
102/96:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
print(zip_check)
print(min(zip_check['percent']))
102/97:
joint_data = pd.merge(irs, acs, how='left', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
102/98:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
print(zip_check)
print(min(zip_check['percent']))
102/99: acs = acs[acs['Total; Estimate; Total population'] != 0].reset_index(drop= True)
102/100:
joint_data = pd.merge(irs, acs, how='left', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
102/101: acs[acs['Total; Estimate; Total population'] == 0].count()
102/102: acs = acs[acs['Total; Estimate; Total population'] != 0].reset_index(drop= True)
102/103:
joint_data = pd.merge(irs, acs, how='left', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
102/104:
zip_check = pd.merge(irs_state, joint_zip, how = 'left', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
print(zip_check)
print(min(zip_check['percent']))
102/105:
joint_data = pd.merge(irs, acs, how='left', 
                      left_on=['zipcode'],  right_on=['ZIP'])
joint_zip = pd.DataFrame(joint_data['STATE'].value_counts())
102/106:
zip_check = pd.merge(irs_state, joint_zip, how = 'outer', left_index = True, right_index = True)
zip_check['percent'] = (zip_check['STATE_x']/ zip_check['STATE_y']) * 100
print(zip_check)
print(min(zip_check['percent']))
102/107:
from sklearn.metrics import mean_squared_error

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10,6)


print("R^2 is(Elastic Net) :\n", elasscore/10)
print("RMSE is(Elastic Net):\n", np.sqrt(mean_squared_error(y, pre_elas)))
plt.scatter(x=pre_elas,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Elastic Net")
plt.show()
plt.savefig('elasnewt.png')

print("R^2 is(Gradient Boosting) :\n", gbscore/10)
print("RMSE is(Gradient Boosting):\n", np.sqrt(mean_squared_error(y, pre_gb)))
plt.scatter(x=pre_gb,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Gradient Boosting")
plt.show()
plt.savefig('gra.png')

print("Elastic Net top 10 attributes:")
for name,weight in sorted(zip(X.columns, elas.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))


    
elasprice_test = elas.predict(X_test)
graprice_test = gra.predict(X_test)
102/108:
from sklearn.metrics import mean_squared_error

plt.style.use(style='ggplot')
plt.rcParams['figure.figsize'] = (10,6)


print("R^2 is(Elastic Net) :\n", elasscore/10)
print("RMSE is(Elastic Net):\n", np.sqrt(mean_squared_error(y, pre_elas)))
plt.scatter(x=pre_elas,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Elastic Net")
plt.show()
plt.savefig('elasnewt.png')

print("R^2 is(Gradient Boosting) :\n", gbscore/10)
print("RMSE is(Gradient Boosting):\n", np.sqrt(mean_squared_error(y, pre_gb)))
plt.scatter(x=pre_gb,y=y)
# plt.xlim(10,14)
# plt.ylim(10,14)
plt.ylabel('Real prices in test dataset')
plt.xlabel('Predicted prices for the test dataset')
plt.title("Gradient Boosting")
plt.show()
plt.savefig('gra.png')

print("Elastic Net top 10 attributes:")
for name,weight in sorted(zip(X.columns, elas.coef_),key=lambda m:abs(m[1]), reverse = True)[:10]:
    print("(%s, %.7f)" %(name,weight))


    
elasprice_test = elas.predict(X_test)
graprice_test = gra.predict(X_test)
107/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/2:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/3:
df_confirm = pd.read_csv('time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('time_series_covid_19_deaths_US.csv')
107/4:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/5: df_confirm.head()
107/6: df_confirm.info()
107/7: df.columns.get_loc("1/22/20 ")
107/8: df_confirm.columns.get_loc("1/22/20 ")
107/9: df_confirm.columns.get_loc("1/22/20")
107/10: date_start_idx = df_confirm.columns.get_loc("1/22/20")
107/11: df_confirm_date = df_confirm.columns[data_start_idx:-1]
107/12: df_confirm_date = df_confirm.columns[date_start_idx:-1]
107/13: df_confirm_date
107/14: df_confirm_date[df_confirm_date]
107/15: df_confirm_date[date_start_idx:-1]
107/16: df_confirm[date_start_idx:-1]
107/17: df2 = df_confirm[date_start_idx:-1]
107/18: df2.head()
107/19: df2 = df_confirm(columns = [date_start_idx:-1])
107/20: df2 = df_confirm[[date_start_idx:-1]]
107/21: df2.head()
107/22: date_start_idx = df_confirm.columns.get_loc("1/22/20")
107/23: df2 = df_confirm[[date_start_idx:-1]]
107/24: df2 = df_confirm[range(date_start_idx,-1,-1)]
107/25: df2 = df_confirm[[confirm_dates]]
107/26: confirm_dates = df_confirm.columns[date_start_idx:-1]
107/27: df2 = df_confirm[[confirm_dates]]
107/28: df2 = df_confirm["4/16/20"]
107/29:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/30:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/31: df_confirm.info()
107/32: date_start_idx = df_confirm.columns.get_loc("1/22/20")
107/33: confirm_dates = df_confirm.columns[date_start_idx:-1]
107/34: df2 = df_confirm["4/16/20"]
107/35: df2 = df_confirm[["4/16/20"]]
107/36: df2.head()
107/37: df2 = df_confirm[["4/16/20"]]
107/38: df2.head()
107/39: df2 = df_confirm["4/16/20"]
107/40: df2.head()
107/41: df2 = df_confirm[confirm_dates]
107/42: df2.head()
107/43: sum(df.confirm[[dates]])
107/44: sum(df_confirm[[dates]])
107/45:
date_start_idx = df_confirm.columns.get_loc("1/22/20")
dates = df_confirm.columns[date_start_idx:-1]
107/46: sum(df_confirm[[dates]])
107/47:
date_start_idx = df_confirm.columns.get_loc("1/22/20")
dates = df_comfirm.columns[date_start_idx:-1]
107/48:
date_start_idx = df_confirm.columns.get_loc("1/22/20")
dates = df_confirm.columns[date_start_idx:-1]
107/49: sum(df_confirm[[dates]])
107/50:
date_start_idx = df_confirm.columns.get_loc("1/22/20")
dates = df_confirm.columns[date_start_idx:-1]
107/51: sum(df_confirm[[dates]])
107/52:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/53:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/54:
date_start_idx = df_confirm.columns.get_loc("1/22/20")
dates = df_confirm.columns[date_start_idx:-1]
107/55: sum(df_confirm[[dates]])
107/56: d2 = df_confirm[[dates]]
107/57: df2.head()
107/58: dates
107/59: df_confirm
107/60: df_confirm[[dates]]
107/61: dates
107/62: df_confirm[dates]
107/63: sum(df_confirm[dates])
107/64: df_confirm[dates].sum()
107/65: df_confirm.head()
107/66:
df = pd.concate(df_confirm[['Province_State', 'Population']],
                            df_confirm[df_confirm.columns.get_loc("1/22/20"):-1])
107/67:
df = pd.concat(df_confirm[['Province_State', 'Population']],
                            df_confirm[df_confirm.columns.get_loc("1/22/20"):-1])
107/68:
df = pd.concat(df_confirm[['UID','Province_State']],
                            df_confirm[df_confirm.columns.get_loc("1/22/20"):-1])
107/69: df_confirm[df_confirm.columns.get_loc("1/22/20"):-1]
107/70: X = df_confirm.columns.get_loc("1/22/20"):-1]
107/71: X = df_confirm.columns.get_loc("1/22/20")
107/72: X
107/73: df_confirm[df_confirm.columns.get_loc("1/22/20"):-1]
107/74: df_confirm.columns[df_confirm.columns.get_loc("1/22/20"):-1]
107/75: df_confirm[df_confirm.columns[df_confirm.columns.get_loc("1/22/20"):-1]]
107/76:
df = pd.concat(df_confirm[['UID','Province_State']],
               df_confirm[df_confirm.columns[df_confirm.columns.get_loc("1/22/20"):-1]]
107/77:
df = pd.concat(df_confirm[['UID','Province_State']],
               df_confirm[df_confirm.columns[df_confirm.columns.get_loc("1/22/20"):-1]])
107/78:
df = pd.concat([df_confirm[['UID','Province_State']],
               df_confirm[df_confirm.columns[df_confirm.columns.get_loc("1/22/20"):-1]]])
107/79: df_confirm.info()
107/80:
df = df_confirm[['UID','Province_State', 
                 df_confirm[df_confirm.columns[df_confirm.columns.get_loc("1/22/20"):-1]]]
107/81:
df = df_confirm[['UID','Province_State', 
                 df_confirm[df_confirm.columns[df_confirm.columns.get_loc("1/22/20"):-1]]]
107/82:
df = df_confirm[['UID','Province_State', 
                 df_confirm.columns[df_confirm.columns.get_loc("1/22/20"):-1]]]
107/83: dates = df_confirm.columns[df_confirm.columns.get_loc("1/22/20"):-1]
107/84: df = df_confirm[['UID','Province_State', dates]]]
107/85: df = df_confirm[['UID','Province_State', dates]]
107/86: df = df_confirm[['UID','Province_State']]
107/87: df = df_confirm[['UID','Province_State', dates]]
107/88: df = df_confirm[['UID','Province_State'], dates]
107/89: df = df_confirm[dates]
107/90: df = df_confirm[dates,'UID']
107/91:
dates = df_confirm.columns[df_confirm.columns.get_loc("1/22/20"):-1]
dates.append('UID')
107/92:
dates = df_confirm.columns[df_confirm.columns.get_loc("1/22/20"):-1]
dates.append(['UID'])
107/93: df_confirm.loc[['1/22/20':]]
107/94: df_confirm.loc['1/22/20':]
107/95: df_confirm['1/22/20':]
107/96: df_confirm['1/22/20':'4/16/20']
107/97: df_confirm[['1/22/20':'4/16/20']]
107/98: df_confirm[['1/22/20':'4/16/20']]
107/99: df_confirm.loc[:,'1/22/20':'4/16/20']
107/100: df_confirm.loc[:,'UID','1/22/20':'4/16/20']
107/101: df_confirm.loc[:,'UID''1/22/20':'4/16/20']
107/102: df_confirm.filter(items = ['1/22/20':'4/16/20'])
107/103: len(df_confirm, axis =1)
107/104: df_confirm.shape()
107/105: df_confirm.shape
107/106: df_confirm.shape[-1]
107/107:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
df_confirm.filter(items = ['1/22/20':'4/16/20'])
107/108:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_ = df_confirm.columns[date_start_idx_confirm:-1]
107/109: list(dates_)
107/110: df_confirm = df_confirm[['UID','Province_State']]
107/111: df_confirm = df_confirm[['UID','Province_State'],dates_]
107/112: df_confirm = df_confirm[['UID','Province_State'], dates_]
107/113: df_confirm = df_confirm[['UID','Province_State'], dates_ ]
107/114:

df_confirm = df_confirm[dates_ ]
107/115:

df_confirm = df_confirm[ [dates_ ]]
107/116:

df_confirm = df_confirm[ dates.tolist]
107/117:

df_confirm = df_confirm[[dates.tolist]]
107/118:

df_confirm = df_confirm[[dates_.tolist]]
107/119:

df_confirm = df_confirm[dates_.tolist]
107/120:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_ = df_confirm.columns[date_start_idx_confirm:-1]
dates_.tolist
107/121:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_ = df_confirm.columns[date_start_idx_confirm:-1]
dates_values
107/122:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_ = df_confirm.columns[date_start_idx_confirm:-1]
dates
107/123:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_ = df_confirm.columns[date_start_idx_confirm:-1]
107/124:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
#dates_ = df_confirm.columns[date_start_idx_confirm:-1]
107/125:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/126:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
#dates_ = df_confirm.columns[date_start_idx_confirm:-1]
107/127:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_ = df_confirm.columns[date_start_idx_confirm:-1]
107/128:

df_confirm2 = df_confirm[dates_.tolist]
107/129:

df_confirm2 = df_confirm[[dates_.tolist]]
107/130:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_ = df_confirm.columns[date_start_idx_confirm:-1]
dates_.tolist
107/131: df_confirm2 = df_confirm[[list(dates_)]]
107/132: df_confirm2 = df_confirm[list(dates_)]
107/133: df_confirm2 = df_confirm[['UID'], list(dates_)]
107/134: df_confirm2 = df_confirm[['UID', 'Province_State'] +list(dates_)]
107/135:
df_confirm2 = df_confirm[['UID', 'Province_State'] +list(dates_)]
df_confirm2.head()
107/136:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_ = df_confirm.columns[date_start_idx_confirm:-1]
107/137:
df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
107/138:
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_ = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_)]
107/139:
df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
107/140:
df_confirm.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
107/141:
df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
107/142:
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_confirm:-1]
df_death_new = df_death[['UID', 'Province_State', 'Population'] +list(dates_confirm)]
107/143:
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_confirm:-1]
df_death_new = df_death[['UID', 'Province_State', 'Population'] +list(dates_death)]
107/144:
df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
df_confirm_new.set_index('UID')
107/145:
df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
df_confirm_new.set_index('UID')
107/146:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
df_confirm_new.set_index('UID')
107/147:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State', 'Population'],
                   var_name = 'Date', value_name ='Value')
df_death_new.set_index('UID')
107/148:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_confirm:-1]
df_death_new = df_death[['UID', 'Province_State', 'Population'] +list(dates_death)]
107/149:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
df_confirm_new.set_index('UID')
107/150:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State', 'Population'],
                   var_name = 'Date', value_name ='Value')
df_death_new.set_index('UID')
107/151:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State', 'Population'], 
                                 var_name = 'Date', value_name ='Value')
df_death_new.set_index('UID')
107/152:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State', 'Population'], 
                                 var_name = 'Date', value_name ='Value')
107/153: df_death_new
107/154:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/155:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/156:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/157:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_confirm:-1]
df_death_new = df_death[['UID', 'Province_State', 'Population'] +list(dates_death)]
107/158:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
df_confirm_new.set_index('UID')
107/159: df_death_new
107/160:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_confirm:-1]
df_death_new = df_death[['UID', 'Province_State', 'Population'] +list(dates_death)]
107/161:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
df_confirm_new.set_index('UID')
107/162:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/163:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/164: df_death.head()
107/165:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/166:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_confirm:-1]
df_death_new = df_death[['UID', 'Province_State', 'Population'] +list(dates_death)]
107/167: df_death_new.head()
107/168:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_confirm:-1]
df_death_new = df_death[['UID', 'Province_State'] +list(dates_death)]
107/169: df_death_new.head()
107/170:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/171:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/172:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/173:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_confirm:-1]
df_death_new = df_death[['UID', 'Province_State'] +list(dates_death)]
107/174:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/175: df_death_new.head()
107/176:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
df_confirm_new.set_index('UID')
107/177: df_death_new
107/178:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Value')
df_death_new.set_index('UID')
107/179:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirmed')
df_confirm_new.set_index('UID')
107/180:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death')
df_death_new.set_index('UID')
107/181:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
df_confirm_new.set_index('UID')
107/182:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/183:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/184:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/185:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/186:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Value')
df_confirm_new.set_index('UID')
107/187:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/188:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/189:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirmed')
df_confirm_new.set_index('UID')
107/190:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death')
df_death_new.set_index('UID')
107/191: pd.merge(df_confirm_new, df_death_new, how='left', on= 'UID')
107/192:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/193:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/194:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/195:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/196:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirmed')
df_confirm_new.set_index('UID')
107/197:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death')
df_death_new.set_index('UID')
107/198: pd.merge(df_confirm_new, df_death_new, how='left', on= 'UID')
107/199: total_df = pd.merge(df_confirm_new, df_death_new, how='left', left_on= 'UID')
107/200: total_df = pd.merge(df_confirm_new, df_death_new, how='left', lefton= 'UID')
107/201: total_df = pd.merge(df_confirm_new, df_death_new, how='inner', on= 'UID')
107/202: total_df.head()
107/203:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirmed')
107/204:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death')
107/205: total_df = pd.merge(df_confirm_new, df_death_new, how='inner', on= 'UID')
107/206: total_df.head()
107/207:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/208:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/209:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/210:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/211:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm')
df_confirm_new.set_index('UID')
107/212:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death')
df_death_new.set_index('UID')
107/213: total_df = pd.merge(df_confirm_new, df_death_new, how='inner', on= 'UID')
107/214: df_confirm_new.join(df_death_new)
107/215: df_confirm_new.head()
107/216:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/217:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/218:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/219:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/220:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm')
df_confirm_new = df_confirm_new.set_index('UID')
107/221:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death')
df_death_new = df_death_new.set_index('UID')
107/222: df_death_new.head()
107/223: df_confirm_new.join(df_death_new)
107/224: df_death_new.info()
107/225:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm')
#df_confirm_new = df_confirm_new.set_index('UID')
107/226:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/227:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/228:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/229:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm')
#df_confirm_new = df_confirm_new.set_index('UID')
107/230: df_death_new['UID'] == df_confirm_new['UID']
107/231: pd.merge(df_confirm_new, df_death_new, how ='inner', on = 'UID')
107/232:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/233:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/234:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/235:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/236:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm')
#df_confirm_new = df_confirm_new.set_index('UID')
107/237:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death')
#df_death_new = df_death_new.set_index('UID')
107/238: pd.merge(df_confirm_new, df_death_new, how ='inner', on = 'UID')
107/239: pd.merge(df_confirm_new, df_death_new, how ='right', on = 'UID')
107/240: df_total = pd.concat([df_confirm_new, df_death_new], axis=1, join ='left')
107/241:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/242:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/243:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/244:
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')
#df_confirm_new = df_confirm_new.set_index('UID')
107/245:
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')
#df_death_new = df_death_new.set_index('UID')
107/246: df_total = pd.concat([df_confirm_new, df_death_new], axis=1, join ='left')
107/247: df_total = pd.concat([df_confirm_new, df_death_new], axis=1, join ='outer')
107/248: df_total.head()
107/249: df_total
107/250: df_total.groupby(['Date', 'Province_State'])['Confirm'].sum()
107/251: df_total.groupby(['Date', 'Province_State']).sum()
107/252: df_total = pd.concat([df_confirm_new, df_death_new.rename(colums={'Province_State':'PS', 'Date':'Dt'})], axis=1, join ='outer')
107/253: df_total = pd.concat([df_confirm_new, df_death_new.rename(columns={'Province_State':'PS', 'Date':'Dt'})], axis=1, join ='outer')
107/254: df_total
107/255: df_total.groupby(['Date','Province_State']).sum()
107/256: df_total.groupby(['Date'])['Confirm'].sum()
107/257: df_total.groupby(['Date'])['Confirm'].sum().plot()
107/258: df_total.groupby(['Date'])['Death'].sum().plot()
107/259: df_total.groupby(['Date'])['Confirm'].sum().plot()
107/260: df_total.groupby(['Date'])['Death'].sum().plot()
107/261: df_total.groupby(['Date'])['Confirm'].sum()
107/262: print(df_total.groupby(['Date'])['Confirm'].sum())
107/263: df_total.groupby(['Date'])['Confirm'].sum().plot()
107/264: df_total.groupby(['Date'])['Confirm'].sum()
107/265: df_total.info()
107/266: df_total
107/267: df_total.head()
107/268: df_total.head['Date'] = pd.to_datetime(df_total.head['Date'])
107/269: df_total['Date'] = pd.to_datetime(df_total['Date'])
107/270: df_total
107/271: df_total.head()
107/272: df_total.info()
107/273: df_total.groupby(['Date','Province_State']).sum()
107/274: df_total.groupby(['Date','Province_State']).sum().plot()
107/275: df_total.groupby(['Date','Province_State']).sum()
107/276: df_total.groupby(['Date','Province_State']).sum()
107/277: df_total.groupby(['Date'])['Confirm'].sum()
107/278: df_total.groupby(['Date'])['Confirm'].sum().plot()
107/279: df_total
107/280:
# merge df_confirm and df_death together
df_total = pd.concat([df_confirm_new, df_death_new.rename(columns={'Province_State':'PS', 'Date':'Dt'})], axis=1, join ='outer')
107/281:
# change the object str to date date
df_total['Date'] = pd.to_datetime(df_total['Date'])
107/282: df_total.info()
107/283:
# Create figure and plot space
fig, ax = plt.subplots(figsize=(10, 10))

# Add x-axis and y-axis
df_total.groupby(['Date'])['Confirm'].sum().plot()

# Set title and labels for axes
ax.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")

# plt.show()
# fig, ax = subplot(figsize = (10,10))
# df_total.groupby(['Date'])['Confirm'].sum().plot()
107/284:
# Create figure and plot space
fig, ax = plt.subplots(figsize=(6, 6))

# Add x-axis and y-axis
df_total.groupby(['Date'])['Confirm'].sum().plot()

# Set title and labels for axes
ax.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")

# plt.show()
# fig, ax = subplot(figsize = (10,10))
# df_total.groupby(['Date'])['Confirm'].sum().plot()
107/285:
# Create figure and plot space
fig, ax = plt.subplots(figsize=(4, 3))

# Add x-axis and y-axis
df_total.groupby(['Date'])['Confirm'].sum().plot()

# Set title and labels for axes
ax.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")

# plt.show()
# fig, ax = subplot(figsize = (10,10))
# df_total.groupby(['Date'])['Confirm'].sum().plot()
107/286:
# Create figure and plot space
fig, ax = plt.subplots(figsize=(5, 5))

df_total.groupby(['Date'])['Confirm'].sum().plot().kde()

# Set title and labels for axes
ax.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/287:
# Create figure and plot space
fig, ax = plt.subplots(figsize=(5, 5))

df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black')

# Set title and labels for axes
ax.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/288:
# Create figure and plot space
fig, ax = plt.subplots(figsize=(5, 5))

df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black')
df_total.groupby(['Date'])['Death'].sum().plot(c ='red')
# Set title and labels for axes
ax.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/289:
# Create figure and plot space
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
ax1=plt.subplot(2, 2, 1)

df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black')
df_total.groupby(['Date'])['Death'].sum().plot(c ='red')
# Set title and labels for axes
ax.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/290:
# Create figure and plot space
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
ax1=plt.subplot(2, 2, 1)

df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black')
df_total.groupby(['Date'])['Death'].sum().plot(c ='red')
# Set title and labels for axes
ax1.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/291:
# Create figure and plot space
fig, (ax1, ax2) = plt.subplots(ncols = 2, figsize=(10, 5))

df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black')
df_total.groupby(['Date'])['Death'].sum().plot(c ='red')
# Set title and labels for axes
ax1.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/292:
# Create figure and plot space
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))

df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black')
df_total.groupby(['Date'])['Death'].sum().plot(c ='red')
# Set title and labels for axes
ax0.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/293:
# Create figure and plot space
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))

ax0 = df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black')
# df_total.groupby(['Date'])['Death'].sum().plot(c ='red')
# Set title and labels for axes
ax0.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/294:
# Create figure and plot space
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))

ax0 = df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black')
ax1  = df_total.groupby(['Date'])['Death'].sum().plot(c ='red')
# Set title and labels for axes
ax0.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/295:
# Create figure and plot space
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0)
ax1  = df_total.groupby(['Date'])['Death'].sum().plot(c ='red')

ax0.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/296:
# Create figure and plot space
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0)
ax1  = df_total.groupby(['Date'])['Death'].sum().plot(c ='red', ax = ax1)

ax0.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/297:
# Create figure and plot space
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0)
df_total.groupby(['Date'])['Death'].sum().plot(c ='red', ax = ax0)
df_total.groupby(['Date'])['Death'/'Confirm'].mean().plot(c ='red', ax = ax0)

ax0.set(xlabel="Date",
       ylabel="Precipitation (inches)",
       title="Daily Total Precipitation\nBoulder, Colorado in July 2018")
107/298:
# change the object str to datetime dtype
print(df_total['Date'] = pd.to_datetime(df_total['Date']).sum())
107/299:
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date']).sum()
107/300:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/301:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/302:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/303:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/304:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')
107/305:
# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')
107/306:
# merge df_confirm and df_death together
df_total = pd.concat([df_confirm_new, df_death_new.rename(columns={'Province_State':'PS', 'Date':'Dt'})], axis=1, join ='outer')
107/307:
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date']).sum()
107/308:
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])
107/309:
def div_two(df, var1, var2):
    return df[var1].sum() / float(df[var1].sum())
df_total.groupby(['Date']).apply(div_two(df_total, 'Death', 'Confirm'))
107/310:
def div_two(df, var1, var2):
    print(df[var1])
    return df[var1].sum() / float(df[var1].sum())
df_total.groupby(['Date']).apply(div_two(df_total, 'Death', 'Confirm'))
107/311:
def div_two(df, var1, var2):
    return df[var1].sum() / float(df[var1].sum())
df_total.groupby(['Date']).apply(div_two(df_total, 'Death', 'Confirm'))
107/312:
def div_two(df, var1, var2):
    return df[var1].sum() / df[var2].sum()
df_total.groupby(['Date']).apply(div_two(df_total, 'Death', 'Confirm'))
107/313:
def div_two(df, var1, var2):
    return df[var1].sum()
df_total.groupby(['Date']).apply(div_two(df_total, 'Death', 'Confirm'))
107/314:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
df_total.groupby(['Date']).apply(div_two)
107/315:
# Create figure and plot space
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0)
df_total.groupby(['Date'])['Death'].sum().plot(c ='red', ax = ax0)
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax1)

ax0.set(xlabel="Date",ylabel="Cases")
ax1.set(xlabel="Date",ylabel="Ratio")
107/316:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
107/317:
# Create figure and plot space
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0)
df_total.groupby(['Date'])['Death'].sum().plot(c ='red', ax = ax0)
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax1)

ax0.set(xlabel="Date",ylabel="Cases")
ax1.set(xlabel="Date",ylabel="Ratio")
107/318:
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])
df_total['Confirm_diff'] = df_total['Confirm'].diff()
df_total['Death_diff'] = df_total['Death'].diff()
107/319: df_total.head()
107/320:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 2, figsize=(10, 5))
df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0)
df_total.groupby(['Date'])['Confirm_diff'].sum().plot(c ='red', ax = ax0)

df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1)
df_total.groupby(['Date'])['Death_diff'].sum().plot(c ='red', ax = ax1)

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)

ax0.set(xlabel="Date",ylabel="Confirmed Cases", legend = ['Daily', 'Cumulative'])
ax1.set(xlabel="Date",ylabel="Death Cases", legend = ['Daily', 'Cumulative'])
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/321:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(5, 10))
df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0)
df_total.groupby(['Date'])['Confirm_diff'].sum().plot(c ='red', ax = ax0)

df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1)
df_total.groupby(['Date'])['Death_diff'].sum().plot(c ='red', ax = ax1)

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)

ax0.set(xlabel="Date",ylabel="Confirmed Cases", legend = ['Daily', 'Cumulative'])
ax1.set(xlabel="Date",ylabel="Death Cases", legend = ['Daily', 'Cumulative'])
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/322:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(5, 10))
df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
df_total.groupby(['Date'])['Confirm_diff'].sum().plot(c ='red', ax = ax0, label ='Daily')

df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
df_total.groupby(['Date'])['Death_diff'].sum().plot(c ='red', ax = ax1,label ='Daily')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)

ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/323:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(5, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
df_total.groupby(['Date'])['Confirm_diff'].sum().plot(c ='red', ax = ax0, label ='Daily')

#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
df_total.groupby(['Date'])['Death_diff'].sum().plot(c ='red', ax = ax1,label ='Daily')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)

ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/324: df_total.head(-10)
107/325: df_total.head(-5:)
107/326: df_total.head(:-5)
107/327: df_total.head(-1:-1:-10)
107/328: df_total.tail()
107/329:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
def confirm_daily(df):
    return df.diff()
107/330: df_total.groupby(['Date'])['Death_diff'].sum().apply(confirm_daily)
107/331:
# sum_table
df_new = df_total.groupby(['Date'])['Confirm'].sum()
107/332: df_new.head()
107/333:
# sum_table
df_new = pd.Series(df_total.groupby(['Date'])['Confirm'].sum())
107/334: df_new.head()
107/335:
# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
107/336: df_new.head()
107/337:
# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
107/338:
# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
107/339: df_new
107/340:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
107/341:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(5, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new['Date'], df_new['Confirm'], c = 'black')
ax0.plot(df_new['Date'], df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new['Date'], df_new['Death'], c = 'black')
ax1.plot(df_new['Date'], df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/342:
# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
df_new.reset_index()
107/343:
# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum()).reset_index()
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
107/344: df_new.head()
107/345:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(5, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new['Date'], df_new['Confirm'], c = 'black')
ax0.plot(df_new['Date'], df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new['Date'], df_new['Death'], c = 'black')
ax1.plot(df_new['Date'], df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/346:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/347:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/348:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]
107/349:
# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/350:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')
107/351:
# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')
107/352:
# merge df_confirm and df_death together
df_total = pd.concat([df_confirm_new, df_death_new.rename(columns={'Province_State':'PS', 'Date':'Dt'})], axis=1, join ='outer')
107/353:
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])
107/354:
# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum()).reset_index()
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
107/355: df_new.head()
107/356:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
107/357:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(5, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new['Date'], df_new['Confirm'], c = 'black')
ax0.plot(df_new['Date'], df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new['Date'], df_new['Death'], c = 'black')
ax1.plot(df_new['Date'], df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/358:
# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
107/359: df_new.head()
107/360:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(5, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index_values, df_new['Confirm'], c = 'black')
ax0.plot(df_new.index_values, df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index_values, df_new['Death'], c = 'black')
ax1.plot(df_new.index_values, df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/361:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(5, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/362: df_new.DatetimeIndex
107/363:
# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum()).set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
107/364: df_new.head()
107/365:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
107/366:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(5, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/367:
# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
107/368: df_new.head()
107/369: df_new.info()
107/370:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
107/371:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(5, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/372:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(5, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new['Confirm'], c = 'black')
ax0.plot(df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot( df_new['Death'], c = 'black')
ax1.plot(df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/373:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/374:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(8, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/375:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')

# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])

# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
107/376:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
107/377:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/378:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/379:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')

# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])

# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
107/380:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
107/381:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(8, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax1.set(xlabel="Date",ylabel="Death Cases")
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/382:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(8, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/383:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(8, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')

df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/384:
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
ax0.plot(df_new[df_new['Date'] >= start_date]['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/385:
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
ax0.plot(df_new[df_new.index.values >= start_date]['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/386: start_date = '2020-03-01'
107/387:
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
ax0.plot(df_new[df_new.index.values >= start_date]['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/388:
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
ax0.plot(df_new[df_new.index.values > start_date]['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/389: df_new.index.values > '2020-03-01'
107/390: df_new.index.values
107/391: df_new.index.values > '2020-03-01T00:00:00.000000000'
107/392: pd.to_datetime(df_new.index.values) > '2020-03-01'
107/393: index_mask = pd.to_datetime(df_new.index.values) > '2020-03-01'
107/394:
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
ax0.plot(df_new[index_mask]['Confirm'], c = 'black')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/395:
fig, (ax0, ax1) = plt.subplots(ncols = 2, figsize=(10, 5))
ax0.plot(df_new[index_mask]['Confirm'], c = 'black')
#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/396:
fig, (ax0, ax1) = plt.subplots(nrows = 2, figsize=(5, 10))
ax0.plot(df_new[index_mask]['Confirm'], c = 'black')
#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/397:
fig, (ax0, ax1) = plt.subplots(nrows = 2, figsize=(8, 10))
ax0.plot(df_new[index_mask]['Confirm'], c = 'black')
#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/398:
fig, (ax0, ax1) = plt.subplots(nrows = 2, figsize=(8, 10))
ax0.plot(df_new[index_mask]['Confirm'], c = 'black')
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black')
#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/399:
fig, (ax0, ax1) = plt.subplots(nrows = 2, figsize=(8, 10))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black')

#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/400:
fig, (ax0, ax1) = plt.subplots(nrows = 1, figsize=(8, 6))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black')
ax
#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/401:
fig, (ax0, ax1) = plt.subplots(nrows = 1, figsize=(8, 6))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'black')
#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/402:
fig, ax0 = plt.subplots( figsize=(8, 6))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'black')
#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/403:
fig, ax0 = plt.subplots( figsize=(8, 6))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red')
#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/404:
fig, ax0 = plt.subplots( figsize=(8, 6))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red')
ax.set_xlabel('Date')
#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/405:
fig, ax0 = plt.subplots( figsize=(8, 6))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red')
ax0.set_xlabel('Date')
#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/406:
fig, ax0 = plt.subplots( figsize=(8, 6))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
#ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red')
107/407:
fig, ax0 = plt.subplots( figsize=(7, 5))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
107/408:
fig, ax0 = plt.subplots( figsize=(8, 5))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
107/409: df_total['Province_State'] in ['California']
107/410: df_total['Province_State'].isin(['California', 'Washingtion'])
107/411: df_new = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
107/412: df_new.head()
107/413: df_new['Province_State'].isin(['California'])
107/414: df_new['Province_State']
107/415: df_new.info()
107/416: df_new.index()
107/417: df_new.loc['California']
107/418: df_new = pd.DataFrame(df_total.groupby(['Province_State'])['Confirm'].sum())
107/419: df_new()
107/420: df_new.head()
107/421: df_new = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
107/422: df_new2 = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
107/423: df_new2 = df_new2.stack()
107/424: df_new2
107/425: df_new2.head()
107/426: df_new2.tail()
107/427: df_new2 = pd.DateFrame(df_new2.stack())
107/428: df_new2 = pd.DataFrame(df_new2.stack())
107/429: df_new2 = df_new2.stack()
107/430: df_new2 = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
107/431: df_new2 = df_new2.stack()
107/432: df_new2 = pd.DataFrame(df_new2)
107/433: df_new2
107/434: df_new2.head()
107/435: df_new2 = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
107/436: df_new2.head()
107/437: df_new2 = pd.DataFrame(df_new2)
107/438: df_new2.stack()
107/439: df_new2 = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
107/440: df_new2.head()
107/441: df_new2.loc(axis=0)[pd.IndexSlice[:, 'California']]
107/442: df_new2.loc(axis=0)[pd.IndexSlice[:, ['California','Washington']]
107/443: df_new2.loc(axis=0)[pd.IndexSlice[:, ['California','Washington']]]
107/444: df_new_west = df_new_state.loc(axis=0)[pd.IndexSlice[:, ['California','Washington']]]
107/445: df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
107/446: df_new_west = df_new_state.loc(axis=0)[pd.IndexSlice[:, ['California','Washington']]]
107/447: df_new_west
107/448: df_new_west['Confirm']
107/449: df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm','Death'].sum())
107/450: df_new_west = df_new_state.loc(axis=0)[pd.IndexSlice[:, ['California','Washington']]]
107/451: df_new_west.head()
107/452: df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
107/453: df_new_west = df_new_state.loc(axis=0)[pd.IndexSlice[:, ['California','Washington']]]
107/454: df_new_west['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
107/455: df_new_west = df_new_state.loc(axis=0)[pd.IndexSlice[:, ['California','Washington']]]
107/456: df_new_west['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
107/457:
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum())
107/458:
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
107/459: df_new_west = df_new_state.loc(axis=0)[pd.IndexSlice[:, ['California','Washington']]]
107/460: df_new_west.head()
107/461: df_new_west.tail()
107/462: df_total.groupby(['Date','Province_State'])['Death'].sum()
107/463: df_new_east = df_new_state.loc(axis=0)[pd.IndexSlice[:,['NewYork']]]
107/464: df_new_newyork = df_new_state.loc(axis=0)[pd.IndexSlice[:,['NewYork']]]
107/465: df_new_newyork
107/466: df_new_newyork.head()
107/467: df_new_newyork = df_new_state.loc(axis=0)[pd.IndexSlice[:,['NewYork']]]
107/468: df_new_newyork.head()
107/469: df_new_newyork = df_new_state.loc(axis=0)[pd.IndexSlice[:,['New York']]]
107/470: df_new_newyork.head()
107/471: df_new_newyork.Confirm.plot()
107/472: df_new_newyork.index.values
107/473: df_new_newyork.index.values(:)
107/474: df_new_newyork.index.values(axis=1)
107/475: df_new_newyork.ix[level="first"]
107/476: df_new_newyork.ix[level ="first"]
107/477: df_new_newyork.get_level_values('first')
107/478: df_new_newyork.xs('bar', level='first')
107/479: df_new_newyork = df_new_state.loc(axis=0)[pd.IndexSlice[:,['New York']]]
107/480: df_new_newyork.index.get_level_values('first')
107/481: df_new_newyork.index.get_level_values('Date')
107/482: plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork_confirm)
107/483: plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm)
107/484: plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm.diff())
107/485:
fig, ax = subplots(figsize = (6,10))
plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm.diff())
107/486:
fig, ax = plot.subplots(figsize = (6,10))
plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm.diff())
107/487:
fig, ax = plt.subplots(figsize = (6,10))
plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm.diff())
107/488:
fig, ax = plt.subplots(figsize = (6,5))
plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm.diff())
107/489:
fig, ax = plt.subplots(figsize = (8,5))
plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm.diff())
107/490: df_new_newjersy = df_new_state.loc(axis=0)[pd.IndexSlice[:,['New Jersy']]]
107/491:
fig, ax = plt.subplots(figsize = (8,5))
plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm.diff()) 
plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newjersy.Confirm.diff())
107/492:
fig, ax = plt.subplots(figsize = (8,5))
plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm.diff()) 
plt.plot(df_new_newjersy.index.get_level_values('Date'), df_new_newjersy.Confirm.diff())
107/493:
fig, ax = plt.subplots(figsize = (8,5))
plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm.diff(), c ='black') 
plt.plot(df_new_newjersy.index.get_level_values('Date'), df_new_newjersy.Confirm.diff(), c ='red')
107/494: df_new_newjersy = df_new_state.loc(axis=0)[pd.IndexSlice[:,['New Jersy']]]
107/495: df_new_newyork = df_new_state.loc(axis=0)[pd.IndexSlice[:,['New York']]]
107/496:
fig, ax = plt.subplots(figsize = (8,5))
plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm.diff(), c ='black') 
plt.plot(df_new_newjersy.index.get_level_values('Date'), df_new_newjersy.Confirm.diff(), c ='red')
107/497: df_new_newjersy = df_new_state.loc(axis=0)[pd.IndexSlice[:,['New Jersey']]]
107/498: df_new_newjersey = df_new_state.loc(axis=0)[pd.IndexSlice[:,['New Jersey']]]
107/499:
fig, ax = plt.subplots(figsize = (8,5))
plt.plot(df_new_newyork.index.get_level_values('Date'), df_new_newyork.Confirm.diff(), c ='black') 
plt.plot(df_new_newjersey.index.get_level_values('Date'), df_new_newjersey.Confirm.diff(), c ='red')
107/500:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    plt.plot(df_new_.index.get_level_values('Date'), df_new_.Confirm)
ax.set_prop_cycle
107/501:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    plt.plot(df_new_.index.get_level_values('Date'), df_new_.Confirm, label = state)
ax.set_prop_cycle
ax.lgend()
107/502:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    plt.plot(df_new_.index.get_level_values('Date'), df_new_.Confirm, label = state)
ax.set_prop_cycle
ax.legend()
107/503:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    plt.plot(df_new_.index.get_level_values('Date'), df_new_.Confirm.diff(), label = state)
ax.set_prop_cycle
ax.legend()
107/504:
pacific_states = ["Washington", "Oregon", "California"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in pacific_states:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    plt.plot(df_new_.index.get_level_values('Date'), df_new_.Confirm.diff(), label = state)
ax.set_prop_cycle
ax.legend()
107/505:
dc_alliance = ['District of Columbia','Virginia', 'Maryland']
fig, ax = plt.subplots(figsize = (8,5))
for state in dc_alliance:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    plt.plot(df_new_.index.get_level_values('Date'), df_new_.Confirm.diff(), label = state)
ax.set_prop_cycle
ax.legend()
107/506:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/507:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/508:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')

# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])

# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
107/509:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
107/510:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.plot(df_new['Confirm_diff'], df_new['Death_diff'])
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/511:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.plot(df_new['Confirm_diff'], df_new['Death_diff'])
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio', 'o')
107/512:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.scatter(df_new['Confirm_diff'], df_new['Death_diff'])
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
107/513:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.scatter(df_new['Confirm_diff'], df_new['Death_diff'])
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
ax3.set(xlabel ='Daily Confirm', ylabel = 'Daily Death')
107/514:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.scatter(df_new['Confirm'], df_new['Death'])
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
ax3.set(xlabel ='Confirm', ylabel = 'Death')
107/515:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio')
ax3.set(xlabel ='Confirm', ylabel = 'Death rate')
107/516:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
ax3.set(xlabel ='Confirm', ylabel = 'Death Rate')
107/517:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
%matplotlib inline
107/518:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[dc_alliance]]]
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(df_new_)
107/519: df_new_state.head()
107/520:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[pacific_states]]]
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(df_new_)
107/521:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,pacific_states]]
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(df_new_)
107/522: np_scaled.head()
107/523: np_scaled
107/524: np_scaled.plot()
107/525: np_scaled.shape
107/526:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    min_max_scaler = preprocessing.MinMaxScaler()
    df_new_scaled = min_max_scaler.fit_transform(df_new_.Confirm.diff())
    plt.plot(df_new_.index.get_level_values('Date'), df_new_scaled, label = state)
ax.set_prop_cycle
ax.legend()
107/527:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    min_max_scaler = preprocessing.MinMaxScaler()
    df_new_scaled = min_max_scaler.fit_transform(df_new_.Confirm.diff())
    print(df_new_scaled)
    plt.plot(df_new_.index.get_level_values('Date'), df_new_scaled, label = state)
ax.set_prop_cycle
ax.legend()
107/528:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    min_max_scaler = preprocessing.MinMaxScaler()
    df_new_scaled = min_max_scaler.fit_transform(df_new_.Confirm.diff())
    print(df_new_scaled.shape)
    plt.plot(df_new_.index.get_level_values('Date'), df_new_scaled, label = state)
ax.set_prop_cycle
ax.legend()
107/529:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(df_new_.Confirm)
np_scaled.shape
107/530:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(df_new_['Confirm'])
np_scaled.shape
107/531:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]
min_max_scaler = preprocessing.MinMaxScaler()
df_new_
107/532:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]
min_max_scaler = preprocessing.MinMaxScaler()
np_scaled = min_max_scaler.fit_transform(df_new_['Confirm'])
np_scaled.head()
107/533:
df_new_confirm = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]['Confirm']
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_)
107/534:
df_new_confirm = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]['Confirm']
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_)
plt.plot(df_new_.index.get_level_values('Date'),np_scaled)
107/535:
df_new_confirm = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]['Confirm']
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_confirm)
plt.plot(df_new_.index.get_level_values('Date'),np_scaled)
107/536:
df_new_confirm = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]['Confirm']
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_confirm)
plt.plot(df_new_.index.get_level_values('Date'),np_scaled)
107/537:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]['Confirm']
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_)
plt.plot(df_new_.index.get_level_values('Date'),np_scaled)
107/538:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]['Confirm']
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_)
plt.plot(df_new_.index.get_level_values('Date'), np_scaled)
107/539:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_)
plt.plot(df_new_.index.get_level_values('Date'), np_scaled)
107/540:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_[:,1])
plt.plot(df_new_.index.get_level_values('Date'), np_scaled)
107/541:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_[:,0])
plt.plot(df_new_.index.get_level_values('Date'), np_scaled)
107/542:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_[0,:])
plt.plot(df_new_.index.get_level_values('Date'), np_scaled)
107/543:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Connecticut']]
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_)
plt.plot(df_new_.index.get_level_values('Date'), np_scaled[:,0])
107/544:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
    df_new_scaled = min_max_scaler.fit_transform(df_new_)
    plt.plot(df_new_.index.get_level_values('Date'), np.diff(df_new_scaled[:,0]), label =state)
ax.set_prop_cycle
ax.legend()
107/545:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
    df_new_scaled = min_max_scaler.fit_transform(df_new_)
    plt.plot(df_new_.index.get_level_values('Date'), df_new_scaled[:,0], label =state)
ax.set_prop_cycle
ax.legend()
107/546:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
    df_new_scaled = min_max_scaler.fit_transform(df_new_)
    plt.plot(df_new_.index.get_level_values('Date'), np.diff(df_new_scaled[:,0]), label =state)
ax.set_prop_cycle
ax.legend()
107/547:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    df_new_['Confirm_diff'] = df_new_.Confirm.diff()
    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
    df_new_scaled = min_max_scaler.fit_transform(df_new_)
    plt.plot(df_new_.index.get_level_values('Date'), df_new_scaled[:,2], label =state)
ax.set_prop_cycle
ax.legend()
107/548:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_state['Confirm_diff'] = df_new_state.Confirm.diff()
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
    df_new_scaled = min_max_scaler.fit_transform(df_new_)
    plt.plot(df_new_.index.get_level_values('Date'), df_new_scaled[:,2], label =state)
ax.set_prop_cycle
ax.legend()
107/549:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_state['Confirm_diff'] = df_new_state.Confirm.diff()
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
    df_new_scaled = min_max_scaler.fit_transform(df_new_)
    plt.plot(df_new_.index.get_level_values('Date'), df_new_scaled[:,2], label =state)
ax.set_prop_cycle
ax.legend()
107/550:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']]
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_)
plt.plot(df_new_.index.get_level_values('Date'), np_scaled[:,0])
107/551:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']]
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_)
plt.plot(df_new_.index.get_level_values('Date'), np_scaled[:,0])
107/552:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']]
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_)
plt.plot(df_new_.index.get_level_values('Date'), df_new_.Confirm)
107/553:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']]
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_)
plt.plot(df_new_.index.get_level_values('Date'), df_new_.Confirm.diff())
107/554:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_state['Confirm_diff'] = df_new_state.Confirm.diff()
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
    df_new_scaled = min_max_scaler.fit_transform(df_new_)
    plt.plot(df_new_.index.get_level_values('Date'), df_new_scaled[:,-1], label =state)
ax.set_prop_cycle
ax.legend()
107/555:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    #df_new_state['Confirm_diff'] = df_new_state.Confirm.diff()
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
    df_new_scaled = min_max_scaler.fit_transform(df_new_)
    plt.plot(df_new_.index.get_level_values('Date'), df_new_scaled[:,-1], label =state)
ax.set_prop_cycle
ax.legend()
107/556:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')

# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])

# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
107/557:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
%matplotlib inline
107/558:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
107/559:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
107/560:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')

# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])

# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
107/561:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
107/562:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
ax3.set(xlabel ='Confirm', ylabel = 'Death Rate')
107/563: index_mask = pd.to_datetime(df_new.index.values) > '2020-03-01'
107/564:
fig, ax0 = plt.subplots( figsize=(8, 5))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
107/565:
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
107/566: df_new_state.head()
107/567:
df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']]
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
np_scaled = min_max_scaler.fit_transform(df_new_)
plt.plot(df_new_.index.get_level_values('Date'), df_new_.Confirm.diff())
107/568:
regional_advisory_council = ["Connecticut", "Delaware", "Massachusetts", "New Jersey", "New York", "Pennsylvania","Rhode Island"] 
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
    df_new_scaled = min_max_scaler.fit_transform(df_new_)
    plt.plot(df_new_.index.get_level_values('Date'), df_new_scaled[:,-1], label =state)
ax.set_prop_cycle
ax.legend()
107/569:
def alliance_states_plot(df_new_state, states):
    for state in states:
        df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
        df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
        df_new_scaled = min_max_scaler.fit_transform(df_new_)
        dic{states} = [df_new_.index.get_level_values('Date'), 
                       df_new_scaled]
        
    return dic
107/570:
def alliance_states(df_new_state, states, dic):
    for state in states:
        df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
        df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
        df_new_scaled = min_max_scaler.fit_transform(df_new_)
        dic[states] = [df_new_.index.get_level_values('Date'), 
                       df_new_scaled]
    return dic
107/571:
dic = {}
dates_, scaled_ = alliance_states(df_new_state, regional_advisory_council)
dic[regional_advisory_council].append([dates_, scaled])
107/572:
def alliance_states(df_new_state, states):
    for state in states:
        df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
        df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
        df_new_scaled = min_max_scaler.fit_transform(df_new_)
    return df_new_.index.get_level_values('Date'), df_new_scaled
107/573:
dic = {}
dates_, scaled_ = alliance_states(df_new_state, regional_advisory_council)
dic[regional_advisory_council].append([dates_, scaled])
107/574:
dic = {}
dates_, scaled_ = alliance_states(df_new_state, regional_advisory_council)
dic[tuple(regional_advisory_council)].append([dates_, scaled])
107/575:
dic = {}
ss = regional_advisory_council
dates_, scaled_ = alliance_states(df_new_state, ss)
dic[str(ss)].append([dates_, scaled])
107/576:
dic = {}
ss = regional_advisory_council
dates_, scaled_ = alliance_states(df_new_state, ss)
dic[str(tuple(ss))].append([dates_, scaled])
107/577:
dic = {}
dates_, scaled_ = alliance_states(df_new_state, regional_advisory_council)
dic[regional_advisory_council].append([dates_, scaled])
107/578:
dic = {}
dates_, scaled_ = alliance_states(df_new_state, regional_advisory_council)
dic[str(eval(regional_advisory_council))].append([dates_, scaled])
107/579:
dates_, scaled_ = alliance_states(df_new_state, regional_advisory_council)
dic[regional_advisory_council].append([dates_, scaled])
107/580:
dates_, scaled_ = alliance_states(df_new_state, regional_advisory_council)
dic[set(regional_advisory_council)].append([dates_, scaled])
107/581:
dates_, scaled_ = alliance_states(df_new_state, regional_advisory_council)
dic[set(regional_advisory_council)] = [dates_, scaled]
107/582:
dates_, scaled_ = alliance_states(df_new_state, regional_advisory_council)
dic[regional_advisory_council] = [dates_, scaled_]
107/583:
dates_, scaled_ = alliance_states(df_new_state, regional_advisory_council)
dic[tuple(regional_advisory_council)] = [dates_, scaled_]
107/584: pd.merge(df_new_state, df_total['Population'], how ='left', on = 'Province_State')
107/585: pd.merge(df_new_state, df_total[['Population', 'Province_State']], how ='left', on = 'Province_State')
107/586:
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
107/587: pd.merge(df_new_state, df_total[['Population', 'Province_State']], how ='left', on = 'Province_State')
107/588: df_new_state
107/589: pd.merge(df_new_state, df_total[['Date','Population', 'Province_State']], how ='left', on = ['Date','Province_State'])
107/590: df_new_state.join(df_total[['Date','Population', 'Province_State'].set_index('Date')])
107/591: df_new_state.join(df_total[['Date','Population', 'Province_State']].set_index('Date')])
107/592: df_new_state.join(df_total[['Date','Population', 'Province_State']].set_index('Date'))
107/593: df_new_state.join(df_total[['Date','Population', 'Province_State']])
107/594: df_new_state.join(df_total[['Date','Population', 'Province_State']].rename_axis(['Date', 'Province_State']))
107/595: df_new_state.join(df_total[['Date','Population', 'Province_State']].rename_axis(['Date', 'Province_State']).rename('Population'))
107/596: df_new_state.join(df_total['Population'], on= ['Province_State'])
107/597: df_new_state.concat(df_total['Population'], on= ['Province_State'])
107/598: df_new_state.join(df_total[['Population', 'Province_State']], on= ['Province_State'])
107/599:
firsts = df_new_state.index.get_level_values('Province_State')
df_new_state['Province_State'] = df_total.loc[firsts].values
df_new_state
107/600:
firsts = df_new_state.index.get_level_values('Province_State')
df_new_state['Province_State'] = df_total.loc[firsts].values
df_new_state.head()
107/601:
df_new_state.set_index(['Date', 'Province_State'])
firsts = df_new_state.index.get_level_values('Province_State')
df_new_state['Province_State'] = df_total.loc[firsts].values
df_new_state.head()
107/602:
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
107/603: df_new_state.head()
107/604:
df_new_state.set_index(['Date', 'Province_State'])
firsts = df_new_state.index.get_level_values('Province_State')
df_new_state['Population'] = df_total.loc[firsts].Population
df_new_state.head()
107/605:
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
107/606: df_new_state.head()
107/607:
df_new_state.set_index(['Date', 'Province_State'])
firsts = df_new_state.index.get_level_values('Province_State')
df_new_state['Population'] = df_total.loc[firsts].Population
df_new_state.head()
107/608:
df_new_state.set_index(['Date', 'Province_State'])
firsts = df_new_state.index.get_level_values('Province_State')
df_new_state['Population'] = df_total[['Province_State', 'Population']].loc[firsts].Population
df_new_state.head()
107/609:
firsts = df_new_state.index.get_level_values('Province_State')
df_new_state['Population'] = df_total[['Province_State', 'Population']].loc[firsts].Population
df_new_state.head()
107/610:
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
107/611: df_new_state.head()
107/612:
firsts = df_new_state.index.get_level_values('Province_State')
df_new_state['Population'] = df_total[['Province_State', 'Population']].loc[firsts].Population
df_new_state.head()
107/613:
firsts = df_new_state.index.get_level_values('Province_State')
df_new_state['Population'] = df_total[['Province_State', 'Population']].set_index(['Province_State']).loc[firsts].Population
df_new_state.head()
108/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
%matplotlib inline
108/2:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
108/3:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
108/4:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')

# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])

# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
108/5:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')

df_total = pd.concat([df_confirm_new, df_death_new])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])

# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
108/6: df_confirm_new.head()
108/7: df_confirm_new.join(df_death_new)
108/8: df_confirm_new.join(df_death_new, how ='left')
108/9: df_confirm_new.join(df_death_new, on = ['Province_State', 'Date'], how ='left')
108/10: df_confirm_new.join(df_death_new, on = ['Province_State', 'Date'])
108/11: df_confirm_new.shape
108/12: df_death_new.shape
108/13: df_death_new.info()
108/14: df_confirm_new.head()
108/15: df_confirm_new.join(df_death_new, on = ['UID'])
108/16: df_confirm_new.merge(df_death_new, on = ['UID'])
108/17: df_confirm_new.join(df_death_new, on = ['UID'], lsuffix='_x', rsuffix='_y')
108/18: df_confirm_new.head()
108/19: df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date'])
108/20: df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
108/21: df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
108/22: df_total.shape
108/23:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')


df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])


# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
108/24:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
%matplotlib inline
108/25:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
108/26:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
108/27:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')


df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])


# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
108/28: df_new.head()
108/29:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
108/30:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
ax3.set(xlabel ='Confirm', ylabel = 'Death Rate')
108/31: index_mask = pd.to_datetime(df_new.index.values) > '2020-03-01'
108/32:
fig, ax0 = plt.subplots( figsize=(8, 5))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
108/33: df_total.head()
108/34:
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
108/35: df_new_state.head()
108/36:
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Population'].mean()
108/37: df_new_state.head()
108/38: df_total['Province_State','Population']
108/39: df_total[['Province_State','Population']]
108/40: df_total.groupby(['Date','Province_State'])['Population'].sum()
108/41:
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Population'].sum()
108/42: df_new_state.head()
108/43:
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
df_new_state['Population'] = df_total.groupby(['Date','Province_State'])['Population'].sum()
108/44: df_new_state.head()
108/45: df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']]
108/46: df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']].apply(['Confirm']/['Population'])
108/47: df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']].apply(lambda x: x['Confirm']/x['Population'])
108/48: df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']].apply(lambda x: x['Confirm']/x['Population'])
108/49: df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']].apply(lambda x: def div: return x['Confirm']/x['Population'])
108/50: df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']].apply(lambda x: def div_: return x['Confirm']/x['Population'])
108/51: df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']].apply(lambda x: def div_: return np.divide(x['Confirm'], x['Population'])
108/52:
def div_two2(df):
    return df['Confirm'] / df['Population']
108/53: df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']].apply(div_two2)
108/54: df_new__ = df_new_state.loc(axis=0)[pd.IndexSlice[:,'Delaware']]
108/55: df_new__['Confirm']/df_new__['Population'].plot()
108/56: (df_new__['Confirm']/df_new__['Population']).plot()
108/57: df_new__.info()
108/58:
# Date starting from March 1st
index_mask = pd.to_datetime(df_new.index.values) > '2020-03-01'
# plot figure
fig, ax0 = plt.subplots( figsize=(8, 5))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
108/59: df_new_state.head()
108/60:
regional_advisory_council = ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"]
fig, ax = plt.subplots(figsize = (8,5))
for state in regional_advisory_council:
    df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
    df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
    min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
    df_new_scaled = min_max_scaler.fit_transform(df_new_)
    plt.plot(df_new_.index.get_level_values('Date'), df_new_scaled[:,-1], label =state)
ax.set_prop_cycle
ax.legend()
108/61: df_new_states.head()
108/62: df_new_state.head()
108/63: df_new_state.head()
108/64:
fig, ax = plt.plot(figsize = (10, 10))
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
i = 0
for key, states in state_dic:
    dates_, scaled_data = alliance_states(df_new_states, states)
    ax[i].plot(dates_, scaled_data.Confirm.diff(), legend = key)
    i += 1

ax.set_prop_cycle
ax.legend()
108/65:
fig, ax = plt.plot(figsize = (10, 10))
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
i = 0
for key, states in state_dic:
    print(states)
    dates_, scaled_data = alliance_states(df_new_states, states)
    ax[i].plot(dates_, scaled_data.Confirm.diff(), legend = key)
    i += 1

ax.set_prop_cycle
ax.legend()
108/66:
fig, ax = plt.subplots(figsize = (10, 10))
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
i = 0
for key, states in state_dic:
    print(states)
    dates_, scaled_data = alliance_states(df_new_states, states)
    ax[i].plot(dates_, scaled_data.Confirm.diff(), legend = key)
    i += 1

ax.set_prop_cycle
ax.legend()
108/67:
fig, ax = plt.subplots(figsize = (10, 10))
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
i = 0
for key, states in state_dic.items():
    print(states)
    dates_, scaled_data = alliance_states(df_new_states, states)
    ax[i].plot(dates_, scaled_data.Confirm.diff(), legend = key)
    i += 1

ax.set_prop_cycle
ax.legend()
108/68:
# function that sum all info based on alliance states
def alliance_states(df_new_state, states):
    for state in states:
        df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
        df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
        df_new_scaled = min_max_scaler.fit_transform(df_new_)
    return df_new_.index.get_level_values('Date'), df_new_scaled
108/69:
fig, ax = plt.subplots(figsize = (10, 10))
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
i = 0
for key, states in state_dic.items():
    print(states)
    dates_, scaled_data = alliance_states(df_new_states, states)
    ax[i].plot(dates_, scaled_data.Confirm.diff(), legend = key)
    i += 1

ax.set_prop_cycle
ax.legend()
108/70: df_new_state.head()
108/71:
fig, ax = plt.subplots(figsize = (10, 10))
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
i = 0
for key, states in state_dic.items():
    print(states)
    dates_, scaled_data = alliance_states(df_new_state, states)
    ax[i].plot(dates_, scaled_data.Confirm.diff(), legend = key)
    i += 1

ax.set_prop_cycle
ax.legend()
108/72:

state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    print(states)
    dates_, scaled_data = alliance_states(df_new_state, states)
    ax[i].plot(dates_, scaled_data.Confirm.diff(), legend = key)
    i += 1

ax.set_prop_cycle
ax.legend()
108/73: x, y = alliance_states(df_new_state, ["Washington", "Oregon", "California"])
108/74: y
108/75:

state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    print(states)
    dates_, scaled_data = alliance_states(df_new_state, states)
    ax[i].plot(dates_, scaled_data, legend = key)
    i += 1

ax.set_prop_cycle
ax.legend()
108/76: x, y = alliance_states(df_new_state, ["Washington", "Oregon", "California"])
108/77: y.shape
108/78:

state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    dates_, scaled_data = alliance_states(df_new_state, states)
    ax[i].plot(dates_, scaled_data[:,-1], legend = key)
    i += 1

ax.set_prop_cycle
ax.legend()
108/79: y[:,-1]
108/80:

state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    dates_, scaled_data = alliance_states(df_new_state, states)
    ax[i].plot(dates_, scaled_data[:,-1], label = key)
    i += 1

ax.set_prop_cycle
ax.legend()
108/81:
# function that sum all info based on alliance states
def alliance_states(df_new_state, states):
    dic ={}
    for idx, state in enumerate(states):
        df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
        df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
        df_new_scaled = min_max_scaler.fit_transform(df_new_)
        dic[state] = [df_new_.index.get_level_values('Date'), df_new_scaled[:,-1]]
    return dic
108/82:

state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    states_data = alliance_states(df_new_state, states)
    for state in states_data:
        ax[i].plot(states_data[state][0], states_data[state][1], label = key)
    ax[i].set_prop_cycle
    ax[i].legend()
    i += 1
108/83:

state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    states_data = alliance_states(df_new_state, states)
    for state in states_data:
        ax[i].plot(states_data[state][0], states_data[state][1], label = state)
    ax[i].set_prop_cycle
    ax[i].legend()
    i += 1
108/84:
# Normalized time course of daily confirmation cased on different states
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    states_data = alliance_states(df_new_state, states)
    for state in states_data:
        ax[i].plot(states_data[state][0], states_data[state][1], label = state)
    ax[i].set_prop_cycle
    ax[i].legend()
    i += 1
109/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
%matplotlib inline
109/2:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
109/3:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
109/4:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')


df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])


# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
109/5:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
109/6:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
ax3.set(xlabel ='Confirm', ylabel = 'Death Rate')
109/7:
# Date starting from March 1st
index_mask = pd.to_datetime(df_new.index.values) > '2020-03-01'
# plot figure
fig, ax0 = plt.subplots( figsize=(8, 5))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
109/8:
# sum table based on date and states
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
df_new_state['Population'] = df_total.groupby(['Date','Province_State'])['Population'].sum()
109/9:
# function that sum all info based on alliance states
def alliance_states(df_new_state, states):
    dic ={}
    for idx, state in enumerate(states):
        df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
        df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
        df_new_scaled = min_max_scaler.fit_transform(df_new_)
        dic[state] = [df_new_.index.get_level_values('Date'), df_new_scaled[:,-1]]
    return dic
109/10:
# Normalized time course of daily confirmation cased on different states
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    states_data = alliance_states(df_new_state, states)
    for state in states_data:
        ax[i].plot(states_data[state][0], states_data[state][1], label = state)
    ax[i].set_prop_cycle
    ax[i].legend()
    i += 1
109/11: df_new_state.head()
109/12: df_new_state['ConfirmVSPopulation'] = df_new_state['Confirm']/df_new_state['Population']
109/13: df_new_state.head()
109/14: df_new_state.tail()
109/15: df_new_state[ df_new_state['Date'] == '2020-04-15']
109/16: df_new_state[ df_new_state.index== '2020-04-15']
109/17: df_new_state[ df_new_state.index == pd.to_datetime('2020-04-15')]
109/18: df_new_state[df_new_state.index == pd.to_datetime('2020-04-15')]
109/19: df_new_state.index
109/20: df_new_state.ix[level="first"]
109/21: df_new_state.ix[level="Date"]
109/22: df_new_state.ix[ level=="Date"]
109/23: df_new_state.ix
109/24: print(df_new_state.ix)
109/25: df_new_state.get_level_values('Date')
109/26: df_new_state.info()
109/27: df.index.get_level_values(0)
109/28: df_new_state.index.get_level_values(0)
109/29: df_new_state.index.get_level_values('Date')
109/30: df_new_state.index.get_level_values('Date')[-1]
109/31: df_new_state[df_new_state.index.get_level_values('Date')[-1]]
109/32: df_new_state[df_new_state.index.get_level_values('Date') == df_new_state.index.get_level_values('Date')[-1]]
109/33: last_df = df_new_state[df_new_state.index.get_level_values('Date') == df_new_state.index.get_level_values('Date')[-1]]
109/34: last_df.reset_index(level=0, drop=True)
109/35: sns.heatmap(df_new)
109/36: df_new_state.head()
109/37: result = df_new_state.pivot(index = "Date", columns = "Province_State", values ="Confirm")
109/38: df_total.head()
109/39: result = df_total.pivot(index = "Date", columns = "Province_State", values ="Confirm")
109/40: df_total.info()
109/41: result = df_total.pivot(index = "Date", columns = "Province_State", values ="Confirm")
109/42: df_total.reset_index()
109/43: result = df_total.reset_index().pivot(index = "Date", columns = "Province_State", values ="Confirm")
109/44: result = df_total.pivot(index = "Date", columns = "Province_State", values ="Confirm")
109/45: result = df_total.pivot(index = df_total.index.values, columns = "Province_State", values ="Confirm")
109/46: result = df_total.pivot(index = df_total['Date'], columns = "Province_State", values ="Confirm")
109/47: result = df_total.pivot(index = df_total['Date'], columns = df_total["Province_State"], values = df_total["Confirm"])
109/48: df_new_state
109/49:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
%matplotlib inline
109/50:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
109/51:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
109/52:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')


df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])


# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
109/53:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
109/54:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
ax3.set(xlabel ='Confirm', ylabel = 'Death Rate')
109/55:
# Date starting from March 1st
index_mask = pd.to_datetime(df_new.index.values) > '2020-03-01'
# plot figure
fig, ax0 = plt.subplots( figsize=(8, 5))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
109/56:
# sum table based on date and states
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
df_new_state['Population'] = df_total.groupby(['Date','Province_State'])['Population'].sum()
109/57:
# function that sum all info based on alliance states
def alliance_states(df_new_state, states):
    dic ={}
    for idx, state in enumerate(states):
        df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
        df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
        df_new_scaled = min_max_scaler.fit_transform(df_new_)
        dic[state] = [df_new_.index.get_level_values('Date'), df_new_scaled[:,-1]]
    return dic
109/58:
# Normalized time course of daily confirmation cased on different states
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    states_data = alliance_states(df_new_state, states)
    for state in states_data:
        ax[i].plot(states_data[state][0], states_data[state][1], label = state)
    ax[i].set_prop_cycle
    ax[i].legend()
    i += 1
109/59: df_new_state.head()
109/60: df_new_state_pivot = df_new_state.pivot_table(index ="Date", columns = "Population", values = "Confirm")
109/61: df_new_state_pivot.head()
109/62: sns.heatmap(df_new_state_pivot)
109/63: df_new_state.Population.unique()
109/64: len(df_new_state.Population.unique())
109/65: df_new_state_pivot.shape
109/66: df_new_state_pivot = df_new_state.pivot_table(index ="Date", columns = "Population", values = "Confirm", aggfunc = "mean")
109/67: sns.heatmap(df_new_state_pivot)
109/68: df_new_state_pivot = df_new_state.pivot_table(index ="Date", columns = "Population", values = "Confirm")
109/69: sns.heatmap(df_new_state_pivot)
109/70: df_new_state['ConfirmVSPopulation'] = df_new_state['Confirm']/df_new_state['Population']
109/71: df_new_state.head()
109/72: df_new_state_pivot = df_new_state.pivot_table(index ="Date", columns = "Population", values = "ConfirmVSPopulation")
109/73: sns.heatmap(df_new_state_pivot)
109/74: df_new_state_pivot = df_new_state.pivot_table(index = df_new_state[df_new[index_mask]]["Date"], columns = "Population", values = "ConfirmVSPopulation")
109/75: df_new_state[index_mask]
109/76: df_new_state_pivot = df_new_state.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
109/77: df_new_state[df_new_state["Date"] > '2020-03-01']
109/78: df_new_state["Date"] > '2020-03-01'
109/79: df_new_state["Date"]
109/80: df_new_state
109/81: df_new_state.index
109/82: pd.to_datetime(df_new.index.values) > '2020-03-01'
109/83: index_mask
109/84: df_new_state[index_mask]
109/85: index_mask = pd.to_datetime(df_new_state.index.values) > '2020-03-01'
109/86: df_new_state.index.values > '2020-03-01'
109/87: df_new_state.index.values
109/88: df_new_state.get_level_values(0)
109/89: df_new_state.head()
109/90: df_new_state.index.get_level_values(0)
109/91: df_new_state.index.get_level_values(0) > '2020-03-01'
109/92: dff = df_new_state[df_new_state.index.get_level_values(0) > '2020-03-01']
109/93: dff
109/94: df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
109/95: sns.heatmap(df_new_state_pivot)
109/96: dff
109/97: df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "Confirm")
109/98: sns.heatmap(df_new_state_pivot)
109/99: sns.heatmap(df_new_state_pivot,vmin=0, vmax=1)
109/100: sns.heatmap(df_new_state_pivot,vmin=0, vmax=2)
109/101: sns.heatmap(df_new_state_pivot,vmin=0)
109/102: df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
109/103: sns.heatmap(df_new_state_pivot,vmin=0)
109/104: sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.01)
109/105: sns.heatmap(df_new_state_pivot,vmin=0, vmax=max(df_new_state['ConfirmVSPopulation']))
109/106: max(df_new_state['ConfirmVSPopulation'] )
109/107: min(df_new_state['ConfirmVSPopulation'])
109/108: df_new_state
109/109: df_new_state.dropna(how ="all")
109/110: df_new_state = df_new_state.dropna(how ="all")
109/111: dff = df_new_state[df_new_state.index.get_level_values(0) >= '2020-03-01']
109/112: df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
109/113: sns.heatmap(df_new_state_pivot,vmin=0, vmax=max(df_new_state['ConfirmVSPopulation']))
109/114: max(df_new_state['ConfirmVSPopulation'])
109/115: df_new_state
109/116: df_new_state = df_new_state.dropna(how ="all")
109/117: df_new_state
109/118: sns.heatmap(df_new_state_pivot,vmin=0, vmax=1)
109/119: sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.01)
109/120: sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.05)
109/121: sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.02)
109/122: dff = df_new_state[df_new_state.index.get_level_values(0) >= '2020-03-15']
109/123: df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
109/124: sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.02)
109/125: dff = df_new_state[df_new_state.index.get_level_values(0) >= '2020-03-20']
109/126: df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
109/127: sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.02)
109/128: sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.01)
109/129:
df_new_state['ConfirmVSPopulation'] = df_new_state['Confirm']/df_new_state['Population']
dff = df_new_state[df_new_state.index.get_level_values(0) >= '2020-03-20']
109/130:
df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.01)
110/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
%matplotlib inline
110/2:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
110/3:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
110/4:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')


df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])


# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
110/5:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
110/6:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
ax3.set(xlabel ='Confirm', ylabel = 'Death Rate')
110/7:
# Date starting from March 1st
index_mask = pd.to_datetime(df_new.index.values) > '2020-03-01'
# plot figure
fig, ax0 = plt.subplots( figsize=(8, 5))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
110/8:
# sum table based on date and states
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
df_new_state['Population'] = df_total.groupby(['Date','Province_State'])['Population'].sum()
110/9:
# function that sum all info based on alliance states
def alliance_states(df_new_state, states):
    dic ={}
    for idx, state in enumerate(states):
        df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
        df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
        df_new_scaled = min_max_scaler.fit_transform(df_new_)
        dic[state] = [df_new_.index.get_level_values('Date'), df_new_scaled[:,-1]]
    return dic
110/10:
# Normalized time course of daily confirmation cased on different states
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    states_data = alliance_states(df_new_state, states)
    for state in states_data:
        ax[i].plot(states_data[state][0], states_data[state][1], label = state)
    ax[i].set_prop_cycle
    ax[i].legend()
    i += 1
110/11:
# Add confirm case based on population - normalized the data
df_new_state['ConfirmVSPopulation'] = df_new_state['Confirm']/df_new_state['Population']
# only observe when pandamic occurs
dff = df_new_state[df_new_state.index.get_level_values(0) >= '2020-03-20']
110/12:
# generate pivot table and heatmap
df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.01)
110/13:
# sum table based on date and states
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
df_new_state['Population'] = df_total.groupby(['Date','Province_State'])['Population'].sum()
110/14:
# Add confirm case based on population - normalized the data
df_new_state['ConfirmVSPopulation'] = df_new_state['Confirm']/df_new_state['Population']
# only observe when pandamic occurs
dff = df_new_state[df_new_state.index.get_level_values(0) >= '2020-03-20']
110/15:
# generate pivot table and heatmap
df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.01)
111/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
%matplotlib inline
111/2:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
111/3:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
111/4:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')


df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])


# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
111/5:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
111/6:
# Create figure and plot space
fig, (ax0, ax1, ax2, ax3) = plt.subplots(nrows = 4, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax3.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
ax3.set(xlabel ='Confirm', ylabel = 'Death Rate')
111/7:
# Date starting from March 1st
index_mask = pd.to_datetime(df_new.index.values) > '2020-03-01'
# plot figure
fig, ax0 = plt.subplots( figsize=(8, 5))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
111/8:
# sum table based on date and states
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
df_new_state['Population'] = df_total.groupby(['Date','Province_State'])['Population'].sum()
111/9:
# function that sum all info based on alliance states
def alliance_states(df_new_state, states):
    dic ={}
    for idx, state in enumerate(states):
        df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
        df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
        df_new_scaled = min_max_scaler.fit_transform(df_new_)
        dic[state] = [df_new_.index.get_level_values('Date'), df_new_scaled[:,-1]]
    return dic
111/10:
# Add confirm case based on population - normalized the data
df_new_state['ConfirmVSPopulation'] = df_new_state['Confirm']/df_new_state['Population']
# only observe when pandamic occurs
dff = df_new_state[df_new_state.index.get_level_values(0) >= '2020-03-20']
111/11:
# generate pivot table and heatmap
df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.01)
111/12:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
111/13:
fig, ax = subplot(figsize = 8,5)
ax.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax.set(xlabel ='Confirm', ylabel = 'Death Rate')
111/14:
fig, ax = plt.subplots(figsize = (8,5))
ax.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax.set(xlabel ='Confirm', ylabel = 'Death Rate')
112/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
%matplotlib inline
112/2:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
112/3:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
112/4:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')


df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])


# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
112/5:
def div_two(df):
    return df['Death'].sum() / df['Confirm'].sum()
112/6:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
112/7:
fig, ax = plt.subplots(figsize = (8,5))
ax.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax.set(xlabel ='Confirm', ylabel = 'Death Rate')
112/8:
# Date starting from March 1st
index_mask = pd.to_datetime(df_new.index.values) > '2020-03-01'
# plot figure
fig, ax0 = plt.subplots( figsize=(8, 5))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
112/9:
# sum table based on date and states
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
df_new_state['Population'] = df_total.groupby(['Date','Province_State'])['Population'].sum()
112/10:
# function that sum all info based on alliance states
def alliance_states(df_new_state, states):
    dic ={}
    for idx, state in enumerate(states):
        df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
        df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
        df_new_scaled = min_max_scaler.fit_transform(df_new_)
        dic[state] = [df_new_.index.get_level_values('Date'), df_new_scaled[:,-1]]
    return dic
112/11:
# Normalized time course of daily confirmation cased on different states
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    states_data = alliance_states(df_new_state, states)
    for state in states_data:
        ax[i].plot(states_data[state][0], states_data[state][1], label = state)
    ax[i].set_prop_cycle
    ax[i].legend()
    i += 1
112/12:
# Add confirm case based on population - normalized the data
df_new_state['ConfirmVSPopulation'] = df_new_state['Confirm']/df_new_state['Population']
# only observe when pandamic occurs
dff = df_new_state[df_new_state.index.get_level_values(0) >= '2020-03-20']
112/13:
# generate pivot table and heatmap
df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.01)
114/1: import pandas as pd
114/2:
df17 = pd.read_csv('PartD_Prescriber_PUF_NPI_17/PartD_Prescriber_PUF_NPI_17.txt')
df17.head()
114/3:
df17 = pd.read_csv('PartD_Prescriber_PUF_NPI_17/PartD_Prescriber_PUF_NPI_17.txt', error_bad_lines=False)
df17.head()
114/4:
df17 = pd.read_csv('PartD_Prescriber_PUF_NPI_17/PartD_Prescriber_PUF_NPI_17.txt', error_bad_lines=False)
pd.options.display.max_columns = 999
114/5: pd.get_option("display.max_columns")
114/6:
from decimal import *
getcontext().prec = 10
114/7: df17
114/8:
df17 = pd.read_csv('PartD_Prescriber_PUF_NPI_17/PartD_Prescriber_PUF_NPI_17.txt')
pd.options.display.max_columns = 999
114/9:
df17 = pd.read_sas('PartD_Prescriber_PUF_NPI_17/PartD_Prescriber_PUF_NPI_17.sas')
df17.head()
114/10:
df17 = pd.read_csv('PartD_Prescriber_PUF_NPI_16/PartD_Prescriber_PUF_NPI_16.txt')
df17.head()
114/11:
df17 = pd.read_text('PartD_Prescriber_PUF_NPI_16/PartD_Prescriber_PUF_NPI_16.txt')
df17.head()
114/12:
df17 = pd.read_txt('PartD_Prescriber_PUF_NPI_16/PartD_Prescriber_PUF_NPI_16.txt')
df17.head()
114/13:
df17 = pd.read_table('PartD_Prescriber_PUF_NPI_16/PartD_Prescriber_PUF_NPI_16.txt')
df17.head()
114/14:
df17 = pd.read_table('PartD_Prescriber_PUF_NPI_16/PartD_Prescriber_PUF_NPI_16.txt', sep ='\t')
df17.head()
114/15:
df17 = pd.read_csv('PartD_Prescriber_PUF_NPI_16/PartD_Prescriber_PUF_NPI_16.txt', sep ='\t')
df17.head()
114/16: df17.describe()
114/17: df17.info()
114/18: (df17['BENE_COUNT'] <= 10).sum/ df17.shape[0]
114/19: (df17['bene_count '] <= 10).sum/ df17.shape[0]
114/20: (df17['bene_count'] <= 10).sum/ df17.shape[0]
114/21: (df17['bene_count'] <= 10).sum()/ df17.shape[0]
114/22: (df17['bene_count'] <= 10).sum()
114/23: (df17['bene_count'] <= 10).sum()
114/24: (df17['bene_count'] >10).sum()
114/25: (df17['bene_count'] <=10).sum()
114/26:
# double check the number lower than 10 beneficiaries. 
(df17['bene_count'] <=10).sum()
avg_bene = df17.groupby('NPI')['bene_count'].mean()
114/27:
# double check the number lower than 10 beneficiaries. 
(df17['bene_count'] <=10).sum()
avg_bene = df17.groupby('npi')['bene_count'].mean()
114/28:
# double check the number lower than 10 beneficiaries. 
(df17['bene_count'] <=10).sum()
avg_bene = df17.groupby('npi')['bene_count'].mean()
avg_ben
114/29:
# double check the number lower than 10 beneficiaries. 
(df17['bene_count'] <=10).sum()
avg_bene = df17.groupby('npi')['bene_count'].mean()
avg_bene
114/30:
# double check the number lower than 10 beneficiaries. 
(df17['bene_count'] <=10).sum()
avg_bene = df17.groupby('npi')['bene_count']
avg_bene
114/31:
# double check the number lower than 10 beneficiaries. 
(df17['bene_count'] <=10).sum()
avg_bene = df17.groupby('npi')['bene_count'].count()
avg_bene
114/32: (df17['bene_count'] <=10).sum()
114/33:  df['bene_count'].isnull().sum()
114/34:  df17['bene_count'].isnull().sum()
114/35: (df17['bene_count'] <=10).sum(),  df17['bene_count'].isnull().sum()
114/36: df17[df17['bene_count']]['npi']
114/37: df17[df17['bene_count'].isnull()]['npi']
114/38: df17.loc(10)
114/39: df17.iloc(10)
114/40: df17.iloc[10]
114/41: df17.iloc[10].head()
114/42: df17.iloc[10]
114/43: df17.shape[0]
114/44:
# double check the number lower than 10 beneficiaries. 
df17[]
114/45:
# double check the number lower than 10 beneficiaries.
(df17['bene_count'] <=10).sum(),  df17['bene_count'].isnull().sum()
114/46:
# calculate average 
df17['bene_count'].notna().sum()/ len(df17['bene_count'].notna())
114/47:
# calculate average 
df17['bene_count'].notna().sum()
114/48:
avg_=np.sum(df17['bene_count'])/len(df17['npi'])
print('%s' % float('%.10g' % AVE_BENE_COUNT_per_provider))
114/49:
import pandas as pd
import numpy as np
114/50:
avg_=np.sum(df17['bene_count'])/len(df17['npi'])
print('%s' % float('%.10g' % AVE_BENE_COUNT_per_provider))
114/51:
avg_=np.sum(df17['bene_count'])/len(df17['npi'])
print('%s' % float('%.10g' % avg_))
114/52:
# calculate average 
df17['bene_count'].notna().sum()
114/53: np.sum(df17['bene_count'])
114/54:
# calculate average 
df17['bene_count'].notna()
114/55:
# calculate average 
df17['bene_count'].dropna().sum()
114/56: df17['bene_count'].sum)
114/57: df17['bene_count'].sum()
114/58:
# calculate average 
df17['bene_count'].sum()/ df17['bene_count'].notna().sum()
114/59:
# calculate average 
avg_ = df17['bene_count'].sum()/ df17['bene_count'].notna().sum()
print('average number of beneficiaries per provide is %2d'.format(avg_))
114/60:
# calculate average 
avg_ = df17['bene_count'].sum()/ df17['bene_count'].notna().sum()
print('average number of beneficiaries per provide is %s'.format(avg_))
114/61:
# calculate average 
avg_ = df17['bene_count'].sum()/ df17['bene_count'].notna().sum()
print('average number of beneficiaries per provide is %s'.format(%avg_))
114/62:
# calculate average 
avg_ = df17['bene_count'].sum()/ df17['bene_count'].notna().sum()
print('average number of beneficiaries per provide is %2d'.format(%avg_))
114/63:
# calculate average 
avg_ = df17['bene_count'].sum()/ df17['bene_count'].notna().sum()
print('average number of beneficiaries per provide is %2d'.format( '%.10g' %avg_))
114/64:
# calculate average 
avg_ = df17['bene_count'].sum()/ df17['bene_count'].notna().sum()
print('average number of beneficiaries per provide is {:f}'.format(avg_))
114/65:
# calculate average 
avg_ = df17['bene_count'].sum()/ df17['bene_count'].notna().sum()
print('Average number of beneficiaries per provide is {:f}'.format(avg_))
114/66:
# calculate average 
avg_ = df17['bene_count'].sum()/ df17['bene_count'].notna().sum()
print('Average number of beneficiaries per provide is {:f}'.format(round(avg_,2))
114/67:
# calculate average 
avg_ = df17['bene_count'].sum()/ df17['bene_count'].notna().sum()
print('Average number of beneficiaries per provide is {:f}'.format(round(avg_))
114/68:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
print('Average number of beneficiaries per provide is {:f}'.format(avg_)
114/69:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
avg_
print('Average number of beneficiaries per provide is {:f}'.format(avg_)
114/70:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
print(avg_)
#print('Average number of beneficiaries per provide is {:f}'.format(avg_)
114/71:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
print(avg_)
print('Average number of beneficiaries per provide is {:f}'.format(float(avg_))
114/72:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
print('Average number of beneficiaries per provide is {:f}'.format(avg_)
114/73:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
print('Average number of beneficiaries per provide is {:f}'.format(avg_))
114/74:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
print('Average number of beneficiaries per provide is {:f}'.format(round(avg_),2)
114/75:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
print('Average number of beneficiaries per provide is {:f}'.format(round(avg_),2))
114/76:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
print('Average number of beneficiaries per provide is {:f}'.format(avg_)
114/77:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
print('Average number of beneficiaries per provide is {:f}'.format(avg_))
114/78:
# double check the number lower than 10 beneficiaries.
(df17['bene_count'] <=10).sum(),  df17['bene_count'].isnull().sum()
114/79: (df17['total_claim_count'] <1_000).sum(),  df17['total_claim_count'].isnull().sum()
114/80: df17.groupby('npi').['total_claim_count'].sum()
114/81: df17.groupby('npi')['total_claim_count'].sum()
114/82: df17['total_claim_count'].isnull().sum()
114/83: avg_length = df['total_day_supply'].divide(df['total_claim_count'])
114/84: avg_length = df17['total_day_supply'].divide(df17['total_claim_count'])
114/85: avg_length = (df17['total_day_supply'].divide(df17['total_claim_count'])).median
114/86: avg_length = (df17['total_day_supply'].divide(df17['total_claim_count'])).median()
114/87:
avg_length = df17['total_day_supply'].divide(df17['total_claim_count'])
print('Average number of beneficiaries per provide is {:f}'.format(avg_length.median))
114/88:
avg_length = df17['total_day_supply'].divide(df17['total_claim_count'])
print('Average number of beneficiaries per provide is {:f}'.format(avg_length.median()))
114/89:
avg_length = df17['total_day_supply'].divide(df17['total_claim_count'])
print('Average number of beneficiaries per provide is {:f}'.format(round(avg_length.median(),2))
114/90:
avg_length = df17['total_day_supply'].divide(df17['total_claim_count'])
print('Average number of beneficiaries per provide is {:f}'.format(round(avg_length.median(),2))
114/91:
avg_length = df17['total_day_supply'].divide(df17['total_claim_count'])
print('Average number of beneficiaries per provide is {:f}'.format((avg_length.median())))
114/92:
avg_prescription = df17['total_day_supply'].divide(df17['total_claim_count'])
median_avg_prescription = avg_prescription.median() 
print('Average number of beneficiaries per provide is {:f}'.format(median_avg_prescription))
114/93:
avg_prescription = df17['total_day_supply'].divide(df17['total_claim_count'])
median_avg_prescription = round(avg_prescription.median(),2)
print('Average number of beneficiaries per provide is {:f}'.format(median_avg_prescription))
114/94: df17['specialty_description']
114/95: df17.groupby('speciality_description')[['brand_claim_count', 'total_claim_count']]
114/96: df17.groupby('specialty_description')[['brand_claim_count', 'total_claim_count']]
114/97: df17.groupby('specialty_description')[['brand_claim_count', 'total_claim_count']].sum()
114/98: df17['brand_suppress_flag']
114/99: df17['brand_suppress_flag'].isnull
114/100: df17['brand_suppress_flag'].isnull()
114/101: df17[df17['brand_suppress_flag'].isnull()]['brand_suppress_flag']
114/102:
# Find the providers have not been suppressed
df17_notsuppress = df17[df17['brand_suppress_flag'].isnull()].reset_index()
114/103:
# Find the providers have not been suppressed
df17_notsuppress = df17[df17['brand_suppress_flag'].isnull()].reset_index()
df17_notsuppress.head()
114/104:
# Find the providers have not been suppressed
df17_notsuppress = df17[df17['brand_suppress_flag'].isnull()].reset_index()
df17_notsuppress['brand_suppress_flag']
114/105:
# Find the providers have not been suppressed
df17_notsuppress = df17[df17['brand_suppress_flag'].isnull()].reset_index()
114/106: df17_notsuppress.groupby('specialty_description')[['brand_claim_count', 'total_claim_count']].sum()
114/107: df17_special_description = df17_notsuppress.groupby('specialty_description')[['brand_claim_count', 'total_claim_count']].sum()
114/108: df17_special_description['total_claim_count']
114/109: df17_special_description['total_claim_count'] >= 1000
114/110: df17_special_description[[df17_special_description['total_claim_count'] >= 1000]]
114/111: df17_special_description = df17_notsuppress.groupby('specialty_description')[['brand_claim_count', 'total_claim_count']].sum().reset_index()
114/112: df17_special_description[[df17_special_description['total_claim_count'] >= 1000]]
114/113: df17_special_description[[df17_special_description['total_claim_count'] >= 1000]]
114/114: df17_special_description = pd.DataFrame(df17_notsuppress.groupby('specialty_description')[['brand_claim_count', 'total_claim_count']].sum())
114/115: df17_special_description[[df17_special_description['total_claim_count'] >= 1000]]
114/116: df17_special_description
114/117: df17_special_description.head()
114/118: df17_special_description = pd.DataFrame(df17_notsuppress.groupby('specialty_description')[['brand_claim_count', 'total_claim_count']].sum().reset_index())
114/119: df17_special_description.head()
114/120:  df17_special_description[df17_special_description['total_claim_count'] >=1_000]
114/121:
# calculate the fraction
special_frac = df17_special_description['brand_claim_count'].divide('total_claim_count')
114/122:
# calculate the fraction
special_frac = df17_special_desc['brand_claim_count'].divide(df17_special_desc['total_claim_count'])
114/123:
# Remove the specialities with total claims less than 1000
df17_special_description = pd.DataFrame(df17_notsuppress.groupby('specialty_description')[['brand_claim_count', 'total_claim_count']].sum().reset_index())
df17_special_desc = df17_special_description[df17_special_description['total_claim_count'] >=1_000]
114/124:
# calculate the fraction
special_frac = df17_special_desc['brand_claim_count'].divide(df17_special_desc['total_claim_count'])
114/125:
# calculate the fraction
special_frac = df17_special_desc['brand_claim_count'].divide(df17_special_desc['total_claim_count'])
special_frac_median = special_frac.median()
114/126:
avg_prescription = df17['total_day_supply'].divide(df17['total_claim_count'])
median_avg_prescription = round(avg_prescription.median(),2)
print('Median of the length of the average prescription is {:f}'.format(median_avg_prescription))
114/127:
# calculate the fraction
special_frac = df17_special_desc['brand_claim_count'].divide(df17_special_desc['total_claim_count'])
special_frac_std = round(special_frac.std(),2)
print('Standard deviation of these specialites specfractions is {:f}'.format(special_frac_std))
114/128: df17.groupby('NPPES_PROVIDER_STATE')[['opioid_bene_count', 'antibiotic_bene_count']]
114/129: df17.groupby('nppes_provider_state')[['opioid_bene_count', 'antibiotic_bene_count']]
114/130: df17.groupby('nppes_provider_state')[['opioid_bene_count', 'antibiotic_bene_count']].sum()
114/131:
def divide_two_cols(df):
    return df['opioid_bene_count'].sum() / float(df['antibiotic_bene_count'].sum())
df17.groupby('nppes_provider_state').apply(divide_two_cols)
114/132:
def DivideTwoColumns(df):
    return df['opioid_bene_count'].sum() / float(df['antibiotic_bene_count'].sum())
ratio_ = df17.groupby('nppes_provider_state').apply(DivideTwoColumns)
diff_ratio = round((max(ratio_) - min(ratio_)),2)
print('The difference between the largest and smallest ratios is {:f}'.format(diff_ratio))
114/133: ratio_
114/134: max(ratio_)
114/135: df17.groupby('nppes_provider_state')[['opioid_bene_count', 'antibiotic_bene_count']]
114/136: df17.groupby('nppes_provider_state')[['opioid_bene_count', 'antibiotic_bene_count']].sum()
114/137: min(ratio_)
114/138: df17['bene_count_ge65_suppress_flag']
114/139: df17['ge65_suppress_flag']
114/140: df17_65notsp = df17['ge65_suppress_flag'].isna()
114/141:
df17_65notsp = df17['ge65_suppress_flag'].isna()
df17_65notsp
114/142: df17['lis_suppress_flag']
114/143: df17_notsp = df17.dropna(subset=['ge65_suppress_flag', 'lis_suppress_flag'], how ='any',inplace=True)
114/144:
df17_notsp = df17.dropna(subset=['ge65_suppress_flag', 'lis_suppress_flag'], how ='any',inplace=True)
df17_notsp['ge65_suppress_flag']
114/145:
df17_notsp = df17.dropna(subset=['ge65_suppress_flag', 'lis_suppress_flag'], how ='any',inplace=True)
df17_notsp['ge65_suppress_flag']
114/146: df17_notsp = df17.dropna(subset=['ge65_suppress_flag', 'lis_suppress_flag'], how ='any',inplace=True)
114/147: df17_notsp
114/148: df17_notsp.head()
114/149: df17.dropna(subset=['ge65_suppress_flag', 'lis_suppress_flag'], how ='any',inplace=True)
114/150: df17.dropna(subset=['ge65_suppress_flag', 'lis_suppress_flag'], how ='any',inplace=True)
114/151: df17
114/152: df17.dropna(subset=['ge65_suppress_flag', 'lis_suppress_flag'])
114/153: df17_notsp = df17.dropna(subset=['ge65_suppress_flag', 'lis_suppress_flag'], inplace = True)
114/154: df17_notsp
114/155: df17_notsp.head()
114/156:
import pandas as pd
import numpy as np
114/157:
df17 = pd.read_csv('PartD_Prescriber_PUF_NPI_16/PartD_Prescriber_PUF_NPI_16.txt', sep ='\t')
df17.head()
114/158: df17_notsp = df17.dropna(subset=['ge65_suppress_flag', 'lis_suppress_flag'])
114/159: df17_notsp
114/160: df17_notsp['ge65_suppress_flag']
114/161: df['ge65_suppress_flag'].isnull() && df['lis_suppress_flag'].isnull()
114/162: df['ge65_suppress_flag'].isnull() and df['lis_suppress_flag'].isnull()
114/163: df['ge65_suppress_flag'].isnull()
114/164: df17['ge65_suppress_flag'].isnull()
114/165: all(df17['ge65_suppress_flag'].isnull(), df17['lis_suppress_flag'].isnull())
114/166: df17.query('ge65_suppress_flag== @nan')
114/167: df17.query('ge65_suppress_flag== @NAN')
114/168: df17.query('ge65_suppress_flag.isnull()')
114/169: df17.query('ge65_suppress_flag ~= ge65_suppress_flag')
114/170: df17.query('ge65_suppress_flag != ge65_suppress_flag')
114/171: df17_notsp = df17.query('ge65_suppress_flag != ge65_suppress_flag and lis_suppress_flag != lis_suppress_flag').reset_index()
114/172: df17_notsp[['ge65_suppress_flag', 'lis_suppress_flag']]
114/173:
ge65_frac = df17_notsp['total_claim_count_ge65']/ df17_notsp['total_claim_count']
lic_frac = df17_notsp['lis_claim_count']/df17_notsp['total_claim_count']
114/174: ge65_frac
114/175:
import pandas as pd
import numpy as np
from scipy import stats
114/176:
ge65_frac = df17_notsp['total_claim_count_ge65']/ df17_notsp['total_claim_count']
lic_frac = df17_notsp['lis_claim_count']/df17_notsp['total_claim_count']
corr_coef = stats.pearsonr(ge65_frac, lic_frac)
114/177:
ge65_frac = df17_notsp['total_claim_count_ge65']/ df17_notsp['total_claim_count']
lic_frac = df17_notsp['lis_claim_count']/df17_notsp['total_claim_count']
corr_coef = stats.pearsonr(ge65_frac, lic_frac)
corr_coef
114/178:
ge65_frac = df17_notsp['total_claim_count_ge65']/ df17_notsp['total_claim_count']
lic_frac = df17_notsp['lis_claim_count']/df17_notsp['total_claim_count']
corr_coef = stats.pearsonr(ge65_frac, lic_frac)
corr_coef[0]
114/179:
ge65_frac = df17_notsp['total_claim_count_ge65']/ df17_notsp['total_claim_count']
lic_frac = df17_notsp['lis_claim_count']/df17_notsp['total_claim_count']
corr_coef = round(stats.pearsonr(ge65_frac, lic_frac)[0], 2)
print('The Pearson correlation coefficient is {:f}'.format(corr_coef))
114/180: df17['avg_length']
114/181: df17['opioid_day_supply']/df17['opioid_claim_count']
114/182: df17['opioid_claim_count'].isnull()
114/183: df17['opioid_claim_count'].isnull().sum()
114/184: df17['opioid_claim_count'].isnull().sum()
114/185: df17['opioid_claim_count'].notna()
114/186:
def prescription_len(df):
    return df['opioid_day_supply']/ df['opioid_claim_count']
df17[df17['opioid_claim_count'].notna()].apply(prescription_len)
114/187:
def prescription_len(df):
    return df['opioid_day_supply']/ df['opioid_claim_count']
df17[df17['opioid_claim_count'].notna()].reset_index().apply(prescription_len)
114/188: df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
114/189:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
prescription_length = df17_opcliam_notna['opioid_day_supply']/df17['opioid_claim_count']
114/190: mean(prescription_length)
114/191: np.mean(prescription_length)
114/192: prescription_length
114/193:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
prescription_length = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
114/194: np.mean(prescription_length)
114/195:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
114/196:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
df17_opcliam_notna.groupby(['nppes_provider_state','specialty_description'])['npi'].count()
114/197:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
df17_opcliam_notna.groupby(['nppes_provider_state','specialty_description'])['npi']
114/198:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
df17_opcliam_notna.groupby(['nppes_provider_state','specialty_description'])['npi'].agg({'npi_count': np.size})
114/199:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
dfx = df17_opcliam_notna.groupby(['nppes_provider_state','specialty_description'])['npi'].agg({'npi_count': np.size})
114/200: dfx['npi_count'] > 100
114/201: df17.groupby(['nppes_provider_state','specialty_description'])['npi'].agg({'npi_count': np.size})
114/202: dfx = df17.groupby(['nppes_provider_state','specialty_description'])['npi'].agg({'npi_count': np.size})
114/203: (dfx['npi_count'] > 100).sum()
114/204:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
dfx = df17_opcliam_notna.groupby(['nppes_provider_state','specialty_description'])['npi'].agg({'npi_count': np.size})
114/205: (dfx['npi_count'] > 100).sum()
114/206: df17_npicount[df17_npicount['npi_count'] > 100]
114/207: df17_npicount[df17_npicount['npi_count'] > 100]
114/208:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
df17_npicount = df17_opcliam_notna.groupby(['nppes_provider_state','specialty_description'])['npi'].agg({'npi_count': np.size})
114/209: df17_npicount[df17_npicount['npi_count'] > 100]
114/210:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']

df17_npicount = df17_opcliam_notna.groupby(
    ['nppes_provider_state','specialty_description'])['npi'].agg(
    {'npi_count': np.size})
114/211:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']

df17_npicount = df17_opcliam_notna.groupby(
    ['nppes_provider_state','specialty_description']).apply(
    lambda x: pd.Series({
        'npi_count': x['npi'].count()
        'prescr_length_mean': x['prescr_length'].mean()
    }))
114/212:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']

df17_npicount = df17_opcliam_notna.groupby(
    ['nppes_provider_state','specialty_description']).apply(
    lambda x: pd.Series({
        'npi_count': x['npi'].count(),
        'prescr_length_mean': x['prescr_length'].mean()
    }))
114/213: df17_npicount
114/214: df17_npicount['prescr_length'].mean()
114/215: df17_npicount['prescr_length']
114/216: df17_opcliam_notna['prescr_length']
114/217:
df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']

df17_npicount = df17_opcliam_notna.groupby(
    ['nppes_provider_state','specialty_description']).apply(
    lambda x: pd.Series({
        'npi_count': x['npi'].count(),
        'prescr_length_mean': x['prescr_length'].mean()
    }))
114/218: df17_opcliam_notna['prescr_length']
114/219: df17_opcliam_notna = df17[df17['opioid_claim_count'].notna()].reset_index()
114/220: df17_opcliam_notna['prescr_length']
114/221: df17_opcliam_notna['prescr_length']
114/222: df17_opcliam_notna['opioid_claim_count']
114/223:
#check if the denominator is nan
df17['opioid_claim_count'].isnull().sum(), (df17['opioid_claim_count'] ==0).sum()
114/224: df17.query( 'opioid_claim_count > 0')['opioid_claim_count']
114/225:
df17_opcliam_notna = df17.query('opioid_claim_count > 0').reset_index()
df17_opcliam_notna['prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']

df17_npicount = df17_opcliam_notna.groupby(
    ['nppes_provider_state','specialty_description']).apply(
    lambda x: pd.Series({
        'npi_count': x['npi'].count(),
        'prescr_length_mean': x['prescr_length'].mean()
    }))
114/226: df17_npicount
114/227:
df17_opcliam_notna = df17.query('opioid_claim_count > 0').reset_index()
df17_opcliam_notna['opioid_prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
df17_opcliam_notna['avg_prescr_length'] = df17_opcliam_notna['total_day_supply']/df17_opcliam_notna['total_claim_count']

df17_npicount_mean_prescr_length = df17_opcliam_notna.groupby(
    ['nppes_provider_state','specialty_description']).apply(
    lambda x: pd.Series({
        'npi_count': x['npi'].count(),
        'opioid_length_mean': x['prescr_length'].mean()
        'avg_length_mean': x['avg_prescr_length'].mean()
    }))
df17_npicount_mean_prescr_length[df17_npicount_mean_prescr_length['npi_count'>= 100]]
114/228:
df17_opcliam_notna = df17.query('opioid_claim_count > 0').reset_index()
df17_opcliam_notna['opioid_prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
df17_opcliam_notna['avg_prescr_length'] = df17_opcliam_notna['total_day_supply']/df17_opcliam_notna['total_claim_count']

df17_npicount_mean_prescr_length = df17_opcliam_notna.groupby(
    ['nppes_provider_state','specialty_description']).apply(
    lambda x: pd.Series({
        'npi_count': x['npi'].count(),
        'opioid_length_mean': x['prescr_length'].mean(),
        'avg_length_mean': x['avg_prescr_length'].mean()
    }))
df17_npicount_mean_prescr_length[df17_npicount_mean_prescr_length['npi_count'>= 100]]
114/229:
df17_opcliam_notna = df17.query('opioid_claim_count > 0').reset_index()
df17_opcliam_notna['opioid_prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
df17_opcliam_notna['avg_prescr_length'] = df17_opcliam_notna['total_day_supply']/df17_opcliam_notna['total_claim_count']

df17_npicount_mean_prescr_length = df17_opcliam_notna.groupby(
    ['nppes_provider_state','specialty_description']).apply(
    lambda x: pd.Series({
        'npi_count': x['npi'].count(),
        'opioid_length_mean': x['opioid_prescr_length'].mean(),
        'avg_length_mean': x['avg_prescr_length'].mean()
    }))
df17_npicount_mean_prescr_length[df17_npicount_mean_prescr_length['npi_count'>= 100]]
114/230: df17_npicount_mean_prescr_length
114/231: df17_npicount_mean_prescr_length >= float(100)
114/232:
df17_opcliam_notna = df17.query('opioid_claim_count > 0').reset_index()
df17_opcliam_notna['opioid_prescr_length'] = df17_opcliam_notna['opioid_day_supply']/df17_opcliam_notna['opioid_claim_count']
df17_opcliam_notna['avg_prescr_length'] = df17_opcliam_notna['total_day_supply']/df17_opcliam_notna['total_claim_count']

df17_npicount_mean_prescr_length = df17_opcliam_notna.groupby(
    ['nppes_provider_state','specialty_description']).apply(
    lambda x: pd.Series({
        'npi_count': x['npi'].count(),
        'opioid_length_mean': x['opioid_prescr_length'].mean(),
        'avg_length_mean': x['avg_prescr_length'].mean()
    }))
114/233:
df17_npicount_mean_prescr_length['presc_length_ratio'] = \
df17_npicount_mean_prescr_length['opioid_length_mean']/ \
df17_npicount_mean_prescr_length['avg_length_mean']
114/234: df17_npicount_mean_prescr_length
114/235: l_ratio = df17_ss_groupby[df17_ss_groupby['npi_count'] > float(100)]['presc_length_ratio']
114/236:
df17_opcl = df17.query('opioid_claim_count > 0').reset_index()
df17_opcl['opioid_prescr_length'] = df17_opcl['opioid_day_supply']/df17_opcl['opioid_claim_count']
df17_opcl['avg_prescr_length'] = df17_opcl['total_day_supply']/df17_opcl['total_claim_count']

df17_ss_groupby = df17_opcl.groupby(
    ['nppes_provider_state','specialty_description']).apply(
    lambda x: pd.Series({
        'npi_count': x['npi'].count(),
        'opioid_length_mean': x['opioid_prescr_length'].mean(),
        'avg_length_mean': x['avg_prescr_length'].mean()
    }))

df17_ss_groupby['presc_length_ratio'] = df17_ss_groupby['opioid_length_mean']/df17_ss_groupby['avg_length_mean']
114/237:
l_ratio = df17_ss_groupby[df17_ss_groupby['npi_count'] > float(100)]['presc_length_ratio']
l
114/238: l_ratio = df17_ss_groupby[df17_ss_groupby['npi_count'] > float(100)]['presc_length_ratio']
114/239:
l_ratio = df17_ss_groupby[df17_ss_groupby['npi_count'] > float(100)]['presc_length_ratio']
l_ratio.max()
114/240:
l_ratio = df17_ss_groupby[df17_ss_groupby['npi_count'] > float(100)]['presc_length_ratio']
max_ratio = round(l_ratio.max(),2)
print('The max ratio of average length in each specialty across all states is {:f}'.format(max_ratio))
114/241:
df17 = pd.read_csv('PartD_Prescriber_PUF_NPI_17/PartD_Prescriber_PUF_NPI_17.txt', sep ='\t')
df17.head()
115/1:
import pandas as pd
import numpy as np
from scipy import stats
115/2:
df17 = pd.read_csv('PartD_Prescriber_PUF_NPI_17/PartD_Prescriber_PUF_NPI_17.txt', sep ='\t')
df17.head()
115/3: df17.info()
115/4: df17.info()
115/5:
# double check the number lower than 10 beneficiaries.
(df17['bene_count'] <=10).sum(),  df17['bene_count'].isnull().sum()
115/6:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
print('Average number of beneficiaries per provide is {:f}'.format(avg_))
115/7: df17['total_claim_count'].isnull().sum()
115/8:
avg_prescription = df17['total_day_supply'].divide(df17['total_claim_count'])
median_avg_prescription = round(avg_prescription.median(),2)
print('Median of the length of the average prescription is {:f}'.format(median_avg_prescription))
115/9:
# Find the providers have not been suppressed
df17_notsuppress = df17[df17['brand_suppress_flag'].isnull()].reset_index()
115/10:
# Remove the specialities with total claims less than 1000
df17_special_description = pd.DataFrame(df17_notsuppress.groupby('specialty_description')[['brand_claim_count', 'total_claim_count']].sum().reset_index())
df17_special_desc = df17_special_description[df17_special_description['total_claim_count'] >=1_000]
115/11:
# calculate the fraction
special_frac = df17_special_desc['brand_claim_count'].divide(df17_special_desc['total_claim_count'])
special_frac_std = round(special_frac.std(),2)
print('Standard deviation of these specialites specfractions is {:f}'.format(special_frac_std))
115/12:
def DivideTwoColumns(df):
    return df['opioid_bene_count'].sum() / float(df['antibiotic_bene_count'].sum())
ratio_ = df17.groupby('nppes_provider_state').apply(DivideTwoColumns)
diff_ratio = round((max(ratio_) - min(ratio_)),2)
print('The difference between the largest and smallest ratios is {:f}'.format(diff_ratio))
115/13:
# Find the providers have not been suppressed
df17_notsp = df17.query('ge65_suppress_flag != ge65_suppress_flag and lis_suppress_flag != lis_suppress_flag').reset_index()
115/14:
# calcuate two fractions
ge65_frac = df17_notsp['total_claim_count_ge65']/ df17_notsp['total_claim_count']
lic_frac = df17_notsp['lis_claim_count']/df17_notsp['total_claim_count']
corr_coef = round(stats.pearsonr(ge65_frac, lic_frac)[0], 2)
print('The Pearson correlation coefficient is {:f}'.format(corr_coef))
115/15:
#check if the denominator is nan
df17['opioid_claim_count'].isnull().sum(), (df17['opioid_claim_count'] ==0).sum()
115/16:
# remove zeros and nan denominator 
# get opioid prescr length and average prescr length
df17_opcl = df17.query('opioid_claim_count > 0').reset_index()
df17_opcl['opioid_prescr_length'] = df17_opcl['opioid_day_supply']/df17_opcl['opioid_claim_count']
df17_opcl['avg_prescr_length'] = df17_opcl['total_day_supply']/df17_opcl['total_claim_count']

# groupby states and specialty
# calculate the provider count, average opoid prescr length and average prescr length
df17_ss_groupby = df17_opcl.groupby(
    ['nppes_provider_state','specialty_description']).apply(
    lambda x: pd.Series({
        'npi_count': x['npi'].count(),
        'opioid_length_mean': x['opioid_prescr_length'].mean(),
        'avg_length_mean': x['avg_prescr_length'].mean()
    }))

# calculate the ratio
df17_ss_groupby['presc_length_ratio'] = df17_ss_groupby['opioid_length_mean']/df17_ss_groupby['avg_length_mean']
115/17:
# select each (state, specialty) pair with at least 100 providers
# calculate the ratio
l_ratio = df17_ss_groupby[df17_ss_groupby['npi_count'] > float(100)]['presc_length_ratio']
max_ratio = round(l_ratio.max(),2)
print('The max ratio of average length in each specialty across all states is {:f}'.format(max_ratio))
115/18: df16 = pd.read_csv('PartD_Prescriber_PUF_NPI_16/PartD_Prescriber_PUF_NPI_16.txt', sep ='\t')
115/19: pd.isnull(df16)
115/20: supression_col = [col for col in df.columns if 'suppress_flag' in col]
115/21: supression_col = [col for col in df16.columns if 'suppress_flag' in col]
115/22: supression_col
115/23: suppression_col = [col for col in df16.columns if 'suppress_flag' in col]
115/24: pd.isnull(df[suppression_col])
115/25: pd.isnull(df16[suppression_col])
115/26: pd.isnull(df16[suppression_col]).all()
115/27: pd.isnull(df16[suppression_col]).all(1)
115/28: pd.isnull(df16[suppression_col]).all()
115/29: pd.isnull(df16[suppression_col]).all(axis = 1)
115/30: df16[pd.isnull(df16[suppression_col]).all(axis =1)]['npi']
115/31: suppression_col = [col for col in df16.columns if 'suppress_flag' in col]
115/32:
df16_new = df16[pd.isnull(df16[suppression_col]).all(axis =1)]
df17_new = df17[pd.isnull(df17[suppression_col]).all(axis =1)]
115/33: df_all = df16_new.join(df17_new, on='nip', lsuffix= '_16', rsuffox= '_17')
115/34: df_all = df16_new.join(df17_new, on='nip', lsuffix= '_16', rsuffix= '_17')
115/35: df_all = pd.merge(df16_new, df17_new, on='nip', how= 'outer', lsuffix= '_16', rsuffix= '_17')
115/36: df_all = pd.merge(df16_new, df17_new, on='nip', how= 'inner', suffixes=('_16', '_17'))
115/37: df16.columns
115/38: df16.head()
115/39: df_all = pd.merge(df16_new, df17_new, on='nip', how= 'inner', suffixes=('_16', '_17'))
115/40: df_all = pd.merge(df16_new, df17_new, left_on='nip', right_on = 'nip', how= 'inner', suffixes=('_16', '_17'))
115/41: df16_new.head()
115/42: df_all = pd.merge(df16_new, df17_new, left_on='nip', how= 'inner', suffixes=('_16', '_17'))
115/43: len(df16_new), len(df17_new)
115/44: df_16_new['npi']
115/45: df16_new['npi']
115/46: np.unique(df16_new['npi'] + df17_new['npi'])
115/47: (df16_new['npi'] + df17_new['npi']).unique()
115/48: int((df16_new['npi'] + df17_new['npi'])).unique()
115/49: df16_new['nip'].append(df17_new['nip'])
115/50:
pd.merge(df16_new[['npi', 'total_drug_cost', 'total_day_supply']], 
        df17_new[['npi', 'total_drug_cost', 'total_day_supply']],
        on = 'npi')
115/51: len(df16_new)
115/52:
pd.merge(df16_new[['npi', 'total_drug_cost', 'total_day_supply']], 
        df17_new[['npi', 'total_drug_cost', 'total_day_supply']],
        on = 'npi', how = 'outer')
115/53:
pd.merge(df16_new[['npi', 'total_drug_cost', 'total_day_supply']], 
        df17_new[['npi', 'total_drug_cost', 'total_day_supply']],
        on = 'npi', how = 'outer', suffixes=('_16', '_17'))
115/54:
df1617 = pd.merge(df16_new[['npi', 'total_drug_cost', 'total_day_supply']], 
        df17_new[['npi', 'total_drug_cost', 'total_day_supply']],
        on = 'npi', how = 'inner', suffixes=('_16', '_17'))
115/55:
inflation_rate = df1617['total_drug_cost_17'] / df1617['total_day_supply_17'] \
- df1617['total_drug_cost_16'] / df1617['total_day_supply_16']
115/56: inflation_rate
115/57:
inflation_rate = df1617['total_drug_cost_17'] / df1617['total_day_supply_17'] \
- df1617['total_drug_cost_16'] / df1617['total_day_supply_16']
mean_inflation_rate = round(inflation_rate.mean(),2)
print('The average inflation rate is {:f}'.format(mean_inflation_rate))
115/58:
mean_inflation_rate = round(inflation_rate.mean(),2)
round(inflation_rate.mean(),2)
115/59:
inflation_rate = df1617['total_drug_cost_17'] / df1617['total_day_supply_17'] \
- df1617['total_drug_cost_16'] / df1617['total_day_supply_16']
mean_inflation_rate = inflation_rate.mean()
print('The average inflation rate is {:f}'.format(mean_inflation_rate))
115/60: mean_inflation_rate
115/61:
inflation_rate = (df1617['total_drug_cost_17'] / df1617['total_day_supply_17'] \
- df1617['total_drug_cost_16'] / df1617['total_day_supply_16']) / (df1617['total_drug_cost_17'] / df1617['total_day_supply_17'])
mean_inflation_rate = inflation_rate.mean()
print('The average inflation rate is {:f}'.format(mean_inflation_rate))
115/62:
# merge 16 and 17 based on npi

df1617 = pd.merge(df16_new[['npi', 'total_drug_cost', 'total_day_supply']], 
        df17_new[['npi', 'total_drug_cost', 'total_day_supply']],
        on = 'npi', how = 'inner', suffixes=('_16', '_17'))
df1617['daily_cost_17'] =  df1617['total_drug_cost_17'] / df1617['total_day_supply_17']
df1617['daily_cost_16'] = df1617['total_drug_cost_16'] / df1617['total_day_supply_16']
115/63:
inflation_rate = (df1617['daily_cost_17'] - df1617['daily_cost_16'])/ df1617['daily_cost_17']
mean_inflation_rate = inflation_rate.mean()
print('The average inflation rate is {:f}'.format(mean_inflation_rate))
115/64:
# merge 16 and 17 based on npi

df1617 = pd.merge(df16_new[['npi', 'total_drug_cost', 'total_day_supply']], 
        df17_new[['npi', 'total_drug_cost', 'total_day_supply']],
        on = 'npi', how = 'inner', suffixes=('_16', '_17'))
df1617['daily_cost_17'] =  df1617['total_drug_cost_17'] / df1617['total_day_supply_17']
df1617['daily_cost_16'] = df1617['total_drug_cost_16'] / df1617['total_day_supply_16']
115/65:
inflation_rate = (df1617['daily_cost_17'] - df1617['daily_cost_16'])/ df1617['daily_cost_17']
mean_inflation_rate = inflation_rate.mean()
print('The average inflation rate is {:f}'.format(mean_inflation_rate))
115/66:
# load 16 data
df16 = pd.read_csv('PartD_Prescriber_PUF_NPI_16/PartD_Prescriber_PUF_NPI_16.txt', sep ='\t')
115/67:
# remove the suppressed provider
suppression_col = [col for col in df16.columns if 'suppress_flag' in col]
df16_new = df16[pd.isnull(df16[suppression_col]).all(axis =1)]
df17_new = df17[pd.isnull(df17[suppression_col]).all(axis =1)]
115/68:
# merge 16 and 17 based on npi
df1617 = pd.merge(df16_new[['npi', 'total_drug_cost', 'total_day_supply']], 
        df17_new[['npi', 'total_drug_cost', 'total_day_supply']],
        on = 'npi', how = 'inner', suffixes=('_16', '_17'))
df1617['daily_cost_17'] =  df1617['total_drug_cost_17'] / df1617['total_day_supply_17']
df1617['daily_cost_16'] = df1617['total_drug_cost_16'] / df1617['total_day_supply_16']
115/69:
inflation_rate = (df1617['daily_cost_17'] - df1617['daily_cost_16'])/ df1617['daily_cost_17']
mean_inflation_rate = inflation_rate.mean()
print('The average inflation rate is {:f}'.format(mean_inflation_rate))
115/70: mean_inflation_rate
115/71:
inflation_rate = (df1617['daily_cost_17'] - df1617['daily_cost_16'])/ df1617['daily_cost_17']
mean_inflation_rate = round(inflation_rate.mean(),2)
print('The average inflation rate is {:f}'.format(mean_inflation_rate))
115/72:
inflation_rate = (df1617['daily_cost_17'] - df1617['daily_cost_16'])/ df1617['daily_cost_16']
mean_inflation_rate = round(inflation_rate.mean(),2)
print('The average inflation rate is {:f}'.format(mean_inflation_rate))
115/73:
# merge 16 and 17 based on npi
df1617 = pd.merge(df16_new[['npi', 'total_drug_cost', 'total_day_supply']], 
        df17_new[['npi', 'total_drug_cost', 'total_day_supply']],
        on = 'npi', how = 'inner', suffixes=('_16', '_17'))
df1617['daily_cost_17'] =  df1617['total_drug_cost_17'] / df1617['total_day_supply_17']
df1617['daily_cost_16'] = df1617['total_drug_cost_16'] / df1617['total_day_supply_16']
115/74:
inflation_rate = (df1617['daily_cost_17'] - df1617['daily_cost_16'])/ df1617['daily_cost_16']
mean_inflation_rate = round(inflation_rate.mean(),2)
print('The average inflation rate is {:f}'.format(mean_inflation_rate))
115/75:
df1617_spec = pd.merge(df16[['npi', 'specialty_description']],
                      df17[['npi', 'specialty_description']],
                      on = 'npi', suffixes=('_16', '_17'))
115/76: df1617_spec
115/77: df1617_spec.query['specialty_description_16' == 'specialty_description_17']
115/78: df1617_spec['specialty_description_16' == 'specialty_description_17']
115/79: df1617_spec['specialty_description_16']== df1617_spec['specialty_description_17']
115/80: (df1617_spec['specialty_description_16']== df1617_spec['specialty_description_17']).sum()
115/81: len(df1617_spec)
115/82:
df1617_spec = pd.merge(df16[['npi', 'specialty_description']],
                      df17[['npi', 'specialty_description']],
                      on = ('npi','specialty_description'), suffixes=('_16', '_17'))
115/83: df1617_spec.shape
115/84:
df1617_spec = pd.merge(df16[['npi', 'specialty_description']],
                      df17[['npi', 'specialty_description']], how = 'outer',
                      on = ('npi','specialty_description'), suffixes=('_16', '_17'))
115/85: df1617_spec.shape
115/86: df17.shape()
115/87: df17.shape
115/88:
df1617_spec = pd.merge(df16[['npi', 'specialty_description']],
                      df17[['npi', 'specialty_description']], how = 'outer',
                      on = ('npi'), suffixes=('_16', '_17'))
115/89:
df1617_spec = pd.merge(df16[['npi', 'specialty_description']],
                      df17[['npi', 'specialty_description']], how = 'outer',
                      on = 'npi', suffixes=('_16', '_17'))
115/90: df1617_spec.head()
115/91: df1617_spec['specialy_description_16'].isnull
115/92: df1617_spec.query('specialty_description_17 != specialty_description_17 and specialty_description_16 == specialty_description_16')
115/93: df1617_spec.groupby('specialty_description_17')
115/94: df1617_spec.groupby('specialty_description_17').sum()
115/95: df1617_spec.groupby(['specialty_description_16', 'specialty_description_17').sum()
115/96: df1617_spec.groupby(['specialty_description_16', 'specialty_description_17']).sum()
115/97:
df17_spec_groupby = df1617_spec.groupby(
    ['specialty_description']).apply(
    lambda x: pd.Series({
        'npi_16: x['npi_16'].sum(),
        'npi_17: x['npi_17'].sum(),
    }))
115/98:
df17_spec_groupby = df1617_spec.groupby(
    ['specialty_description']).apply(
    lambda x: pd.Series({
        'npi_16': x['npi_16'].sum(),
        'npi_17': x['npi_17'].sum(),
    }))
115/99:
df17_spec_groupby = df1617_spec.groupby(
    ['specialty_description']).apply(
    lambda x: pd.Series({
        'npi_16': x['npi_16'].sum(),
        'npi_17': x['npi_17'].sum()
    }))
115/100:
df17_spec_groupby = df1617_spec.groupby(
    ['specialty_description_16']).apply(
    lambda x: pd.Series({
        'npi_16': x['npi_16'].sum(),
        'npi_17': x['npi_17'].sum()
    }))
115/101:
df17_spec_groupby = df1617_spec.groupby(
    ['specialty_description_16']).apply(
    lambda x: pd.Series({
        'npi_16_sum': x['npi_16'].sum(),
        'npi_17_sum': x['npi_17'].sum()
    }))
115/102:
 df1617_spec.groupby(
    ['specialty_description_16'])
115/103:
 df1617_spec.groupby(
    ['specialty_description_16']).sum()
115/104: df1617_spec.head()
115/105:
df1617_spec = pd.merge(df16[['npi', 'specialty_description']],
                      df17[['npi', 'specialty_description']], how = 'outer',
                      on = ('npi', 'specialty_description'), suffixes=('_16', '_17'))
115/106: df1617_spec.head()
115/107:
df1617_spec = pd.join(df16[['npi', 'specialty_description']],
                      df17[['npi', 'specialty_description']], how = 'outer',
                      on = ('npi', 'specialty_description'), suffixes=('_16', '_17'))
115/108:
df16_spec, df17_spec  = df16[['npi', 'specialty_description']], df17[['npi', 'specialty_description']]
df1617_spec = df16_spec.join(df17_spec, lsuffix='_16', rsuffix='_17')
115/109: df1617_spec.shape
115/110: df1617_spec.head()
115/111: df16_spec.shape, df17_spec.shape
115/112: df1617_spec.shape()
115/113: df1617_spec.shape()
115/114: df1617_spec.shape
115/115:
df16_spec, df17_spec  = df16[['npi', 'specialty_description']], df17[['npi', 'specialty_description']]
df1617_spec = df16_spec.join(df17_spec, how = 'right'lsuffix='_16', rsuffix='_17')
115/116:
df16_spec, df17_spec  = df16[['npi', 'specialty_description']], df17[['npi', 'specialty_description']]
df1617_spec = df16_spec.join(df17_spec, how = 'right', lsuffix='_16', rsuffix='_17')
115/117: df1617_spec.shape
115/118: df1617_spec.head()
115/119:
df16_spec, df17_spec  = df16[['npi', 'specialty_description']], df17[['npi', 'specialty_description']]
df1617_spec = df16_spec.join(df17_spec, how = 'outer', lsuffix='_16', rsuffix='_17')
115/120: df1617_spec.head()
115/121:
df16_spec, df17_spec  = df16[['npi', 'specialty_description']], df17[['npi', 'specialty_description']]
df1617_spec = df17_spec.join(df16_spec, lsuffix='_17', rsuffix='_16')
115/122: df1617_spec.head()
115/123:
df16_spec, df17_spec  = df16[['npi', 'specialty_description']], df17[['npi', 'specialty_description']]
df1617_spec = df17_spec.join(df16_spec, lsuffix='_17', rsuffix='_16')
115/124: df1617_spec.head()
115/125: df1617_spec['npi_16']
115/126: df1617_spec.head()
115/127: df16_spec = df16.groupby('specialty_description')['npi'].sum()
115/128: df16_spec = df16.groupby('specialty_description')['npi'].count()
115/129:
df16_spec = df16.groupby('specialty_description')['npi'].count()
df16_spec
115/130: df16_spec = df16.groupby('specialty_description')['npi'].count().reset_index()
115/131:
df16_spec = df16.groupby('specialty_description')['npi'].count().reset_index()
df16_spec
115/132:
df16_spec = df16.groupby('specialty_description')['npi'].count().reset_index()
df17_spec = df17.groupby('specialty_description')['npi'].count().reset_index()
115/133: df17_spec
115/134:
df16_spec = df16.groupby('specialty_description')['npi'].agg({'npi_count':count()}).reset_index()
df17_spec = df17.groupby('specialty_description')['npi'].agg({'npi_count':count()}).reset_index()
115/135:
df16_spec = df16.groupby('specialty_description')['npi'].count().reset_index().columns = 'count'
df17_spec = df17.groupby('specialty_description')['npi'].count().reset_index()
115/136:
df16_spec = df16.groupby('specialty_description')['npi'].count().reset_index()
df17_spec = df17.groupby('specialty_description')['npi'].count().reset_index()
115/137: df16_spec['npi'] >= 1_000
115/138: df16_spec[df16_spec['npi'] >= 1_000]
115/139: df16_spec_new = df16_spec[df16_spec['npi'] >= 1_000].reset_index()
115/140: df16_spec_new
115/141: df16_spec_new = df16_spec[df16_spec['npi'] >= 1_000]
115/142: df16_spec_new
115/143:
df16_spec = df16.groupby('specialty_description')['npi'].count()
df16_spec_new = df16_spec[df16_spec['npi'] >= 1_000]
115/144:
df16_spec = pd.DataFrame(df16.groupby('specialty_description')['npi'].count())
df16_spec_new = df16_spec[df16_spec['npi'] >= 1_000]
115/145: df16_spec_new
115/146: df16_spec_new.reset_index()
115/147:
df16_spec = pd.DataFrame(df16.groupby('specialty_description')['npi'].count())
df17_spec = pd.DataFrame(df17.groupby('specialty_description')['npi'].count())
df16_spec_new = df16_spec[df16_spec['npi'] >= 1_000].reset_index()
115/148: df1617_spec = df16_spec_new.join(df17_spec, how ='left', on = 'specialty_description',lsuffix= '_16', rsuffix= '_17')
115/149: df1617_spec
115/150: df1617_spec['diff_fraction'] = (df1617_spec['npi_13'] - df1617_spec['npi_14'])/df1617_spec['npi_13']
115/151: df1617_spec['npi_13']
115/152: df1617_spec
115/153: df1617_spec.columns
115/154: df1617_spec['diff_fraction'] = (df1617_spec['npi_16'] - df1617_spec['npi_17'])/df1617_spec['npi_16']
115/155: df1617_spec['diff_fraction']
115/156:
left_fraction = (df1617_spec['npi_16'] - df1617_spec['npi_17'])/df1617_spec['npi_16']
max_left_fraction = max(left_fraction)
print('The largest fraction is {:f}'.format(max_left_fraction))
115/157:
df16_spec = pd.DataFrame(df16.groupby('specialty_description')['npi'].count())
df16_spec_new = df16_spec[df16_spec['npi'] >= 1_000]
df1617_spec = df16_spec_new.join(df17_spec, how ='left', on = 'specialty_description',lsuffix= '_16', rsuffix= '_17')
115/158:
df1617_spec = df16_spec_new.join(df17_spec, how ='left', on = 'specialty_description',lsuffix= '_16', rsuffix= '_17')
left_fraction = (df1617_spec['npi_16'] - df1617_spec['npi_17'])/df1617_spec['npi_16']
115/159:
df16_spec = pd.DataFrame(df16.groupby('specialty_description')['npi'].count())
df16_spec_new = df16_spec[df16_spec['npi'] >= 1_000]
df1617_spec = df16_spec_new.join(df17_spec, how ='left', on = 'specialty_description',lsuffix= '_16', rsuffix= '_17')
115/160:
left_fraction = (df1617_spec['npi_16'] - df1617_spec['npi_17'])/df1617_spec['npi_16']
max_left_fraction = max(left_fraction)
print('The largest fraction is {:f}'.format(max_left_fraction))
115/161:
left_fraction = (df1617_spec['npi_16'] - df1617_spec['npi_17'])/df1617_spec['npi_16']
max_left_fraction = round(max(left_fraction),2)
print('The largest fraction is {:f}'.format(max_left_fraction))
116/1:
import pandas as pd
import numpy as np
from scipy import stats
116/2:
df17 = pd.read_csv('PartD_Prescriber_PUF_NPI_17/PartD_Prescriber_PUF_NPI_17.txt', sep ='\t')
df17.head()
116/3: df17.info()
116/4:
# double check the number lower than 10 beneficiaries.
(df17['bene_count'] <=10).sum(),  df17['bene_count'].isnull().sum()
116/5:
# calculate average 
avg_ = round(df17['bene_count'].sum()/ df17['bene_count'].notna().sum(),2)
print('Average number of beneficiaries per provide is {:f}'.format(avg_))
116/6: df17['total_claim_count'].isnull().sum()
116/7:
avg_prescription = df17['total_day_supply'].divide(df17['total_claim_count'])
median_avg_prescription = round(avg_prescription.median(),2)
print('Median of the length of the average prescription is {:f}'.format(median_avg_prescription))
116/8:
# Find the providers have not been suppressed
df17_notsuppress = df17[df17['brand_suppress_flag'].isnull()].reset_index()
116/9:
# Remove the specialities with total claims less than 1000
df17_special_description = pd.DataFrame(df17_notsuppress.groupby('specialty_description')[['brand_claim_count', 'total_claim_count']].sum().reset_index())
df17_special_desc = df17_special_description[df17_special_description['total_claim_count'] >=1_000]
116/10:
# calculate the fraction
special_frac = df17_special_desc['brand_claim_count'].divide(df17_special_desc['total_claim_count'])
special_frac_std = round(special_frac.std(),2)
print('Standard deviation of these specialites specfractions is {:f}'.format(special_frac_std))
116/11:
def DivideTwoColumns(df):
    return df['opioid_bene_count'].sum() / float(df['antibiotic_bene_count'].sum())
ratio_ = df17.groupby('nppes_provider_state').apply(DivideTwoColumns)
diff_ratio = round((max(ratio_) - min(ratio_)),2)
print('The difference between the largest and smallest ratios is {:f}'.format(diff_ratio))
116/12:
# Find the providers have not been suppressed
df17_notsp = df17.query('ge65_suppress_flag != ge65_suppress_flag and lis_suppress_flag != lis_suppress_flag').reset_index()
116/13:
# calcuate two fractions
ge65_frac = df17_notsp['total_claim_count_ge65']/ df17_notsp['total_claim_count']
lic_frac = df17_notsp['lis_claim_count']/df17_notsp['total_claim_count']
corr_coef = round(stats.pearsonr(ge65_frac, lic_frac)[0], 2)
print('The Pearson correlation coefficient is {:f}'.format(corr_coef))
116/14:
#check if the denominator is nan
df17['opioid_claim_count'].isnull().sum(), (df17['opioid_claim_count'] ==0).sum()
116/15:
# remove zeros and nan denominator 
# get opioid prescr length and average prescr length
df17_opcl = df17.query('opioid_claim_count > 0').reset_index()
df17_opcl['opioid_prescr_length'] = df17_opcl['opioid_day_supply']/df17_opcl['opioid_claim_count']
df17_opcl['avg_prescr_length'] = df17_opcl['total_day_supply']/df17_opcl['total_claim_count']

# groupby states and specialty
# calculate the provider count, average opoid prescr length and average prescr length
df17_ss_groupby = df17_opcl.groupby(
    ['nppes_provider_state','specialty_description']).apply(
    lambda x: pd.Series({
        'npi_count': x['npi'].count(),
        'opioid_length_mean': x['opioid_prescr_length'].mean(),
        'avg_length_mean': x['avg_prescr_length'].mean()
    }))

# calculate the ratio
df17_ss_groupby['presc_length_ratio'] = df17_ss_groupby['opioid_length_mean']/df17_ss_groupby['avg_length_mean']
116/16:
# select each (state, specialty) pair with at least 100 providers
# calculate max ratio
l_ratio = df17_ss_groupby[df17_ss_groupby['npi_count'] > float(100)]['presc_length_ratio']
max_ratio = round(l_ratio.max(),2)
print('The max ratio of average length in each specialty across all states is {:f}'.format(max_ratio))
116/17:
# load 16 data
df16 = pd.read_csv('PartD_Prescriber_PUF_NPI_16/PartD_Prescriber_PUF_NPI_16.txt', sep ='\t')
116/18:
# remove the suppressed provider
suppression_col = [col for col in df16.columns if 'suppress_flag' in col]
df16_new = df16[pd.isnull(df16[suppression_col]).all(axis =1)]
df17_new = df17[pd.isnull(df17[suppression_col]).all(axis =1)]
116/19:
# merge 16 and 17 based on npi
df1617 = pd.merge(df16_new[['npi', 'total_drug_cost', 'total_day_supply']], 
        df17_new[['npi', 'total_drug_cost', 'total_day_supply']],
        on = 'npi', how = 'inner', suffixes=('_16', '_17'))
df1617['daily_cost_17'] =  df1617['total_drug_cost_17'] / df1617['total_day_supply_17']
df1617['daily_cost_16'] = df1617['total_drug_cost_16'] / df1617['total_day_supply_16']
116/20:
# calculate inflation rate
inflation_rate = (df1617['daily_cost_17'] - df1617['daily_cost_16'])/ df1617['daily_cost_16']
mean_inflation_rate = round(inflation_rate.mean(),2)
print('The average inflation rate is {:f}'.format(mean_inflation_rate))
116/21:
# groupby data 16, 17 on specialty_description
df16_spec = pd.DataFrame(df16.groupby('specialty_description')['npi'].count())
df17_spec = pd.DataFrame(df17.groupby('specialty_description')['npi'].count())
# remove specialty less than 1000 proviers
df16_spec_new = df16_spec[df16_spec['npi'] >= 1_000].reset_index()
# left join
df1617_spec = df16_spec_new.join(df17_spec, how ='left', on = 'specialty_description',lsuffix= '_16', rsuffix= '_17')
116/22:
left_fraction = (df1617_spec['npi_16'] - df1617_spec['npi_17'])/df1617_spec['npi_16']
max_left_fraction = round(max(left_fraction),2)
print('The largest fraction is {:f}'.format(max_left_fraction))
116/23:
left_fraction = (df1617_spec['npi_16'] - df1617_spec['npi_17'])/df1617_spec['npi_16']
max_left_fraction = round(max(left_fraction),2)
print('The largest left fraction is {:f}'.format(max_left_fraction))
117/1: def mean_cal(N):
117/2: import numpy as np
117/3:
def mean_cal(N):
    np.random.cho
117/4: range(1,11)
117/5:
def mean_cal(N):
    np.random.choice(range(1,N+1), replace=False)
117/6: meal_cal(10)
117/7: mean_cal(10)
117/8: x = mean_cal(10)
117/9: x
117/10: np.random.choice(range(1,11),10 replace=False)
117/11: np.random.choice(range(1,11),10,replace=False)
117/12: np.random.choice(range(1,11),5,replace=False)
117/13:
N = 10
means_ = []
for _ in range(100_000):
    tmp_ = np.random.choice(range(1,N+1),5, replace=False)
    means.append(tmp_.sum())
117/14:
N = 10
totalpay = []
for _ in range(100_000):
    tmp_ = np.random.choice(range(1,N+1),5, replace=False)
    totalpay .append(tmp_.sum())
117/15:
N = 10
totalpay = []
for _ in range(100_000):
    tmp_ = np.random.choice(range(1,N+1),5, replace=False)
    totalpay .append(tmp_.sum())
totalpay.mean()
117/16:
N = 10
totalpay = []
for _ in range(100_000):
    tmp_ = np.random.choice(range(1,N+1),5, replace=False)
    totalpay .append(tmp_.sum())
np.mean(totalpay), np.std(totalpay)
117/17:
N = 10, M = 5
totalpay = []
for _ in range(100_000):
    tmp_ = np.random.choice(range(1,N+1),5, replace=False)
    totalpay .append(tmp_.sum())
np.mean(totalpay), np.std(totalpay)
117/18:
N = 10, M = 5
totalpay = []
for _ in range(100_000):
    tmp_ = np.random.choice(range(1,N+1),M, replace=False)
    totalpay .append(tmp_.sum())
np.mean(totalpay), np.std(totalpay)
117/19:
N = 10, M = 5
totalpay = []
for _ in range(100_000):
    tmp_ = np.random.choice(range(1,N+1), M, replace=False)
    totalpay .append(tmp_.sum())
np.mean(totalpay), np.std(totalpay)
117/20:
N = 10
M = 5
totalpay = []
for _ in range(100_000):
    tmp_ = np.random.choice(range(1, N+1), M, replace=False)
    totalpay .append(tmp_.sum())
np.mean(totalpay), np.std(totalpay)
117/21:
N = 10
M = 5
totalpay = []
for _ in range(1_000_000):
    tmp_ = np.random.choice(range(1, N+1), M, replace=False)
    totalpay .append(tmp_.sum())
np.mean(totalpay), np.std(totalpay)
117/22: comb(3,1)
117/23:
import numpy as np
from math import comb
117/24:
N = 10
for i in range(10):
    mean_, std_ = MN_total(N, M+1)
117/25:
def MN_total(N, M):
    totalpay = []
    for _ in range(100_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        totalpay .append(tmp_.sum())
    return np.mean(totalpay), np.std(totalpay)
117/26:
N = 10
for i in range(10):
    mean_, std_ = MN_total(N, M+1)
    print(mean_,std_)
117/27: M
117/28:
def MN_total(N, M):
    totalpay = []
    for _ in range(100_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        totalpay.append(tmp_.sum())
    print(totalpay)
    return np.mean(totalpay), np.std(totalpay)
117/29: MN_total(10, 11)
117/30: x, y = MN_total(10, 11)
117/31:
N = 10
for i in range(10):
    mean_, std_ = MN_total(N, i+1)
    print(mean_,std_)
117/32:
def MN_total(N, M):
    totalpay = []
    for _ in range(100_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        totalpay.append(tmp_.sum())
    return np.mean(totalpay), np.std(totalpay)
117/33:
N = 10
for i in range(10):
    mean_, std_ = MN_total(N, i+1)
    print(mean_,std_)
117/34:
def MN_total(N, M):
    num = []
    for _ in range(100_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        num.append(np.diff(tmp_).sum() + tmp_[0])
    
    return np.mean(num), np.std(num)
117/35: MN_total(10, 5)
117/36: np.random.choice(range(1, 11), 5, replace=False)
117/37: num.append(np.diff(tmp_).sum() + tmp_[0])
117/38: tmp_ = np.random.choice(range(1, 11), 5, replace=False)
117/39: np.diff(tmp_)
117/40: abs(np.diff(tmp_)).sum()
117/41: abs(np.diff(tmp_))
117/42: abs(np.diff(tmp_)).sum()
117/43: tmp_
117/44: abs(np.diff(tmp_)).sum()
117/45: abs(np.diff(tmp_)).sum() + tmp_[0]
117/46:
def MN_total(N, M):
    num = []
    for _ in range(100_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        num.append(abs(np.diff(tmp_)).sum() + tmp_[0])
    return np.mean(num), np.std(num)
117/47: MN_total(10, 5)
117/48:
N = 10
for i in range(10):
    mean_, std_ = MN_total(N, i+1)
    print(mean_,std_)
117/49: x,y, z = MN_total(10, 10)
117/50:
def MN_total(N, M):
    num = []
    for _ in range(100_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        num.append(abs(np.diff(tmp_)).sum() + tmp_[0])
    return np.mean(num), np.std(num), num
117/51: x,y, z = MN_total(10, 10)
117/52: plt.hist(z)
117/53:
import numpy as np
from matplotlib.pyplot import pyplot
117/54:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
117/55: plt.hist(z)
117/56:
N = 10, M = 10
mean_, std_, val = MN_total(N, M)
print([mean_,std_])
117/57:
def MN_total(N, M):
    num = []
    for _ in range(1_000_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        num.append(abs(np.diff(tmp_)).sum() + tmp_[0])
    return np.mean(num), np.std(num), num
117/58:
N, M = 10, 10
mean_, std_, val = MN_total(N, M)
print([mean_,std_])
117/59:
N, M = 10, 10
mean_, std_, val = MN_total(N, M)
print([mean_,std_])
117/60: plt.hist(val)
117/61:
def MN_total(N, M):
    num = []
    for _ in range(100_000_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        num.append(abs(np.diff(tmp_)).sum() + tmp_[0])
    return np.mean(num), np.std(num), num
117/62:
N, M = 10, 10
mean_, std_, val_10 = MN_total(N, M)
print([mean_,std_])
118/1:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
118/2:
def MN_total(N, M):
    num = []
    for _ in range(100_000_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        num.append(abs(np.diff(tmp_)).sum() + tmp_[0])
    return np.mean(num), np.std(num), num
118/3:
N, M = 10, 10
mean_, std_, val_10 = MN_total(N, M)
print([mean_,std_])
119/1:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
119/2:
def MN_total(N, M):
    num = []
    for _ in range(10_000_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        num.append(abs(np.diff(tmp_)).sum() + tmp_[0])
    return np.mean(num), np.std(num), num
119/3:
N, M = 10, 10
mean_, std_, val_10 = MN_total(N, M)
print([mean_,std_])
120/1:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
120/2:
def MN_total(N, M):
    num = []
    for _ in range(1_000_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        num.append(abs(np.diff(tmp_)).sum() + tmp_[0])
    return np.mean(num), np.std(num), num
120/3:
N, M = 10, 10
mean_, std_, val_10 = MN_total(N, M)
print([mean_,std_])
121/1:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
121/2:
def MN_total(N, M):
    num = []
    for _ in range(10_000_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        num.append(abs(np.diff(tmp_)).sum() + tmp_[0])
    return np.mean(num), np.std(num), num
121/3:
N, M = 10, 10
mean_, std_, val_10 = MN_total(N, M)
print([mean_,std_])
121/4: plt.hist(val_10)
121/5:
N, M = 20, 20
mean_, std_, val_20 = MN_total(N, M)
print([mean_,std_])
121/6:
p_10 = (val_10 >=45).sum()/len(val_10)
p_20 = (val_20 >=160).sum()/len(val_20)
print([p_10, p_20])
121/7:
p_10 = (np.array(val_10) > =45).sum()/len(val_10)
p_20 = (np.array(val_20) >=160).sum()/len(val_20)
print([p_10, p_20])
121/8:
p_10 = (np.array(val_10) >=45).sum()/len(val_10)
p_20 = (np.array(val_20) >=160).sum()/len(val_20)
print([p_10, p_20])
121/9: plt.hist(val_20)
121/10:
p_10 = (np.array(val_10) >=45).sum()/len(val_10)
p_20 = (np.array(val_20) >=160).sum()/len(val_20)
print([p_10, p_20])
121/11:
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
121/12:
def MN_total(N, M):
    num = []
    for _ in range(100_000_000):
        tmp_ = np.random.choice(range(1, N+1), M, replace=False)
        num.append(abs(np.diff(tmp_)).sum() + tmp_[0])
    return np.mean(num), np.std(num), num
121/13:
N, M = 10, 10
mean_, std_, val_10 = MN_total(N, M)
print([mean_,std_])
121/14: plt.hist(val_10)
121/15:
N, M = 20, 20
mean_, std_, val_20 = MN_total(N, M)
print([mean_,std_])
121/16: plt.hist(val_20)
121/17:
p_10 = (np.array(val_10) >=45).sum()/len(val_10)
p_20 = (np.array(val_20) >=160).sum()/len(val_20)
print([p_10, p_20])
121/18:
# import library
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
123/1: import pandas as pd
123/2:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'
df = pd.read_csv(data_path)
df.head()
123/3:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df = pd.read_csv(data_path)
df.head()
123/4:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw = pd.read_csv(data_path)
df_raw.head()
123/5: df_raw.columns[:]
123/6: time_idx = df_raw.columns[4:]
123/7:
df_plot = pd.DataFrame({
    'date':time_idx})
df_plot.head()
123/8: df_raw[df_raw['Country/Region'] == 'US'].iloc[:,4:].sum(axis=0)
123/9: df_raw[df_raw['Country/Region'] == 'Germany'].iloc[:,4:].sum(axis=0)
123/10: country_lst =['Italy', 'US', 'Spain', 'Germany']
123/11:
for each in country_lst:
    df_plot[each] = df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0)
123/12: df_plot()
123/13: df_plot.head()
123/14: df_plot.tail()
123/15:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
123/16:
import pandas as pd 
import numpy as np
123/17:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
123/18: df_plot.tail()
123/19: df_plot.head()
123/20: df_plot.set_index('date').plot()
123/21:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
123/22: df_plot.set_index('date').plot()
123/23: df_plot.set_index('date')
123/24: type(df_plot.date[0])
123/25: df_plot.date()
123/26: df_plot.head()
123/27:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
%matplotlib inline
123/28: datetime.strptime(df_plot.date[0],"%m/%d/%y")
123/29: time_str = [x.strptime("%m/%d/%y") for x in time_idx]
123/30: time_idx = [datetime.strptime(x, "%m/%d/%y") for x in df_plot.date]
123/31: time_idx
123/32: country_lst =['Italy', 'US', 'Spain', 'Germany', 'Korea,South']
123/33:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
123/34: df_plot.head()
123/35: df_plot.set_index('date').plot()
123/36: df_plot['date'] = time_idx
123/37:
df_plot['date'] = time_idx
df_plot.head()
123/38: df_plot.to_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep='\t')
123/39:
pd_data_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
123/40: pd_date_base = pd_data_base.drop(['Lat','Long'],axis=1)
123/41:
pd_date_base = pd_data_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
123/42: test_pd = pd_date_base.set_idex(['state','country'])
123/43:
test_pd = pd_date_base.set_index(['state','country'])
test_pd.head()
123/44:
test_pd = pd_date_base.set_index(['state','country']).T
test_pd.head()
123/45:
test_pd = pd_date_base.set_index(['state','country']).T
test_pd.columns
123/46: test_pd.stack(level=0)
123/47:
pd_data_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
123/48:
pd_data_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
123/49: pd_date_base.set_index(['state','country'])
123/50: pd_date_base.set_index(['state','country']).T
123/51: test_pd = pd_date_base.set_index(['state','country']).T
123/52: test_pd.columns
123/53: test_pd.stack(level=0)
123/54: test_pd.stack(level=[0,1])
123/55: test_pd.stack(level=[0,1]).reset_index()
123/56:
pd_relational_model = pd_date_base.set_index(['state','country']).T \
                        .stack(level =[0,1]).reset_index() \
                        .rename(columns={'level_0':'date', 0:'confirmed'})
123/57: pd_relational_model.head()
123/58: pd_relational_model['date'] = pd_relational_model.astype('datetime64[ns]')
123/59: pd_relational_model.dtype
123/60: pd_relational_model.dtypes
123/61: pd_relational_model['date'] = pd_relational_model.date.astype('datetime64[ns]')
123/62: pd_relational_model.dtypes
123/63: df.head()
124/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
%matplotlib inline
124/2:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw = pd.read_csv(data_path)
df_raw.head()
124/3: time_idx = df_raw.columns[4:]
124/4: country_lst =['Italy', 'US', 'Spain', 'Germany', 'Korea,South']
124/5:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
124/6:
time_idx = df_raw.columns[4:]
df_plot = pd.DataFrame({'date':time_idx})
124/7: country_lst =['Italy', 'US', 'Spain', 'Germany', 'Korea,South']
124/8:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
124/9: df_plot.head()
124/10: df_plot.set_index('date').plot()
124/11: time_idx = [datetime.strptime(x, "%m/%d/%y") for x in df_plot.date]
124/12:
df_plot['date'] = time_idx
df_plot.head()
124/13: df_plot.to_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep='\t')
124/14:
pd_data_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
124/15:
pd_data_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
124/16: test_pd = pd_date_base.set_index(['state','country']).T
124/17:
pd_date_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
124/18:
pd_date_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
124/19: test_pd = pd_date_base.set_index(['state','country']).T
124/20: test_pd.columns
124/21: test_pd.stack(level=[0,1]).reset_index()
124/22:
pd_relational_model = pd_date_base.set_index(['state','country']).T \
                        .stack(level =[0,1]).reset_index() \
                        .rename(columns={'level_0':'date', 0:'confirmed'})
124/23: pd_relational_model.head()
124/24:
pd_date_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
124/25:
pd_date_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
124/26:
pd_date_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
124/27:
pd_date_base.drop(['Lat','Long'],axis=1, replace= True)
pd_date_base.head()
124/28: pd_date_base.drop(['Lat','Long'],axis=1)
124/29: pd_date_base = pd_date_base.drop(['Lat','Long'],axis=1)
124/30: test_pd = pd_date_base.set_index(['state','country']).T
124/31: test_pd.stack(level=[0,1]).reset_index()
124/32: df_plot.to_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep='\t')
124/33:
pd_date_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
124/34: pd_date_base = pd_date_base.drop(['Lat','Long'],axis=1)
124/35:
pd_date_base = pd_date_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
124/36:
pd_date_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
124/37:
pd_date_base = pd_date_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
124/38: pd_date_base.set_index(['state','country']).T
124/39: test_pd = pd_date_base.set_index(['state','country']).T
124/40: test_pd.head()
124/41: test_pd.stack(level=[0,1]).reset_index()
124/42:
pd_relational_model = pd_date_base.set_index(['state','country']).T \
                        .stack(level =[0,1]).reset_index() \
                        .rename(columns={'level_0':'date', 0:'confirmed'})
124/43: pd_relational_model.head()
124/44: df_plot.head()
124/45: df_plot.set_index('date').plot()
124/46:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
124/47:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
124/48: mpl.rcParams['figure.figsize'] = (20, 16)
124/49: df_plot.set_index('date').plot()
124/50: mpl.rcParams['figure.figsize'] = (16, 19)
124/51: df_plot.set_index('date').plot()
124/52:
df_plot.set_index('date').plot()
plt.ylim(10,30000)
124/53:
plt.figure()
df_plot.set_index('date').plot()
plt.ylim(10,30000)
124/54: mpl.rcParams['figure.figsize'] = (16, 9)
124/55:
plt.figure()
df_plot.set_index('date').plot()
plt.ylim(10,30000)
124/56:
plt.figure()
ax = df_plot.set_index('date').plot()
plt.ylim(10,30000)
ax.set_yscale('log')
124/57: mpl.rcParams['figure.figsize'] = (16, 9)
124/58:
plt.figure()
ax = df_plot.set_index('date').plot()
plt.ylim(10,30000)
ax.set_yscale('log')
124/59:
plt.figure()
ax = df_plot.set_index('date').plot()
ax.set_yscale('log')
124/60:
plt.figure()
ax = df_plot.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
124/61:
plt.figure()
ax = df_plot.iloc[15:,:].set_index('date').plot()
ax.set_yscale('log')
124/62: df_plot.dtype()
124/63: df_plot.dtypes
124/64:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
124/65:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
124/66:
plt.figure()
ax = df_plot.iloc[15:,:].set_index('date').plot()
ax.set_yscale('log')
124/67:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
124/68:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
124/69: plotly.__version__
124/70:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
124/71: fig = go.Figure()
124/72:
fig.add_trace[go.Scatter(x=df_plot.date,
                         y=df_plot['US'])]
124/73:
fig.add_trace[go.Scatter(x=df_plot.date,
                         y=df_plot['US'])
124/74:
fig.add_trace(go.Scatter(x=df_plot.date,
                         y=df_plot['US']))
124/75: fig = go.Figure()
124/76:
fig.add_trace(go.Scatter(x=df_plot.date,
                         y=df_plot['US']))
124/77: df_plot.head()
124/78: df_plot['US']
124/79: fig.add_trace(go.Scatter(x=df_plot.date, y=df_plot['US']))
124/80: fig = go.Figure()
124/81:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
124/82:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
124/83: plotly.__version__
124/84:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
124/85: plotly.__version__
124/86:
fig = go.Figure()
fig.add_trace()
124/87:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
124/88: plotly.__version__
125/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
125/2: plotly.__version__
125/3:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
125/4:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
125/5:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw = pd.read_csv(data_path)
df_raw.head()
125/6:
time_idx = df_raw.columns[4:]
df_plot = pd.DataFrame({'date':time_idx})
125/7: country_lst =['Italy', 'US', 'Spain', 'Germany', 'Korea,South']
125/8:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
125/9: df_plot.head()
125/10: df_plot.set_index('date').plot()
125/11: time_idx = [datetime.strptime(x, "%m/%d/%y") for x in df_plot.date]
125/12:
df_plot['date'] = time_idx
df_plot.head()
125/13: df_plot.to_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep='\t')
125/14:
pd_date_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
125/15:
pd_date_base = pd_date_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
125/16: test_pd = pd_date_base.set_index(['state','country']).T
125/17:
pd_relational_model = pd_date_base.set_index(['state','country']).T \
                        .stack(level =[0,1]).reset_index() \
                        .rename(columns={'level_0':'date', 0:'confirmed'})
125/18: pd_relational_model.head()
125/19: pd_relational_model.dtypes
125/20: pd_relational_model['date'] = pd_relational_model.date.astype('datetime64[ns]')
125/21: df_plot.head()
125/22:
plt.figure()
ax = df_plot.iloc[15:,:].set_index('date').plot()
ax.set_yscale('log')
125/23:
fig = go.Figure()
fig.add_trace()
125/24:
fig = go.Figure()
fig.add_trace(go.Scatter(x = df_plot.date, y = df_plot['US']))
125/25:
fig.update_layout(width= 900, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5,5])
125/26:
fig.update_layout(width= 900, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
125/27:
fig = go.Figure()
fig.add_trace(go.Scatter(x = df_plot.date, y = df_plot['US'], mode ='markers+lines'))

fig.update_layout(width= 900, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
125/28:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = each))

fig.update_layout(width= 900, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
125/29:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 900, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
125/30:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 900, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
125/31:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 1600, 
                 height= 1200,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show(renderer='chrome')
125/32:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show(renderer='chrome')
125/33: import dash
125/34: import dash
125/35:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    dcc.Graph(figure=fig)
])
125/36: app.run_server(debug=True, use_reloader=False)
125/37:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show(renderer='chrome')
125/38:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    dcc.Graph(figure=fig)
])
125/39: app.run_server(debug=True, use_reloader=False)
125/40: app.run_server()
125/41:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    dcc.Graph(figure=fig, id = 'main_window_slope')
])
125/42: app.run_server()
125/43:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    dcc.Graph(figure=fig, id = 'main_window_slope')
])
125/44: app.run_server(debug=True, use_reloader=False)
125/45:
if __name__ == '__main__':
    app.server.run(port=8000, host='127.0.0.1')
125/46:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show(renderer='chrome')
125/47:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    dcc.Graph(figure=fig, id = 'main_window_slope')
])
125/48:
if __name__ == '__main__':
    app.server.run(port=8000, host='127.0.0.1')
125/49:
if __name__ == '__main__':
    app.server.run(debug=True, use_reloader=False)
126/1:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show(renderer='chrome')
126/2:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
126/3:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
126/4:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw = pd.read_csv(data_path)
df_raw.head()
126/5:
time_idx = df_raw.columns[4:]
df_plot = pd.DataFrame({'date':time_idx})
126/6: country_lst =['Italy', 'US', 'Spain', 'Germany', 'Korea,South']
126/7:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
126/8: df_plot.head()
126/9: df_plot.set_index('date').plot()
126/10: time_idx = [datetime.strptime(x, "%m/%d/%y") for x in df_plot.date]
126/11:
df_plot['date'] = time_idx
df_plot.head()
126/12: df_plot.to_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep='\t')
126/13:
pd_date_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
126/14:
pd_date_base = pd_date_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
126/15: test_pd = pd_date_base.set_index(['state','country']).T
126/16:
pd_relational_model = pd_date_base.set_index(['state','country']).T \
                        .stack(level =[0,1]).reset_index() \
                        .rename(columns={'level_0':'date', 0:'confirmed'})
126/17: pd_relational_model.head()
126/18: pd_relational_model.dtypes
126/19: pd_relational_model['date'] = pd_relational_model.date.astype('datetime64[ns]')
126/20: df_plot.head()
126/21:
plt.figure()
ax = df_plot.iloc[15:,:].set_index('date').plot()
ax.set_yscale('log')
126/22:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show(renderer='chrome')
126/23:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    dcc.Graph(figure=fig, id = 'main_window_slope')
])
126/24:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    html.Label('Multi-Select Country'),
    dcc.Dropdown(
        id ='country_drop_down',
        options = [
            {'label': 'Italy', 'value': 'Italy'}
            {'label': 'US', 'value': 'US'},
            {'label': 'Spain', 'value': 'Spain'},
            {'label': 'Germany', 'value': 'Germany'},
            {'label': 'Korea, South', 'value': 'Korea, South'}
        ],
        value =['US', 'Germany'],
        multi = True
    )
    dcc.Graph(figure=fig, id = 'main_window_slope')
])
126/25:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    html.Label('Multi-Select Country'),
    dcc.Dropdown(
        id ='country_drop_down',
        options = [
            {'label': 'Italy', 'value': 'Italy'},
            {'label': 'US', 'value': 'US'},
            {'label': 'Spain', 'value': 'Spain'},
            {'label': 'Germany', 'value': 'Germany'},
            {'label': 'Korea, South', 'value': 'Korea, South'}
        ],
        value =['US', 'Germany'],
        multi = True
    )
    dcc.Graph(figure=fig, id = 'main_window_slope')
])
126/26:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    html.Label('Multi-Select Country'),
    dcc.Dropdown(
        id ='country_drop_down',
        options = [
            {'label': 'Italy', 'value': 'Italy'},
            {'label': 'US', 'value': 'US'},
            {'label': 'Spain', 'value': 'Spain'},
            {'label': 'Germany', 'value': 'Germany'},
            {'label': 'Korea, South', 'value': 'Korea, South'}
        ],
        value =['US', 'Germany'],
        multi = True
    ),
    dcc.Graph(figure=fig, id = 'main_window_slope')
])
126/27: app.run_server(debug=True)
126/28:
if __name__ == '__main__':
    app.run_server(debug=True)
126/29:
if __name__ == '__main__':
    app.run_server(port=8000, host='127.0.0.1', debug=True)
126/30:
if __name__ == '__main__':
    app.run_server(host='127.0.0.1', debug=True)
126/31:
if __name__ == '__main__':
    app.run_server(host='127.0.0.1', debug=True, use_reloader=False)
126/32:
if __name__ == '__main__':
    app.run_server(host='127.0.0.1', debug=True)
127/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
127/2:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
127/3:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw = pd.read_csv(data_path)
df_raw.head()
127/4:
time_idx = df_raw.columns[4:]
df_plot = pd.DataFrame({'date':time_idx})
127/5: country_lst =['Italy', 'US', 'Spain', 'Germany', 'Korea,South']
127/6:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
127/7: df_plot.head()
127/8: df_plot.set_index('date').plot()
128/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
128/2:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
128/3:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw = pd.read_csv(data_path)
df_raw.head()
128/4:
time_idx = df_raw.columns[4:]
df_plot = pd.DataFrame({'date':time_idx})
128/5: country_lst =['Italy', 'US', 'Spain', 'Germany', 'Korea,South']
128/6:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
128/7: df_plot.head()
128/8: df_plot.set_index('date').plot()
128/9: time_idx = [datetime.strptime(x, "%m/%d/%y") for x in df_plot.date]
128/10:
df_plot['date'] = time_idx
df_plot.head()
128/11: df_plot.to_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep='\t')
128/12:
pd_date_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
128/13:
pd_date_base = pd_date_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
128/14: test_pd = pd_date_base.set_index(['state','country']).T
128/15:
pd_relational_model = pd_date_base.set_index(['state','country']).T \
                        .stack(level =[0,1]).reset_index() \
                        .rename(columns={'level_0':'date', 0:'confirmed'})
128/16: pd_relational_model.head()
128/17: pd_relational_model.dtypes
128/18: pd_relational_model['date'] = pd_relational_model.date.astype('datetime64[ns]')
128/19: df_plot.head()
128/20:
plt.figure()
ax = df_plot.iloc[15:,:].set_index('date').plot()
ax.set_yscale('log')
128/21:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show(renderer='chrome')
128/22:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    html.Label('Multi-Select Country'),
    dcc.Dropdown(
        id ='country_drop_down',
        options = [
            {'label': 'Italy', 'value': 'Italy'},
            {'label': 'US', 'value': 'US'},
            {'label': 'Spain', 'value': 'Spain'},
            {'label': 'Germany', 'value': 'Germany'},
            {'label': 'Korea, South', 'value': 'Korea, South'}
        ],
        value =['US', 'Germany'],
        multi = True
    ),
    dcc.Graph(figure=fig, id = 'main_window_slope')
])
128/23:
if __name__ == '__main__':
    app.run_server(host='127.0.0.1', debug=True, use_reloader=False)
129/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
129/2:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
129/3:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw = pd.read_csv(data_path)
df_raw.head()
129/4:
time_idx = df_raw.columns[4:]
df_plot = pd.DataFrame({'date':time_idx})
129/5: country_lst =['Italy', 'US', 'Spain', 'Germany', 'Korea,South']
129/6:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
129/7: df_plot.head()
129/8: df_plot.set_index('date').plot()
129/9: time_idx = [datetime.strptime(x, "%m/%d/%y") for x in df_plot.date]
129/10:
df_plot['date'] = time_idx
df_plot.head()
129/11: df_plot.to_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep='\t')
129/12:
pd_date_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
129/13:
pd_date_base = pd_date_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
129/14: test_pd = pd_date_base.set_index(['state','country']).T
129/15:
pd_relational_model = pd_date_base.set_index(['state','country']).T \
                        .stack(level =[0,1]).reset_index() \
                        .rename(columns={'level_0':'date', 0:'confirmed'})
129/16: pd_relational_model.head()
129/17: pd_relational_model.dtypes
129/18: pd_relational_model['date'] = pd_relational_model.date.astype('datetime64[ns]')
129/19: df_plot.head()
129/20:
plt.figure()
ax = df_plot.iloc[15:,:].set_index('date').plot()
ax.set_yscale('log')
129/21:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show(renderer='chrome')
129/22:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    
    html.Label('Multi-Select Country'),
    
    dcc.Dropdown(
        id='country_drop_down',
        options=[
            {'label': 'Italy', 'value': 'Italy'},
            {'label': 'US', 'value': 'US'},
            {'label': 'Spain', 'value': 'Spain'},
            {'label': 'Germany', 'value': 'Germany'},
            {'label': 'Korea, South', 'value': 'Korea, South'}
        ],
        value=['US', 'Germany'], # which are pre-selected
        multi=True
    ),   
        
    dcc.Graph(figure=fig, id='main_window_slope')
])
129/23:
if __name__ == '__main__':
    app.run_server(host='127.0.0.1', debug=True, use_reloader=False)
130/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
130/2:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
130/3:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw = pd.read_csv(data_path)
df_raw.head()
130/4:
time_idx = df_raw.columns[4:]
df_plot = pd.DataFrame({'date':time_idx})
130/5: country_lst =['Italy', 'US', 'Spain', 'Germany', 'Korea,South']
130/6:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
130/7: df_plot.head()
130/8: df_plot.set_index('date').plot()
130/9: time_idx = [datetime.strptime(x, "%m/%d/%y") for x in df_plot.date]
130/10:
df_plot['date'] = time_idx
df_plot.head()
130/11: df_plot.to_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep='\t')
130/12:
pd_date_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
130/13:
pd_date_base = pd_date_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
130/14: test_pd = pd_date_base.set_index(['state','country']).T
130/15:
pd_relational_model = pd_date_base.set_index(['state','country']).T \
                        .stack(level =[0,1]).reset_index() \
                        .rename(columns={'level_0':'date', 0:'confirmed'})
130/16: pd_relational_model.head()
130/17: pd_relational_model.dtypes
130/18: pd_relational_model['date'] = pd_relational_model.date.astype('datetime64[ns]')
130/19: df_plot.head()
130/20:
plt.figure()
ax = df_plot.iloc[15:,:].set_index('date').plot()
ax.set_yscale('log')
130/21:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show(renderer='chrome')
130/22:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    
    html.Label('Multi-Select Country'),
    
    dcc.Dropdown(
        id='country_drop_down',
        options=[
            {'label': 'Italy', 'value': 'Italy'},
            {'label': 'US', 'value': 'US'},
            {'label': 'Spain', 'value': 'Spain'},
            {'label': 'Germany', 'value': 'Germany'},
            {'label': 'Korea, South', 'value': 'Korea, South'}
        ],
        value=['US', 'Germany'], # which are pre-selected
        multi=True
    ),   
        
    dcc.Graph(figure=fig, id='main_window_slope')
])
130/23:
from dash.dependencies import Input, Output

@app.callback(
    Output('main_window_slope','figure'),
    [Input('country_drop_down', 'value')]
)
def update_figure(country_list):
    traces = []
    for each in country_list:
        traces.append(dict(x=df_plot.date,
                                y=df_plot[each],
                                mode='markers+lines',
                                opacity=0.9,
                                line_width=2,
                                marker_size=4, 
                                name=each
                        )
                )
    return {
        'data':traces,
        'layout':dict(
            width= 800, 
            height= 600,
            xaxis_title = 'Time',
            yaxis_title = 'Confirmed infected people, log-scale'
        )
    }
130/24:
if __name__ == '__main__':
    app.run_server(host='127.0.0.1', debug=True, use_reloader=False)
132/1: import pandas as pd
132/2: df_analyse = pd.read('covid-19/data/processed/COVID_small_flat_table.csv', sep= '\t')
132/3: df_analyse = pd.read_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep= '\t')
132/4: df_analyse.head()
132/5:
df_analyse = pd.read_csv('covid-19/data/processed/COVID_small_flat_table.csv',\
                         sep= '\t', parse_dates=[0])
df_analyse.sort_values('date', ascending = True).tail()
132/6:
df_analyse = pd.read_csv('covid-19/data/processed/COVID_small_flat_table.csv',\
                         sep= '\t', parse_dates=[0], index=False)
df_analyse.sort_values('date', ascending = True).tail()
132/7:
df_analyse = pd.read_csv('covid-19/data/processed/COVID_small_flat_table.csv',\
                         sep= '\t',index=False)
df_analyse.sort_values('date', ascending = True).tail()
132/8:
df_analyse = pd.read_csv('covid-19/data/processed/COVID_small_flat_table.csv',\
                         sep= '\t', parse_dates=[0])
df_analyse.sort_values('date', ascending = True).tail()
133/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
133/2:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
133/3:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw = pd.read_csv(data_path)
df_raw.head()
133/4:
time_idx = df_raw.columns[4:]
df_plot = pd.DataFrame({'date':time_idx})
133/5: country_lst =['Italy', 'US', 'Spain', 'Germany', 'Korea,South']
133/6:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
133/7: df_plot.head()
133/8: df_plot.set_index('date').plot()
133/9: time_idx = [datetime.strptime(x, "%m/%d/%y") for x in df_plot.date]
133/10:
df_plot['date'] = time_idx
df_plot.head()
133/11: df_plot.to_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep='\t', index = Fasle)
133/12: df_plot.to_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep='\t', index = False)
133/13:
pd_date_base = df_raw.rename(columns={'Country/Region': 'country',
                                     'Province/State':'state'})
133/14:
pd_date_base = pd_date_base.drop(['Lat','Long'],axis=1)
pd_date_base.head()
133/15: test_pd = pd_date_base.set_index(['state','country']).T
133/16:
pd_relational_model = pd_date_base.set_index(['state','country']).T \
                        .stack(level =[0,1]).reset_index() \
                        .rename(columns={'level_0':'date', 0:'confirmed'})
133/17: pd_relational_model.head()
133/18: pd_relational_model.dtypes
133/19: pd_relational_model['date'] = pd_relational_model.date.astype('datetime64[ns]')
133/20: df_plot.head()
133/21:
plt.figure()
ax = df_plot.iloc[15:,:].set_index('date').plot()
ax.set_yscale('log')
133/22:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show(renderer='chrome')
132/9: import pandas as pd
132/10:
df_analyse = pd.read_csv('covid-19/data/processed/COVID_small_flat_table.csv',\
                         sep= '\t', parse_dates=[0])
df_analyse.sort_values('date', ascending = True).tail()
133/23:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    
    html.Label('Multi-Select Country'),
    
    dcc.Dropdown(
        id='country_drop_down',
        options=[
            {'label': 'Italy', 'value': 'Italy'},
            {'label': 'US', 'value': 'US'},
            {'label': 'Spain', 'value': 'Spain'},
            {'label': 'Germany', 'value': 'Germany'},
            {'label': 'Korea, South', 'value': 'Korea, South'}
        ],
        value=['US', 'Germany'], # which are pre-selected
        multi=True
    ),   
        
    dcc.Graph(figure=fig, id='main_window_slope')
])
133/24:
from dash.dependencies import Input, Output

@app.callback(
    Output('main_window_slope','figure'),
    [Input('country_drop_down', 'value')]
)
def update_figure(country_list):
    traces = []
    for each in country_list:
        traces.append(dict(x=df_plot.date,
                                y=df_plot[each],
                                mode='markers+lines',
                                opacity=0.9,
                                line_width=2,
                                marker_size=4, 
                                name=each
                        )
                )
    return {
        'data':traces,
        'layout':dict(
            width= 800, 
            height= 600,
            xaxis_title = 'Time',
            yaxis_title = 'Confirmed infected people, log-scale'
        )
    }
133/25:
if __name__ == '__main__':
    app.run_server(host='127.0.0.1', debug=True, use_reloader=False)
132/11:
import matplotlib as mpl
import matplotlib.pyplot as plt
import os
import pandas as pd
import numpy as np

%matplotlib inline
mpl.rcParams['figure.figsize'] = (16, 10)
pd.set_option('display.max_rows', 500)

import plotly.graph_objects as go
132/12:
df_analyse = pd.read_csv('covid-19/data/processed/COVID_small_flat_table.csv',\
                         sep= '\t', parse_dates=[0])
df_analyse.sort_values('date', ascending = True).tail()
132/13:
# helper functions
def quick_plot(x_in, df_input,y_scale='log',slider=False):
    """ Quick basic plot for quick static evaluation of a time series
    
        you can push selective columns of your data frame by .iloc[:,[0,6,7,8]]
        
        Parameters:
        ----------
        x_in : array 
            array of date time object, or array of numbers
        df_input : pandas dataframe 
            the plotting matrix where each column is plotted
            the name of the column will be used for the legend
        scale: str
            y-axis scale as 'log' or 'linear'
        slider: bool
            True or False for x-axis slider
    
        
        Returns:
        ----------
        
    """
    fig = go.Figure()

    for each in df_input.columns:
        fig.add_trace(go.Scatter(
                        x=x_in,
                        y=df_input[each],
                        name=each,
                        opacity=0.8))
    
    fig.update_layout(autosize=True,
        width=1024,
        height=768,
        font=dict(
            family="PT Sans, monospace",
            size=18,
            color="#7f7f7f"
            )
        )
    fig.update_yaxes(type=y_scale),
    fig.update_xaxes(tickangle=-45,
                 nticks=20,
                 tickfont=dict(size=14,color="#7f7f7f")
                )
    if slider==True:
        fig.update_layout(xaxis_rangeslider_visible=True)
    fig.show()
132/14: quick_plot(df_analyse.date, df_analyse.iloc[:,1:], y_scale='log', slider=True)
132/15: quick_plot(df_analyse.date, df_analyse.iloc[:,1:], y_scale='liner', slider=True)
132/16: quick_plot(df_analyse.date, df_analyse.iloc[:,1:], y_scale='linear', slider=True)
132/17: threshold = 100
132/18:
compare_list = []
for pos, country in enumerate(df_analyse.columns[1:]):
    compare_list.append(
        np.array(df_analyse[country][df_analyse[country]>threshold]))
132/19: compare_list
132/20: pd.DataFrame(compare_list)
132/21: pd.DataFrame(compare_list, index = df_analyse.columns[1:])
132/22: pd.DataFrame(compare_list, index = df_analyse.columns[1:]).T
132/23: pd_sync_timelines = pd.DataFrame(compare_list, index = df_analyse.columns[1:]).T
132/24: pd_sync_timelines['date'] = np.arange(pd_sync_timelines.shape[0])
132/25: quick_plot(pd_sync_timelines.date, pd_sync_timelines.iloc[:,1:], y_scale = 'linear', slider=True)
132/26: quick_plot(pd_sync_timelines.date, pd_sync_timelines.iloc[:,1:-1], y_scale = 'log', slider=True)
132/27:
def doubling_rate(N_0,t,T_d):
    return N_0*np.power(2,t/T_d)
132/28:
max_days = 34
norm_slope = {
    'doubling every day':doubling_rate(100, np.range(max_days),1),
    'doubling every two days': doubling_rate(100, np.range(max_days),2),
    'doubling every four days': doubling_rate(100, np.range(max_days),4)ï¼Œ
    'doubling every ten days': doubling_rate(100, np.range(max_days),10),
}
132/29:
max_days = 34
norm_slope = {
    'doubling every day':doubling_rate(100, np.range(max_days),1),
    'doubling every two days': doubling_rate(100, np.range(max_days),2),
    'doubling every four days': doubling_rate(100, np.range(max_days),4),
    'doubling every ten days': doubling_rate(100, np.range(max_days),10),
}
132/30:
max_days = 34
norm_slope = {
    'doubling every day':doubling_rate(100, np.arange(max_days),1),
    'doubling every two days': doubling_rate(100, np.arange(max_days),2),
    'doubling every four days': doubling_rate(100, np.arange(max_days),4),
    'doubling every ten days': doubling_rate(100, np.arange(max_days),10),
}
132/31: pd_sync_timelines_w_slope = pd.concat([pd.DataFrame(norm_slopes),pd_sync_timelines], axis=1)
132/32: pd_sync_timelines_w_slope = pd.concat([pd.DataFrame(norm_slope),pd_sync_timelines], axis=1)
132/33:
quick_plot(pd_sync_timelines_w_slope.date,
           pd_sync_timelines_w_slope.iloc[:,0:5],
           y_scale='log',
           slider=True)
132/34:
from sklearn import linear_model
reg = linear_model.LinearRegression(fit_intercept=False)
132/35:
l_vec = len(df_analyse['Germany'])
X = np.arange(l_vec)
y = np.array(df_analyse['Germany'])
132/36: X
132/37: X.reshape[-1,1]
132/38: X.reshape(-1,1)
132/39: reg.fit(X, y)
132/40:
l_vec = len(df_analyse['Germany'])
X = np.arange(l_vec).reshape(-1,1)
y = np.array(df_analyse['Germany'])
132/41: reg.fit(X, y)
132/42: reg.predic([[0]])
132/43: reg.predict([[0]])
132/44:
LR_inspect=df_analyse[['date','Germany']].copy()
LR_inspect['prediction']=Y_hat
132/45:
LR_inspect=df_analyse[['date','Germany']].copy()
LR_inspect['prediction']=y_hat
132/46:
X_hat = np.arange(l_vec).reshape(-1,1)
y_hat = reg.predict(X_hat)
132/47:
LR_inspect=df_analyse[['date','Germany']].copy()
LR_inspect['prediction']=y_hat
132/48:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='log',
           slider=True)
132/49:
LR_inspect=df_analyse[['date','Germany']].copy()
LR_inspect['prediction']=np.exp(y_hat)
132/50:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='log',
           slider=True)
132/51:
LR_inspect=df_analyse[['date','Germany']].copy()
LR_inspect['prediction']= np.exp(y_hat)
132/52:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='log',
           slider=True)
132/53:
LR_inspect=df_analyse[['date','Germany']].copy()
LR_inspect['prediction']= y_hat
132/54:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='log',
           slider=True)
132/55:
X_hat = np.arange(l_vec).reshape(-1,1)
y_hat = np.exp(reg.predict(X_hat))
132/56:
LR_inspect=df_analyse[['date','Germany']].copy()
LR_inspect['prediction']= y_hat
132/57:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='log',
           slider=True)
132/58:
X_hat = np.arange(l_vec).reshape(-1,1)
y_hat = exp(reg.predict(X_hat))
132/59:
X_hat = np.arange(l_vec).reshape(-1,1)
y_hat = math.exp(reg.predict(X_hat))
132/60:
X_hat = np.arange(l_vec).reshape(-1,1)
y_hat = np.exp(reg.predict(X_hat))
132/61:
LR_inspect=df_analyse[['date','Germany']].copy()
LR_inspect['prediction']= scipy.special.expit(y_hat)
132/62:
import matplotlib as mpl
import matplotlib.pyplot as plt
import os
import pandas as pd
import numpy as np
import scipy

%matplotlib inline
mpl.rcParams['figure.figsize'] = (16, 10)
pd.set_option('display.max_rows', 500)

import plotly.graph_objects as go
132/63:
X_hat = np.arange(l_vec).reshape(-1,1)
y_hat = reg.predict(X_hat)
132/64:
LR_inspect=df_analyse[['date','Germany']].copy()
LR_inspect['prediction']= scipy.special.expit(y_hat)
132/65:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='log',
           slider=True)
132/66:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='linear',
           slider=True)
132/67:
LR_inspect=df_analyse[['date','Germany']].copy()
LR_inspect['prediction']= y_hat
132/68:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='linear',
           slider=True)
132/69:
LR_inspect = df_analyse[['date','Germany']].copy()
LR_inspect['prediction'] = np.exp(y_hat)
132/70:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='linear',
           slider=True)
132/71:
LR_inspect = df_analyse[['date','Germany']].copy()
LR_inspect['prediction'] = y_hat
132/72:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='linear',
           slider=True)
132/73:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='log',
           slider=True)
132/74: y_hat
132/75: np.exp(y_hat)
132/76: df_analyse
132/77:
l_vec = len(df_analyse['Germany'])
X = np.arange(l_vec-50).reshape(-1,1)
y = np.array(df_analyse['Germany'][50:])
132/78:
reg = linear_model.LinearRegression(fit_intercept=True)
l_vec = len(df_analyse['Germany'])
X = np.arange(l_vec-50).reshape(-1,1)
y = np.array(df_analyse['Germany'][50:])
132/79: reg.fit(X,y)
132/80: reg.intercept_
132/81: reg.coef_
132/82:
def get_rate_via_regression(in_array):
    y = np.array(in_array)
    X = np.arange(-1,2).reshape(-1,1)
    
    assert len(in_array) == 3
    
    reg.fit(X,y)
    intercept = reg.intercept_
    slope = reg.coef_
    
    return intercept/slope
132/83: df_analyse['Germany'].rolling(window = 3, min_period=3).apply(get_rate_via_regressionia_regression)
132/84: df_analyse['Germany'].rolling(window = 3, min_periods=3).apply(get_rate_via_regressionia_regression)
132/85: df_analyse['Germany'].rolling(window = 3, min_periods=3).apply(get_rate_via_regression)
132/86: df_analyse['Germany_RT'] = df_analyse['Germany'].rolling(window = 3, min_periods=3).apply(get_rate_via_regression)
132/87: quick_plot(df_analyse.date, df_analyse.iloc[:,[6]], y_scale='linear')
132/88: quick_plot(df_analyse.date, df_analyse.iloc[40:,[6]], y_scale='linear')
132/89:
def doubling_time(in_array):
    y = np.array(in_array)
    return len(y)*np.log(2)/np.log(y[-1]/y[0])
132/90: df_analyse['Germany_wiki']=df_analyse['Germany'].rolling(window = 3, min_periods=3).apply(doubling_time)
132/91: quick_plot(df_analyse.date, df_analyse.iloc[40:,[6,7]], y_scale='linear')
134/1:
import subprocess
import os

import pandas as pd
import numpy as np

from datetime import datetime

import requests
import json

def get_johns_hopkins():
    ''' Get data by a git pull request, the source code has to be pulled first
        Result is stored in the predifined csv structure
    '''
    git_pull = subprocess.Popen( "/usr/bin/git pull" ,
                         cwd = os.path.dirname( 'data/raw/COVID-19/' ),
                         shell = True,
                         stdout = subprocess.PIPE,
                         stderr = subprocess.PIPE )
    (out, error) = git_pull.communicate()


    print("Error : " + str(error))
    print("out : " + str(out))


if __name__ == '__main__':
    get_johns_hopkins()
134/2: pwd
134/3:
import subprocess
import os

import pandas as pd
import numpy as np

from datetime import datetime

import requests
import json

def get_johns_hopkins():
    ''' Get data by a git pull request, the source code has to be pulled first
        Result is stored in the predifined csv structure
    '''
    git_pull = subprocess.Popen( "/usr/bin/git pull" ,
                         cwd = os.path.dirname( 'covid-19/data/raw/COVID-19/' ),
                         shell = True,
                         stdout = subprocess.PIPE,
                         stderr = subprocess.PIPE )
    (out, error) = git_pull.communicate()


    print("Error : " + str(error))
    print("out : " + str(out))


if __name__ == '__main__':
    get_johns_hopkins()
135/1:
import subprocess
import os

import pandas as pd
import numpy as np

from datetime import datetime

import requests
import json

def get_johns_hopkins():
    ''' Get data by a git pull request, the source code has to be pulled first
        Result is stored in the predifined csv structure
    '''
    git_pull = subprocess.Popen( "/usr/bin/git pull" ,
                         cwd = os.path.dirname( 'covid-19/data/raw/COVID-19/' ),
                         shell = True,
                         stdout = subprocess.PIPE,
                         stderr = subprocess.PIPE )
    (out, error) = git_pull.communicate()


    print("Error : " + str(error))
    print("out : " + str(out))


if __name__ == '__main__':
    get_johns_hopkins()
135/2:
import pandas as pd
import numpy as np

from datetime import datetime


def store_relational_JH_data():
    ''' Transformes the COVID data in a relational data set

    '''

    data_path='data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
    pd_raw=pd.read_csv(data_path)

    pd_data_base=pd_raw.rename(columns={'Country/Region':'country',
                      'Province/State':'state'})

    pd_data_base['state']=pd_data_base['state'].fillna('no')

    pd_data_base=pd_data_base.drop(['Lat','Long'],axis=1)


    pd_relational_model=pd_data_base.set_index(['state','country']) \
                                .T                              \
                                .stack(level=[0,1])             \
                                .reset_index()                  \
                                .rename(columns={'level_0':'date',
                                                   0:'confirmed'},
                                                  )

    pd_relational_model['date']=pd_relational_model.date.astype('datetime64[ns]')

    pd_relational_model.to_csv('data/processed/COVID_relational_confirmed.csv',sep=';',index=False)
    print(' Number of rows stored: '+str(pd_relational_model.shape[0]))

if __name__ == '__main__':

    store_relational_JH_data()
135/3:
import pandas as pd
import numpy as np

from datetime import datetime


def store_relational_JH_data():
    ''' Transformes the COVID data in a relational data set

    '''

    data_path='covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
    pd_raw=pd.read_csv(data_path)

    pd_data_base=pd_raw.rename(columns={'Country/Region':'country',
                      'Province/State':'state'})

    pd_data_base['state']=pd_data_base['state'].fillna('no')

    pd_data_base=pd_data_base.drop(['Lat','Long'],axis=1)


    pd_relational_model=pd_data_base.set_index(['state','country']) \
                                .T                              \
                                .stack(level=[0,1])             \
                                .reset_index()                  \
                                .rename(columns={'level_0':'date',
                                                   0:'confirmed'},
                                                  )

    pd_relational_model['date']=pd_relational_model.date.astype('datetime64[ns]')

    pd_relational_model.to_csv('data/processed/COVID_relational_confirmed.csv',sep=';',index=False)
    print(' Number of rows stored: '+str(pd_relational_model.shape[0]))

if __name__ == '__main__':

    store_relational_JH_data()
135/4:
import subprocess
import os

import pandas as pd
import numpy as np

from datetime import datetime

import requests
import json

def get_johns_hopkins():
    ''' Get data by a git pull request, the source code has to be pulled first
        Result is stored in the predifined csv structure
    '''
    git_pull = subprocess.Popen( "/usr/bin/git pull" ,
                         cwd = os.path.dirname( 'covid-19/data/raw/COVID-19/' ),
                         shell = True,
                         stdout = subprocess.PIPE,
                         stderr = subprocess.PIPE )
    (out, error) = git_pull.communicate()


    print("Error : " + str(error))
    print("out : " + str(out))


if __name__ == '__main__':
    get_johns_hopkins()
135/5:
import subprocess
import os

import pandas as pd
import numpy as np

from datetime import datetime

import requests
import json

def get_johns_hopkins():
    ''' Get data by a git pull request, the source code has to be pulled first
        Result is stored in the predifined csv structure
    '''
    git_pull = subprocess.Popen( "/usr/bin/git pull" ,
                         cwd = os.path.dirname( 'covid-19/data/raw/COVID-19' ),
                         shell = True,
                         stdout = subprocess.PIPE,
                         stderr = subprocess.PIPE )
    (out, error) = git_pull.communicate()


    print("Error : " + str(error))
    print("out : " + str(out))


if __name__ == '__main__':
    get_johns_hopkins()
135/6:
import subprocess
import os

import pandas as pd
import numpy as np

from datetime import datetime

import requests
import json

def get_johns_hopkins():
    ''' Get data by a git pull request, the source code has to be pulled first
        Result is stored in the predifined csv structure
    '''
    git_pull = subprocess.Popen( "https://github.com/CSSEGISandData/COVID-19" ,
                         cwd = os.path.dirname('covid-19/data/raw/COVID-19'),
                         shell = True,
                         stdout = subprocess.PIPE,
                         stderr = subprocess.PIPE )
    (out, error) = git_pull.communicate()


    print("Error : " + str(error))
    print("out : " + str(out))


if __name__ == '__main__':
    get_johns_hopkins()
135/7:
import subprocess
import os

import pandas as pd
import numpy as np

from datetime import datetime

import requests
import json

def get_johns_hopkins():
    ''' Get data by a git pull request, the source code has to be pulled first
        Result is stored in the predifined csv structure
    '''
    git_pull = subprocess.Popen( "https://github.com/CSSEGISandData/COVID-19.git" ,
                         cwd = os.path.dirname('covid-19/data/raw/COVID-19'),
                         shell = True,
                         stdout = subprocess.PIPE,
                         stderr = subprocess.PIPE )
    (out, error) = git_pull.communicate()


    print("Error : " + str(error))
    print("out : " + str(out))


if __name__ == '__main__':
    get_johns_hopkins()
135/8:
import subprocess
import os

import pandas as pd
import numpy as np

from datetime import datetime

import requests
import json

def get_johns_hopkins(url):
    ''' Get data by a git pull request, the source code has to be pulled first
        Result is stored in the predifined csv structure
    '''
    git_pull = subprocess.Popen( ['/usr/bin/git pull', url],
                         cwd = os.path.dirname('covid-19/data/raw/COVID-19'),
                         shell = True,
                         stdout = subprocess.PIPE,
                         stderr = subprocess.PIPE )
    (out, error) = git_pull.communicate()


    print("Error : " + str(error))
    print("out : " + str(out))


if __name__ == '__main__':
    get_johns_hopkins('https://github.com/CSSEGISandData/COVID-19')
135/9:
import subprocess
import os

import pandas as pd
import numpy as np

from datetime import datetime

import requests
import json

def get_johns_hopkins(url):
    ''' Get data by a git pull request, the source code has to be pulled first
        Result is stored in the predifined csv structure
    '''
    git_pull = subprocess.Popen( ['/usr/bin/git pull', url],
                         cwd = os.path.dirname('covid-19/data/raw/COVID-19'),
                         shell = True,
                         stdout = subprocess.PIPE,
                         stderr = subprocess.PIPE )
    (out, error) = git_pull.communicate()


    print("Error : " + str(error))
    print("out : " + str(out))


if __name__ == '__main__':
    get_johns_hopkins('https://github.com/CSSEGISandData/COVID-19.git')
135/10:
import subprocess
import os

import pandas as pd
import numpy as np

from datetime import datetime

import requests
import json

def get_johns_hopkins(url):
    ''' Get data by a git pull request, the source code has to be pulled first
        Result is stored in the predifined csv structure
    '''
    git_pull = subprocess.Popen( ['/usr/bin/git pull', url],
                         cwd = os.path.dirname('covid-19/data/raw/COVID-192'),
                         shell = True,
                         stdout = subprocess.PIPE,
                         stderr = subprocess.PIPE )
    (out, error) = git_pull.communicate()


    print("Error : " + str(error))
    print("out : " + str(out))


if __name__ == '__main__':
    get_johns_hopkins('https://github.com/CSSEGISandData/COVID-19.git')
136/1:
import subprocess
import os

import pandas as pd
import numpy as np

from datetime import datetime

import requests
import json

def get_johns_hopkins(url):
    ''' Get data by a git pull request, the source code has to be pulled first
        Result is stored in the predifined csv structure
    '''
    git_pull = subprocess.Popen( ['/usr/bin/git pull', url],
                         cwd = os.path.dirname('covid-19/data/raw/COVID-192'),
                         shell = True,
                         stdout = subprocess.PIPE,
                         stderr = subprocess.PIPE )
    (out, error) = git_pull.communicate()


    print("Error : " + str(error))
    print("out : " + str(out))


if __name__ == '__main__':
    get_johns_hopkins('https://github.com/CSSEGISandData/COVID-19.git')
136/2:
import pandas as pd
import numpy as np

from datetime import datetime


def store_relational_JH_data():
    ''' Transformes the COVID data in a relational data set

    '''

    data_path='covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_USA.csv'
    pd_raw=pd.read_csv(data_path)

    pd_data_base=pd_raw.rename(columns={'Country/Region':'country',
                      'Province/State':'state'})

    pd_data_base['state']=pd_data_base['state'].fillna('no')

    pd_data_base=pd_data_base.drop(['Lat','Long'],axis=1)


    pd_relational_model=pd_data_base.set_index(['state','country']) \
                                .T                              \
                                .stack(level=[0,1])             \
                                .reset_index()                  \
                                .rename(columns={'level_0':'date',
                                                   0:'confirmed'},
                                                  )

    pd_relational_model['date']=pd_relational_model.date.astype('datetime64[ns]')

    pd_relational_model.to_csv('covid-19/data/processed/COVID_relational_confirmed.csv',sep='\t',index=False)
    print(' Number of rows stored: '+str(pd_relational_model.shape[0]))

if __name__ == '__main__':

    store_relational_JH_data()
137/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
137/2:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
137/3:
data_path2 = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'
df_raw_us = pd.read_csv(data_path2)
df_raw_us.head()
137/4:
data_path2 = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'
df_raw_us = pd.read_csv(data_path2)
df_raw_us.head()
137/5:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw = pd.read_csv(data_path)
df_raw.head()
137/6:
data_path = 'datasets/time_series_covid19_confirmed_US.csv'
df_raw_us = pd.read_csv(data_path)
df_raw_us.head()
137/7: df_raw.columns[9:]
137/8: df_raw_us.columns[9:]
137/9: df_raw_us.columns[11:]
137/10:
time_idx = df_raw.columns[11:]
df_plot = pd.DataFrame({'date':time_idx})
137/11:
time_idx = df_raw_us.columns[11:]
df_plot = pd.DataFrame({'date':time_idx})
137/12: state_lst = df['Province_State'].unique()
137/13: state_lst = df_raw_us['Province_State'].unique()
137/14:
state_lst = df_raw_us['Province_State'].unique()
state_lst
137/15: state_lst = df_raw_us['Province_State'].unique()
137/16:
for state in state_lst:
    df_plot[state] = np.array(df_raw_us[df_raw_us['Province_State'] == each].iloc[:,4:].sum(axis=0))
137/17:
for state in state_lst:
    df_plot[state] = np.array(df_raw_us[df_raw_us['Province_State'] == state].iloc[:,11:].sum(axis=0))
137/18: df_plot.head()
137/19: df_plot.set_index('date').plot()
137/20: time_idx = [datetime.strptime(x, "%m/%d/%y") for x in df_plot.date]
137/21:
df_plot['date'] = time_idx
df_plot.head()
137/22: df_raw_us.head()
137/23: df_us = pd.concat([df_raw_us['Province_State'], df_raw_us.iloc[:,11:]])
137/24:
df_us = pd.concat([df_raw_us['Province_State'], df_raw_us.iloc[:,11:]])
df_us.head()
137/25:
df_us = pd.concat([df_raw_us['Province_State'], df_raw_us.iloc[:,11:]]).rename(columns={0:'state'})
df_us.head()
137/26: df_raw_us.iloc[:,11:]
137/27:
df_us = pd.concat([df_raw_us['Province_State'], df_raw_us.iloc[:,11:]])
df_us.head()
137/28:
df_us = pd.concat([df_raw_us['Province_State'], df_raw_us.iloc[:,11:]], axis=1)
df_us.head()
137/29:
df_us = pd.concat([df_raw_us['Province_State'], 
                   df_raw_us.iloc[:,11:]], axis=1)
df_us.rename(columns={'Province_State':'state'})
137/30:
df_us = pd.concat([df_raw_us['Province_State'], 
                   df_raw_us.iloc[:,11:]], axis=1)
df_us.head()
137/31: df_us.set_index(['state','country']).T
137/32: df_us.set_index(['Province_State']).T
137/33:
data_path = 'datasets/time_series_covid19_confirmed_US.csv'
df_raw_us = pd.read_csv(data_path)
df_raw_us.shape
137/34: df_raw_us.groupby('Province_State').sum()
137/35:
df_us = pd.concat([df_raw_us['Province_State'], 
                   df_raw_us.iloc[:,11:]], axis=1)
df_us.head()
df_us.shape
137/36: df_raw_us.groupby('Province_State').sum()
137/37: df_us.groupby('Province_State').sum()
137/38:
df_us = pd.concat([df_raw_us['Province_State'], 
                   df_raw_us.iloc[:,11:]], axis=1)
df_us_gp = df_us.groupby('Province_State').sum()
df_us_gp.shape
137/39:
state_lst = df_raw_us['Province_State'].unique()
for state in state_lst:
    df_us[state] = df_raw_us[df_raw_us['Province_State']==state].iloc[:,11:].sum(axis=0)
137/40: df_us.head()
137/41:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
137/42:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
137/43:
data_path = 'datasets/time_series_covid19_confirmed_US.csv'
df_raw_us = pd.read_csv(data_path)
df_raw_us.head()
137/44:
time_idx = df_raw_us.columns[11:]
df_us = pd.DataFrame({'date':time_idx})
137/45:
state_lst = df_raw_us['Province_State'].unique()
for state in state_lst:
    df_us[state] = df_raw_us[df_raw_us['Province_State']==state].iloc[:,11:].sum(axis=0)
137/46: df_us.head()
137/47:
state_lst = df_raw_us['Province_State'].unique()
for state in state_lst:
    df_us[state] = np.array(df_raw_us[df_raw_us['Province_State']==state]).iloc[:,11:].sum(axis=0)
137/48:
state_lst = df_raw_us['Province_State'].unique()
for state in state_lst:
    df_us[state] = np.array(df_raw_us[df_raw_us['Province_State']==state].iloc[:,11:].sum(axis=0))
137/49: df_us.head()
137/50: df_plot.set_index('date').plot()
137/51: df_us.set_index('date').plot()
137/52:
data_path = 'datasets/time_series_covid19_confirmed_US.csv'
df_raw_us = pd.read_csv(data_path)
137/53:
time_idx = df_raw_us.columns[11:]
df_us = pd.DataFrame({'date':time_idx})
df_us.date.astype('datetime64[ns]')
137/54:
state_lst = df_raw_us['Province_State'].unique()
for state in state_lst:
    df_us[state] = np.array(df_raw_us[df_raw_us['Province_State']==state].iloc[:,11:].sum(axis=0))
137/55:
time_idx = df_raw_us.columns[11:]
df_us = pd.DataFrame({'date':time_idx})
df_us['date'] = df_us.date.astype('datetime64[ns]')
137/56:
state_lst = df_raw_us['Province_State'].unique()
for state in state_lst:
    df_us[state] = np.array(df_raw_us[df_raw_us['Province_State']==state].iloc[:,11:].sum(axis=0))
137/57: df_us.head()
137/58:
state_lst = df_raw_us['Province_State'].unique()
for state in state_lst:
    df_us[state] = np.array(df_raw_us[df_raw_us['Province_State']==state].iloc[:,11:].sum(axis=0))
137/59: df_us.head()
140/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
140/2:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
140/3:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw = pd.read_csv(data_path)
df_raw.head()
140/4:
data_path = 'covid-19/data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'
df_raw= pd.read_csv(data_path)
df_raw.head()
140/5:
time_idx = df_raw.columns[4:]
df_plot = pd.DataFrame({'date':time_idx})
140/6: country_lst =['Italy', 'US', 'Spain', 'Germany', 'Korea,South']
140/7:
for each in country_lst:
    df_plot[each] = np.array(df_raw[df_raw['Country/Region'] == each].iloc[:,4:].sum(axis=0))
140/8: df_plot.head()
140/9: df_plot.set_index('date').plot()
140/10: time_idx = [datetime.strptime(x, "%m/%d/%y") for x in df_plot.date]
140/11:
df_plot['date'] = time_idx
df_plot.head()
140/12: df_plot.to_csv('covid-19/data/processed/COVID_small_flat_table.csv', sep='\t', index = False)
140/13:
plt.figure()
ax = df_plot.iloc[15:,:].set_index('date').plot()
ax.set_yscale('log')
140/14:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_plot.date, 
                             y = df_plot[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people, log-scale')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show(renderer='chrome')
140/15:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    
    html.Label('Multi-Select Country'),
    
    dcc.Dropdown(
        id='country_drop_down',
        options=[
            {'label': 'Italy', 'value': 'Italy'},
            {'label': 'US', 'value': 'US'},
            {'label': 'Spain', 'value': 'Spain'},
            {'label': 'Germany', 'value': 'Germany'},
            {'label': 'Korea, South', 'value': 'Korea, South'}
        ],
        value=['US', 'Germany'], # which are pre-selected
        multi=True
    ),   
        
    dcc.Graph(figure=fig, id='main_window_slope')
])
140/16:
if __name__ == '__main__':
    app.run_server(host='127.0.0.1', debug=True, use_reloader=False)
141/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
141/2:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
141/3:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
141/4:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
141/5:
global_confirm_path = 'datasets/time_series_covid19_confirmed_global.csv'
usa_confirm_path ='datasets/time_series_covid19_confirmed_US.csv'
df_usa, state_lst = clean_confirm(usa_confirm_path, 'usa')
df_global, country_lst = clean_confirm(global_confirm_path, 'global')
141/6: df_glbal.head()
141/7: df_global.head()
141/8: sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea,South', 'UK']]
141/9: sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'UK']]
141/10: country_lst
141/11: sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
141/12:
plt.figure()
ax = sub_global.iloc[15:,:].set_index('date').plot()
ax.set_yscale('log')
141/13:
plt.figure()
ax = sub_global.iloc[2:,:].set_index('date').plot()
ax.set_yscale('log')
141/14:
plt.figure()
ax = sub_global.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
141/15: state_lst
141/16:
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas']]
141/17:
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas', 'Florida']]
141/18:
plt.figure()
ax = sub_usa.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
141/19:
fig, axes = subplots(cols=2)
plt.figure()
ax = sub_global.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
141/20:
fig, ax = plt.subplots(cols=2)
ax[0] = sub_global.iloc[:,:].set_index('date').plot()
ax[0].set_yscale('log')
141/21:
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 2)
ax0 = sub_global.iloc[:,:].set_index('date').plot()
ax0.set_yscale('log')
ax1 = sub_usa.iloc[:,:].set_index('date').plot()
ax1.set_yscale('log')
141/22:
fig, (ax0, ax1, ax2) = plt.subplots(ncols = 2)
ax0 = sub_global.iloc[:,:].set_index('date').plot()
ax0.set_yscale('log')
ax1 = sub_usa.iloc[:,:].set_index('date').plot()
ax1.set_yscale('log')
141/23:
fig, (ax0, ax1) = plt.subplots(ncols = 2)
ax0 = sub_global.iloc[:,:].set_index('date').plot()
ax0.set_yscale('log')
ax1 = sub_usa.iloc[:,:].set_index('date').plot()
ax1.set_yscale('log')
141/24:
fig, (ax0, ax1) = plt.subplots(nrows = 2)
ax0 = sub_global.iloc[:,:].set_index('date').plot()
ax0.set_yscale('log')
ax1 = sub_usa.iloc[:,:].set_index('date').plot()
ax1.set_yscale('log')
141/25:
fig, (ax0, ax1) = plt.subplots(nrows = 2)
ax0 = sub_global.iloc[:,:].set_index('date').plot()
#ax0.set_yscale('log')
ax1 = sub_usa.iloc[:,:].set_index('date').plot()
#ax1.set_yscale('log')
141/26:
fig, (ax0, ax1) = plt.subplots(nrows = 2, figures = (10,16))
ax0 = sub_global.iloc[:,:].set_index('date').plot()
ax0.set_yscale('log')
ax1 = sub_usa.iloc[:,:].set_index('date').plot()
ax1.set_yscale('log')
141/27:
fig, (ax0, ax1) = plt.subplots(nrows = 2, figsize = (10,16))
ax0 = sub_global.iloc[:,:].set_index('date').plot()
ax0.set_yscale('log')
ax1 = sub_usa.iloc[:,:].set_index('date').plot()
ax1.set_yscale('log')
141/28:
ax0, ax1 = plt.subplots(nrows = 2)
ax0 = sub_global.iloc[:,:].set_index('date').plot()
ax0.set_yscale('log')
ax1 = sub_usa.iloc[:,:].set_index('date').plot()
ax1.set_yscale('log')
141/29:
ax0, ax1 = plt.subplots(nrows = 2)
ax0 = sub_global.iloc.set_index('date').plot()
ax0.set_yscale('log')
ax1 = sub_usa.iloc.set_index('date').plot()
ax1.set_yscale('log')
141/30:
ax0, ax1 = plt.subplots(nrows = 2)
ax0 = sub_global.iloc[:,:].set_index('date').plot()
ax0.set_yscale('log')
ax1 = sub_usa.iloc[:,:].set_index('date').plot()
ax1.set_yscale('log')
141/31:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
141/32:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
141/33:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
141/34:
global_confirm_path = 'datasets/time_series_covid19_confirmed_global.csv'
usa_confirm_path ='datasets/time_series_covid19_confirmed_US.csv'
df_usa, state_lst = clean_confirm(usa_confirm_path, 'usa')
df_global, country_lst = clean_confirm(global_confirm_path, 'global')
141/35: state_lst
141/36:
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas', 'Florida']]
141/37:
ax0, ax1 = plt.subplots(nrows = 2)
ax0 = sub_global.iloc[:,:].set_index('date').plot()
ax0.set_yscale('log')
ax1 = sub_usa.iloc[:,:].set_index('date').plot()
ax1.set_yscale('log')
141/38:
plt.figure()
ax = sub_usa.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
141/39:
plt.figure()
ax = sub_usa.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
141/40:
plt.figure()
ax = sub_global.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
141/41:
fig = go.Figure()
for ct in country_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
141/42:
fig = go.Figure()
for ct in state_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
141/43:
sub_global_lst = ['Italy', 'US', 'China', 'Korea, South', 'Iran']
sub_usa_lst = ['New York', 'California', 'Michigan', 'Texas', 'Florida']
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas', 'Florida']]
141/44:
fig = go.Figure()
for ct in sub_usa_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='log', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
141/45:
fig = go.Figure()
for ct in sub_usa_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='linear', range=[1.1, 5.5])
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
141/46:
fig = go.Figure()
for ct in sub_usa_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='linear')
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
141/47:
fig = go.Figure()
for ct in sub_usa_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='log')
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
142/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
142/2:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
142/3:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
142/4:
global_confirm_path = 'datasets/time_series_covid19_confirmed_global.csv'
usa_confirm_path ='datasets/time_series_covid19_confirmed_US.csv'
df_usa, state_lst = clean_confirm(usa_confirm_path, 'usa')
df_global, country_lst = clean_confirm(global_confirm_path, 'global')
142/5:
sub_global_lst = ['Italy', 'US', 'China', 'Korea, South', 'Iran']
sub_usa_lst = ['New York', 'California', 'Michigan', 'Texas', 'Florida']
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas', 'Florida']]
142/6:
plt.figure()
ax = sub_global.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
142/7:
plt.figure()
ax = sub_usa.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
142/8:
fig = go.Figure()
for ct in sub_usa_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='log')
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
142/9:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    
    html.Label('Multi-Select State'),
    
    dcc.Dropdown(
        id='state_drop_down',
        options=[
            {'label': 'New York', 'value': 'New York'},
            {'label': 'California', 'value': 'California'},
            {'label': 'Michigan', 'value': 'Michigan'},
            {'label': 'Texas', 'value': 'Texas'},
            {'label': 'Florida', 'value': 'v'}
        ],
        value=['New York', 'California'], # which are pre-selected
        multi=True
    ),   
        
    dcc.Graph(figure=fig, id='main_window_slope')
])
142/10:
from dash.dependencies import Input, Output

@app.callback(
    Output('main_window_slope','figure'),
    [Input('state_drop_down', 'value')]
)
def update_figure(sub_usa_lst):
    traces = []
    for each in sub_usa_lst:
        traces.append(dict(x=df_usa.date,
                           y=df_usa[each],
                           mode='markers+lines',
                           opacity=0.9,
                           line_width=2,
                           marker_size=4, 
                           name=each
                        )
                )
    return {
        'data': traces,
            'layout': dict (
                width=1280,
                height=720,
                xaxis_title="Time",
                yaxis_title="Confirmed infected people in USA (log)",
                xaxis={'tickangle':-45,
                        'nticks':20,
                        'tickfont':dict(size=14,color="#7f7f7f"),
                        
                      },
                yaxis={'type':"log",
                       'range':'[1.1,5.5]'
                      }
        )
    }
142/11:
if __name__ == '__main__':
    app.run_server(host='127.0.0.1', debug=True, use_reloader=False)
145/1:
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
%matplotlib inline

import plotly.graph_objects as go
mpl.rcParams['figure.figsize'] = (16, 10)
145/2: from fbprophet import Prophet
145/3:
import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
%matplotlib inline

import plotly.graph_objects as go
mpl.rcParams['figure.figsize'] = (16, 10)
plt.style.use('fivethirtyeight')

from fbprophet import Prophet
145/4:
df = pd.DataFrame({'X': np.arange(0,10)}) # generate an input df
df['y']=df.rolling(3).mean()
146/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
146/2:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
146/3:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
146/4:
global_confirm_path = 'datasets/time_series_covid19_confirmed_global.csv'
usa_confirm_path ='datasets/time_series_covid19_confirmed_US.csv'
df_usa, state_lst = clean_confirm(usa_confirm_path, 'usa')
df_global, country_lst = clean_confirm(global_confirm_path, 'global')
146/5:
sub_global_lst = ['Italy', 'US', 'China', 'Korea, South', 'Iran']
sub_usa_lst = ['New York', 'California', 'Michigan', 'Texas', 'Florida']
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas', 'Florida']]
146/6:
sub_usa.to_csv('datasets', sep='\t' index=False)
sub_global.to_csv('datesets', sep = '\t',index=False)
146/7:
sub_usa.to_csv('datasets', sep='\t',index=False)
sub_global.to_csv('datesets', sep = '\t',index=False)
146/8:
sub_usa.to_csv('datasets/', sep='\t',index=False)
sub_global.to_csv('datesets/', sep = '\t',index=False)
146/9:
sub_usa.to_csv('datasets/sub_usa.csv', sep='\t',index=False)
sub_global.to_csv('datesets/sub_global.csv', sep = '\t',index=False)
146/10:
sub_usa.to_csv('/datasets/sub_usa.csv', sep='\t',index=False)
sub_global.to_csv('/datesets/sub_global.csv', sep = '\t',index=False)
146/11:
sub_usa.to_csv('datasets/sub_usa.csv', sep='\t',index=False)
sub_global.to_csv('datasets/sub_global.csv', sep = '\t',index=False)
147/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly
147/2:
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
147/3:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
147/4:
global_confirm_path = 'datasets/time_series_covid19_confirmed_global.csv'
usa_confirm_path ='datasets/time_series_covid19_confirmed_US.csv'
df_usa, state_lst = clean_confirm(usa_confirm_path, 'usa')
df_global, country_lst = clean_confirm(global_confirm_path, 'global')
147/5:
sub_global_lst = ['Italy', 'US', 'China', 'Korea, South', 'Iran']
sub_usa_lst = ['New York', 'California', 'Michigan', 'Texas', 'Florida']
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas', 'Florida']]
147/6:
sub_usa.to_csv('datasets/sub_usa.csv', sep='\t',index=False)
sub_global.to_csv('datasets/sub_global.csv', sep = '\t',index=False)
145/5:
df_all = pd.read_csv('/datasets/sub_usa.csv',sep='\t')
df=df_all[['date','New York']]
df=df.rename(columns={'date': 'ds',
                        'New York': 'y'})
145/6:
df_all = pd.read_csv('datasets/sub_usa.csv',sep='\t')
df=df_all[['date','New York']]
df=df.rename(columns={'date': 'ds',
                        'New York': 'y'})
145/7: df.head()
145/8:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
145/9:
# set some basic parameters
# beta/gamma is denoted as  'basic reproduction number'

N0=1000000 #max susceptible population
beta=0.4   # infection spread dynamics
gamma=0.1  # recovery rate


# condition I0+S0+R0=N0
I0=df_analyse.US[10]
S0=N0-I0
R0=0
145/10:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
145/11:
# set some basic parameters
# beta/gamma is denoted as  'basic reproduction number'

N0=1000000 #max susceptible population
beta=0.4   # infection spread dynamics
gamma=0.1  # recovery rate


# condition I0+S0+R0=N0
I0=df_global.US[10]
S0=N0-I0
R0=0
145/12:
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected people
        R: recovered people
        beta: 
        
        overall condition is that the sum of changes (differnces) sum up to 0
        dS+dI+dR=0
        S+I+R= N (constant size of population)
    
    '''
    
    S,I,R=SIR
    dS_dt=-beta*S*I/N0          #S*I is the 
    dI_dt=beta*S*I/N0-gamma*I
    dR_dt=gamma*I
    return([dS_dt,dI_dt,dR_dt])
145/13:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
145/14:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
145/15:
ydata = np.array(df_global[10:])
t=np.arange(len(ydata))
145/16:
# set some basic parameters
# beta/gamma is denoted as  'basic reproduction number'

N0=1000000 #max susceptible population
beta=0.4   # infection spread dynamics
gamma=0.1  # recovery rate


# condition I0+S0+R0=N0
I0=df_global.US[10]
S0=N0-I0
R0=0
145/17:
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected people
        R: recovered people
        beta: 
        
        overall condition is that the sum of changes (differnces) sum up to 0
        dS+dI+dR=0
        S+I+R= N (constant size of population)
    
    '''
    
    S,I,R=SIR
    dS_dt=-beta*S*I/N0          #S*I is the 
    dI_dt=beta*S*I/N0-gamma*I
    dR_dt=gamma*I
    return([dS_dt,dI_dt,dR_dt])
145/18:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
145/19:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
145/20:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
145/21:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
145/22:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize
from scipy import integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
145/23:
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected people
        R: recovered people
        beta: 
        
        overall condition is that the sum of changes (differnces) sum up to 0
        dS+dI+dR=0
        S+I+R= N (constant size of population)
    
    '''
    
    S,I,R=SIR
    dS_dt=-beta*S*I/N0          #S*I is the 
    dI_dt=beta*S*I/N0-gamma*I
    dR_dt=gamma*I
    return([dS_dt,dI_dt,dR_dt])
145/24:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
145/25:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
145/26:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
145/27:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
145/28:
# set some basic parameters
# beta/gamma is denoted as  'basic reproduction number'

N0=1000000 #max susceptible population
beta=0.4   # infection spread dynamics
gamma=0.1  # recovery rate


# condition I0+S0+R0=N0
I0=df_global.US[10]
S0=N0-I0
R0=0
145/29:
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        t: time step, mandatory for integral.odeint
        I: infected people
        R: recovered people
        beta: 
        
        overall condition is that the sum of changes (differnces) sum up to 0
        dS+dI+dR=0
        S+I+R= N (constant size of population)
    
    '''
    
    S,I,R=SIR
    dS_dt=-beta*S*I/N0          #S*I is the 
    dI_dt=beta*S*I/N0-gamma*I
    dR_dt=gamma*I
    return dS_dt,dI_dt,dR_dt
145/30:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
145/31:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
145/32:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
145/33:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
145/34:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize, integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
145/35:
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        t: time step, mandatory for integral.odeint
        I: infected people
        R: recovered people
        beta: 
        
        overall condition is that the sum of changes (differnces) sum up to 0
        dS+dI+dR=0
        S+I+R= N (constant size of population)
    
    '''
    
    S,I,R=SIR
    dS_dt=-beta*S*I/N0          #S*I is the 
    dI_dt=beta*S*I/N0-gamma*I
    dR_dt=gamma*I
    return dS_dt,dI_dt,dR_dt
145/36:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
145/37:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
145/38:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize, integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
148/1:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize, integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
148/2:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
148/3:
ydata = np.array(df_global[10:])
t=np.arange(len(ydata))
148/4:
# set some basic parameters
# beta/gamma is denoted as  'basic reproduction number'

N0=1000000 #max susceptible population
beta=0.4   # infection spread dynamics
gamma=0.1  # recovery rate


# condition I0+S0+R0=N0
I0=df_global.US[10]
S0=N0-I0
R0=0
148/5:
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        t: time step, mandatory for integral.odeint
        I: infected people
        R: recovered people
        beta: 
        
        overall condition is that the sum of changes (differnces) sum up to 0
        dS+dI+dR=0
        S+I+R= N (constant size of population)
    
    '''
    
    S,I,R=SIR
    dS_dt=-beta*S*I/N0          #S*I is the 
    dI_dt=beta*S*I/N0-gamma*I
    dR_dt=gamma*I
    return dS_dt,dI_dt,dR_dt
148/6:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
148/7:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
148/8:
popt, pcov = optimize.curve_fit(fit_odeint, t, ydata)
perr = np.sqrt(np.diag(pcov))
    
print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
148/9:
plt.semilogy(t, ydata, 'o')
plt.semilogy(t, fitted)
plt.title("Fit of SIR model for Germany cases")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
print("Basic Reproduction Number R0 " , popt[0]/ popt[1])
print("This ratio is derived as the expected number of new infections (these new infections are sometimes called secondary infections from a single infection in a population where all subjects are susceptible. @wiki")
148/10: t
148/11:
# set some basic parameters
# beta/gamma is denoted as  'basic reproduction number'

N0=1000000 #max susceptible population
beta=0.4   # infection spread dynamics
gamma=0.1  # recovery rate


# condition I0+S0+R0=N0
I0=df_global.US[10]
S0=N0-I0
R0=0
148/12:
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        t: time step, mandatory for integral.odeint
        I: infected people
        R: recovered people
        beta: 
        
        overall condition is that the sum of changes (differnces) sum up to 0
        dS+dI+dR=0
        S+I+R= N (constant size of population)
    
    '''
    
    S,I,R=SIR
    dS_dt=-beta*S*I/N0          #S*I is the 
    dI_dt=beta*S*I/N0-gamma*I
    dR_dt=gamma*I
    return dS_dt,dI_dt,dR_dt
148/13:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
148/14:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
148/15: ydata
148/16:
ydata = np.array(df_global.US[10:])
t=np.arange(len(ydata))
148/17:
# set some basic parameters
# beta/gamma is denoted as  'basic reproduction number'

N0=1000000 #max susceptible population
beta=0.4   # infection spread dynamics
gamma=0.1  # recovery rate


# condition I0+S0+R0=N0
I0=df_global.US[10]
S0=N0-I0
R0=0
148/18:
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        t: time step, mandatory for integral.odeint
        I: infected people
        R: recovered people
        beta: 
        
        overall condition is that the sum of changes (differnces) sum up to 0
        dS+dI+dR=0
        S+I+R= N (constant size of population)
    
    '''
    
    S,I,R=SIR
    dS_dt=-beta*S*I/N0          #S*I is the 
    dI_dt=beta*S*I/N0-gamma*I
    dR_dt=gamma*I
    return dS_dt,dI_dt,dR_dt
148/19:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
148/20:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
148/21:
popt, pcov = optimize.curve_fit(fit_odeint, t, ydata)
perr = np.sqrt(np.diag(pcov))
    
print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
148/22: plt.plot(ydata)
148/23:
ydata = np.array(df_global.US[20:])
t=np.arange(len(ydata))
148/24:
# set some basic parameters
# beta/gamma is denoted as  'basic reproduction number'

N0=1000000 #max susceptible population
beta=0.4   # infection spread dynamics
gamma=0.1  # recovery rate


# condition I0+S0+R0=N0
I0=df_global.US[20]
S0=N0-I0
R0=0
148/25:
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        t: time step, mandatory for integral.odeint
        I: infected people
        R: recovered people
        beta: 
        
        overall condition is that the sum of changes (differnces) sum up to 0
        dS+dI+dR=0
        S+I+R= N (constant size of population)
    
    '''
    
    S,I,R=SIR
    dS_dt=-beta*S*I/N0          #S*I is the 
    dI_dt=beta*S*I/N0-gamma*I
    dR_dt=gamma*I
    return dS_dt,dI_dt,dR_dt
148/26:
def fit_odeint(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1] # we only would like to get dI
148/27:
# example curve of our differential equationa
popt=[0.4,0.1]
fit_odeint(t, *popt)
148/28:
popt, pcov = optimize.curve_fit(fit_odeint, t, ydata)
perr = np.sqrt(np.diag(pcov))
    
print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
148/29:
plt.semilogy(t, ydata, 'o')
plt.semilogy(t, fitted)
plt.title("Fit of SIR model for Germany cases")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
print("Basic Reproduction Number R0 " , popt[0]/ popt[1])
print("This ratio is derived as the expected number of new infections (these new infections are sometimes called secondary infections from a single infection in a population where all subjects are susceptible. @wiki")
148/30:
plt.semilogy(t, ydata, 'o')
plt.semilogy(t, fitted)
plt.title("Fit of SIR model for Germany cases")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
print("Basic Reproduction Number R0 " , popt[0]/ popt[1])
print("This ratio is derived as the expected number of new infections (these new infections are sometimes called secondary infections from a single infection in a population where all subjects are susceptible. @wiki")
148/31: fitted=fit_odeint(t, *popt)
148/32:
plt.semilogy(t, ydata, 'o')
plt.semilogy(t, fitted)
plt.title("Fit of SIR model for Germany cases")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
print("Basic Reproduction Number R0 " , popt[0]/ popt[1])
print("This ratio is derived as the expected number of new infections (these new infections are sometimes called secondary infections from a single infection in a population where all subjects are susceptible. @wiki")
148/33:
t_initial=28
t_intro_measures=14
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/34:
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/35:
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected people
        R: recovered people
        beta: 
        
        overall condition is that the sum of changes (differnces) sum up to 0
        dS+dI+dR=0
        S+I+R= N (constant size of population)
    
    '''
    
    S,I,R=SIR
    dS_dt=-beta*S*I/N0          #S*I is the 
    dI_dt=beta*S*I/N0-gamma*I
    dR_dt=gamma*I
    return([dS_dt,dI_dt,dR_dt])
148/36:
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/37:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/38:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
148/39:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected USA',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
148/40:
t_initial=19
t_intro_measures=14
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/41:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/42:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected USA',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
148/43:
t_initial=25
t_intro_measures=14
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/44:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/45:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected USA',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
148/46:
t_initial=36
t_intro_measures=14
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/47:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/48:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected USA',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
148/49:
# example curve of our differential equationa
params_=[0.4,0.1]
odeint_solve(t, *params_)
148/50:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize, integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
148/51:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
148/52:
ydata = np.array(df_global.US[20:])
t=np.arange(len(ydata))
148/53:
# set some basic parameters
# beta/gamma is denoted as  'basic reproduction number'

N0=1000000 #max susceptible population
beta=0.4   # infection spread dynamics
gamma=0.1  # recovery rate


# condition I0+S0+R0=N0
I0=df_global.US[20]
S0=N0-I0
R0=0
148/54:
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])
148/55:
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt
148/56:
def odeint_solve(x, beta, gamma):
    '''
    helper function for the integration
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
148/57:
# example curve of our differential equationa
params_=[0.4,0.1]
odeint_solve(t, *params_)
148/58:
popt, pcov = optimize.curve_fit(odeint_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))
    
print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
148/59:
def odeint_solver(x, beta, gamma):
    '''
    helper function
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
148/60:
popt, pcov = optimize.curve_fit(odeint_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))
    
print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
148/61: fitted=SIR_solver(t, *popt)
148/62:
# set some basic parameters
# beta/gamma is denoted as  'basic reproduction number'

N0=1000000 #max susceptible population
beta=0.4   # infection spread dynamics
gamma=0.1  # recovery rate


# condition I0+S0+R0=N0
I0=df_global.US[20]
S0=N0-I0
R0=0
148/63:
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])
148/64:
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt
148/65:
def SIR_solver(x, beta, gamma):
    '''
    helper function
    '''
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
148/66:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
148/67: fitted=SIR_solver(t, *popt)
148/68:
plt.semilogy(t, ydata, 'o')
plt.semilogy(t, fitted)
plt.title("Fit of SIR model for Germany cases")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
print("Basic Reproduction Number R0 " , popt[0]/ popt[1])
print("This ratio is derived as the expected number of new infections (these new infections are sometimes called secondary infections from a single infection in a population where all subjects are susceptible. @wiki")
148/69:
plt.semilogy(t, ydata, 'o')
plt.semilogy(t, fitted)
plt.title("Fit of SIR model for Germany cases")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/70:
plt.semilogy(t, ydata, 'o')
plt.semilogy(t, fitted)
plt.title("SIR MODEL based on the data from confirmation cases in USA")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/71:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label = 'simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
plt.legend()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/72:
plt.semilogy(t, ydata, 'o', label1='true')
plt.semilogy(t, fitted, label2='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
plt.legend()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/73:
ax = plt.semilogy(t, ydata, 'o')
ax = plt.semilogy(t, fitted)
plt.title("SIR MODEL based on the data from confirmation cases in USA")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
plt.legend()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/74:
ax0 = plt.semilogy(t, ydata, 'o')
ax1 = plt.semilogy(t, fitted)
plt.title("SIR MODEL based on the data from confirmation cases in USA")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
plt.legend([ax])
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/75:
ax0 = plt.semilogy(t, ydata, 'o')
ax1 = plt.semilogy(t, fitted)
plt.title("SIR MODEL based on the data from confirmation cases in USA")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
plt.legend([ax0, ax1], ['true', 'simulated'])
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/76:
ax0 = plt.semilogy(t, ydata, 'o', label = 'true')
ax1 = plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.show()
plt.legend(handles=[ax0,ax1])
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/77:
ax0 = plt.semilogy(t, ydata, 'o', label = 'true')
ax1 = plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.legend(handles=[ax0,ax1])
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/78:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.legend()
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/79:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA")
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/80:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=30)
plt.ylabel("Population infected")
plt.xlabel("Days")
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/81:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
print("Basic Reproduction Number R0 " , round(popt[0]/ popt[1]))
148/82:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
148/83:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
148/84:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
148/85:
t_initial=36
t_intro_measures=20
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/86:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/87:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected USA',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
148/88:
t_initial=36
t_intro_measures=36
t_hold=36
t_relax=36

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/89:
t_initial=40
t_intro_measures=50
t_hold=60
t_relax=60

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/90:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/91:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected USA',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
148/92:
t_initial=40
t_intro_measures=50
t_hold=60
t_relax=60

beta_max=0.19
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/93:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/94:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected USA',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
148/95:
t_initial=40
t_intro_measures=50
t_hold=60
t_relax=60

beta_max=0.34
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/96:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/97:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected USA',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
148/98:
t_initial=30
t_intro_measures=30
t_hold=30
t_relax=30

beta_max=0.34
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/99:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/100:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected USA',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
148/101:
t_initial=30
t_intro_measures=30
t_hold=30
t_relax=30

beta_max=0.34
beta_min=0.08
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/102:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
148/103:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected USA',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
148/104:
t_initial=30
t_intro_measures=30
t_hold=30
t_relax=30

beta_max=0.34
beta_min=0.08
gamma=0.05
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
148/105:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
144/1:
import matplotlib as mpl
import matplotlib.pyplot as plt
import os
import pandas as pd
import numpy as np
import scipy

%matplotlib inline
mpl.rcParams['figure.figsize'] = (16, 10)
pd.set_option('display.max_rows', 500)

import plotly.graph_objects as go
144/2:
df_analyse = pd.read_csv('datasets/sub_global.csv',\
                         sep= '\t', parse_dates=[0])
df_analyse.sort_values('date', ascending = True).tail()
144/3:
# helper functions
def quick_plot(x_in, df_input,y_scale='log',slider=False):
    """ Quick basic plot for quick static evaluation of a time series
    
        you can push selective columns of your data frame by .iloc[:,[0,6,7,8]]
        
        Parameters:
        ----------
        x_in : array 
            array of date time object, or array of numbers
        df_input : pandas dataframe 
            the plotting matrix where each column is plotted
            the name of the column will be used for the legend
        scale: str
            y-axis scale as 'log' or 'linear'
        slider: bool
            True or False for x-axis slider
    
        
        Returns:
        ----------
        
    """
    fig = go.Figure()

    for each in df_input.columns:
        fig.add_trace(go.Scatter(
                        x=x_in,
                        y=df_input[each],
                        name=each,
                        opacity=0.8))
    
    fig.update_layout(autosize=True,
        width=1024,
        height=768,
        font=dict(
            family="PT Sans, monospace",
            size=18,
            color="#7f7f7f"
            )
        )
    fig.update_yaxes(type=y_scale),
    fig.update_xaxes(tickangle=-45,
                 nticks=20,
                 tickfont=dict(size=14,color="#7f7f7f")
                )
    if slider==True:
        fig.update_layout(xaxis_rangeslider_visible=True)
    fig.show()
144/4: quick_plot(df_analyse.date, df_analyse.iloc[:,1:], y_scale='linear', slider=True)
144/5: threshold = 100
144/6:
compare_list = []
for pos, country in enumerate(df_analyse.columns[1:]):
    compare_list.append(
        np.array(df_analyse[country][df_analyse[country]>threshold]))
144/7:
import matplotlib as mpl
import matplotlib.pyplot as plt
import os
import pandas as pd
import numpy as np
import scipy

%matplotlib inline
mpl.rcParams['figure.figsize'] = (9, 8)
pd.set_option('display.max_rows', 500)

import plotly.graph_objects as go
144/8:
df_analyse = pd.read_csv('datasets/sub_global.csv',\
                         sep= '\t', parse_dates=[0])
df_analyse.sort_values('date', ascending = True).tail()
144/9:
# helper functions
def quick_plot(x_in, df_input,y_scale='log',slider=False):
    """ Quick basic plot for quick static evaluation of a time series
    
        you can push selective columns of your data frame by .iloc[:,[0,6,7,8]]
        
        Parameters:
        ----------
        x_in : array 
            array of date time object, or array of numbers
        df_input : pandas dataframe 
            the plotting matrix where each column is plotted
            the name of the column will be used for the legend
        scale: str
            y-axis scale as 'log' or 'linear'
        slider: bool
            True or False for x-axis slider
    
        
        Returns:
        ----------
        
    """
    fig = go.Figure()

    for each in df_input.columns:
        fig.add_trace(go.Scatter(
                        x=x_in,
                        y=df_input[each],
                        name=each,
                        opacity=0.8))
    
    fig.update_layout(autosize=True,
        width=1024,
        height=768,
        font=dict(
            family="PT Sans, monospace",
            size=18,
            color="#7f7f7f"
            )
        )
    fig.update_yaxes(type=y_scale),
    fig.update_xaxes(tickangle=-45,
                 nticks=20,
                 tickfont=dict(size=14,color="#7f7f7f")
                )
    if slider==True:
        fig.update_layout(xaxis_rangeslider_visible=True)
    fig.show()
144/10: quick_plot(df_analyse.date, df_analyse.iloc[:,1:], y_scale='linear', slider=True)
144/11: threshold = 100
144/12:
compare_list = []
for pos, country in enumerate(df_analyse.columns[1:]):
    compare_list.append(
        np.array(df_analyse[country][df_analyse[country]>threshold]))
144/13: pd_sync_timelines = pd.DataFrame(compare_list, index = df_analyse.columns[1:]).T
144/14: pd_sync_timelines['date'] = np.arange(pd_sync_timelines.shape[0])
144/15: quick_plot(pd_sync_timelines.date, pd_sync_timelines.iloc[:,1:-1], y_scale = 'log', slider=True)
144/16:
def doubling_rate(N_0,t,T_d):
    return N_0*np.power(2,t/T_d)
144/17:
max_days = 34
norm_slope = {
    'doubling every day':doubling_rate(100, np.arange(max_days),1),
    'doubling every two days': doubling_rate(100, np.arange(max_days),2),
    'doubling every four days': doubling_rate(100, np.arange(max_days),4),
    'doubling every ten days': doubling_rate(100, np.arange(max_days),10),
}
144/18: pd_sync_timelines_w_slope = pd.concat([pd.DataFrame(norm_slope),pd_sync_timelines], axis=1)
144/19:
quick_plot(pd_sync_timelines_w_slope.date,
           pd_sync_timelines_w_slope.iloc[:,0:5],
           y_scale='log',
           slider=True)
144/20:
from sklearn import linear_model
reg = linear_model.LinearRegression(fit_intercept=False)
144/21:
l_vec = len(df_analyse['Germany'])
X = np.arange(l_vec).reshape(-1,1)
y = np.array(df_analyse['Germany'])
144/22:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='log',
           slider=True)
144/23:
l_vec = len(df_analyse['USA'])
X = np.arange(l_vec).reshape(-1,1)
y = np.array(df_analyse['USA'])
144/24:
l_vec = len(df_analyse['US'])
X = np.arange(l_vec).reshape(-1,1)
y = np.array(df_analyse['US'])
144/25: reg.fit(X, y)
144/26:
X_hat = np.arange(l_vec).reshape(-1,1)
y_hat = reg.predict(X_hat)
144/27:
LR_inspect = df_analyse[['date','Germany']].copy()
LR_inspect['prediction'] = y_hat
144/28:
LR_inspect = df_analyse[['date','US']].copy()
LR_inspect['prediction'] = y_hat
144/29:
quick_plot(LR_inspect.date,
           LR_inspect.iloc[:,1:],
           y_scale='log',
           slider=True)
144/30:
reg = linear_model.LinearRegression(fit_intercept=True)
l_vec = len(df_analyse['Germany'])
X = np.arange(l_vec-50).reshape(-1,1)
y = np.array(df_analyse['Germany'][50:])
144/31:
reg = linear_model.LinearRegression(fit_intercept=True)
l_vec = len(df_analyse['US'])
X = np.arange(l_vec-50).reshape(-1,1)
y = np.array(df_analyse['US'][50:])
144/32: reg.fit(X,y)
144/33: reg.intercept_
144/34: reg.coef_
144/35:
def get_rate_via_regression(in_array):
    y = np.array(in_array)
    X = np.arange(-1,2).reshape(-1,1)
    
    assert len(in_array) == 3
    
    reg.fit(X,y)
    intercept = reg.intercept_
    slope = reg.coef_
    
    return intercept/slope
144/36: df_analyse['US_RT'] = df_analyse['US'].rolling(window = 3, min_periods=3).apply(get_rate_via_regression)
144/37: quick_plot(df_analyse.date, df_analyse.iloc[40:,[6]], y_scale='linear')
144/38:
def doubling_time(in_array):
    y = np.array(in_array)
    return len(y)*np.log(2)/np.log(y[-1]/y[0])
144/39: df_analyse['US_wiki']=df_analyse['US'].rolling(window = 3, min_periods=3).apply(doubling_time)
144/40: quick_plot(df_analyse.date, df_analyse.iloc[40:,[6,7]], y_scale='linear')
148/106:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
ydata = np.array(df_global.US[20:])
t=np.arange(len(ydata))
148/107: fitted=SIR_solver(t, *popt)
148/108:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
149/1:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize, integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
149/2:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
ydata = np.array(df_global.US[20:])
t=np.arange(len(ydata))
149/3:
N0=1000000 #max susceptible population, fixed values
beta=0.4   # infection rate
gamma=0.1  # recovery rate

# initial populations I0+S0+R0=N0
I0=df_global.US[20]
S0=N0-I0
R0=0

# SIR model v1
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])

# SIR model v2, same setting as v1
# SIR_t only for optimization 
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt

# helper function to calculate SIR
def SIR_solver(x, beta, gamma):
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
149/4:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
149/5: fitted=SIR_solver(t, *popt)
149/6:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
147/7:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly

mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
147/8:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
147/9:
global_confirm_path = 'datasets/time_series_covid19_confirmed_global.csv'
usa_confirm_path ='datasets/time_series_covid19_confirmed_US.csv'
df_usa, state_lst = clean_confirm(usa_confirm_path, 'usa')
df_global, country_lst = clean_confirm(global_confirm_path, 'global')
147/10:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
147/11:
global_confirm_path = 'datasets/time_series_covid19_confirmed_global.csv'
usa_confirm_path ='datasets/time_series_covid19_confirmed_US.csv'
df_usa, state_lst = clean_confirm(usa_confirm_path, 'usa')
df_global, country_lst = clean_confirm(global_confirm_path, 'global')
147/12:
sub_global_lst = ['Italy', 'US', 'China', 'Korea, South', 'Iran']
sub_usa_lst = ['New York', 'California', 'Michigan', 'Texas', 'Florida']
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas', 'Florida']]
147/13:
sub_usa.to_csv('datasets/sub_usa.csv', sep='\t',index=False)
sub_global.to_csv('datasets/sub_global.csv', sep = '\t',index=False)
147/14:
plt.figure()
ax = sub_global.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
147/15:
plt.figure()
ax = sub_usa.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
147/16:
fig = go.Figure()
for ct in sub_usa_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='log')
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
147/17:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    
    html.Label('Multi-Select State'),
    
    dcc.Dropdown(
        id='state_drop_down',
        options=[
            {'label': 'New York', 'value': 'New York'},
            {'label': 'California', 'value': 'California'},
            {'label': 'Michigan', 'value': 'Michigan'},
            {'label': 'Texas', 'value': 'Texas'},
            {'label': 'Florida', 'value': 'v'}
        ],
        value=['New York', 'California'], # which are pre-selected
        multi=True
    ),   
        
    dcc.Graph(figure=fig, id='main_window_slope')
])
147/18:
from dash.dependencies import Input, Output

@app.callback(
    Output('main_window_slope','figure'),
    [Input('state_drop_down', 'value')]
)
def update_figure(sub_usa_lst):
    traces = []
    for each in sub_usa_lst:
        traces.append(dict(x=df_usa.date,
                           y=df_usa[each],
                           mode='markers+lines',
                           opacity=0.9,
                           line_width=2,
                           marker_size=4, 
                           name=each
                        )
                )
    return {
        'data': traces,
            'layout': dict (
                width=1280,
                height=720,
                xaxis_title="Time",
                yaxis_title="Confirmed infected people in USA (log)",
                xaxis={'tickangle':-45,
                        'nticks':20,
                        'tickfont':dict(size=14,color="#7f7f7f"),
                        
                      },
                yaxis={'type':"log",
                       'range':'[1.1,5.5]'
                      }
        )
    }
147/19:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly

mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
147/20:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
147/21:
global_confirm_path = 'datasets/time_series_covid19_confirmed_global.csv'
usa_confirm_path ='datasets/time_series_covid19_confirmed_US.csv'
df_usa, state_lst = clean_confirm(usa_confirm_path, 'usa')
df_global, country_lst = clean_confirm(global_confirm_path, 'global')
147/22:
sub_global_lst = ['Italy', 'US', 'China', 'Korea, South', 'Iran']
sub_usa_lst = ['New York', 'California', 'Michigan', 'Texas', 'Florida']
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas', 'Florida']]
147/23:
sub_usa.to_csv('datasets/sub_usa.csv', sep='\t',index=False)
sub_global.to_csv('datasets/sub_global.csv', sep = '\t',index=False)
147/24:
plt.figure()
ax = sub_global.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
147/25:
plt.figure()
ax = sub_usa.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
147/26:
fig = go.Figure()
for ct in sub_usa_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='log')
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
147/27:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    
    html.Label('Multi-Select State'),
    
    dcc.Dropdown(
        id='state_drop_down',
        options=[
            {'label': 'New York', 'value': 'New York'},
            {'label': 'California', 'value': 'California'},
            {'label': 'Michigan', 'value': 'Michigan'},
            {'label': 'Texas', 'value': 'Texas'},
            {'label': 'Florida', 'value': 'v'}
        ],
        value=['New York', 'California'], # which are pre-selected
        multi=True
    ),   
        
    dcc.Graph(figure=fig, id='main_window_slope')
])
147/28:
from dash.dependencies import Input, Output

@app.callback(
    Output('main_window_slope','figure'),
    [Input('state_drop_down', 'value')]
)
def update_figure(sub_usa_lst):
    traces = []
    for each in sub_usa_lst:
        traces.append(dict(x=df_usa.date,
                           y=df_usa[each],
                           mode='markers+lines',
                           opacity=0.9,
                           line_width=2,
                           marker_size=4, 
                           name=each
                        )
                )
    return {
        'data': traces,
            'layout': dict (
                width=1280,
                height=720,
                xaxis_title="Time",
                yaxis_title="Confirmed infected people in USA (log)",
                xaxis={'tickangle':-45,
                        'nticks':20,
                        'tickfont':dict(size=14,color="#7f7f7f"),
                        
                      },
                yaxis={'type':"log",
                       'range':'[1.1,5.5]'
                      }
        )
    }
147/29:
if __name__ == '__main__':
    app.run_server(host='127.0.0.1', debug=True, use_reloader=False)
150/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly

mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
150/2:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
150/3:
global_confirm_path = 'datasets/time_series_covid19_confirmed_global.csv'
usa_confirm_path ='datasets/time_series_covid19_confirmed_US.csv'
df_usa, state_lst = clean_confirm(usa_confirm_path, 'usa')
df_global, country_lst = clean_confirm(global_confirm_path, 'global')
150/4:
sub_global_lst = ['Italy', 'US', 'China', 'Korea, South', 'Iran']
sub_usa_lst = ['New York', 'California', 'Michigan', 'Texas', 'Florida']
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas', 'Florida']]
150/5:
sub_usa.to_csv('datasets/sub_usa.csv', sep='\t',index=False)
sub_global.to_csv('datasets/sub_global.csv', sep = '\t',index=False)
150/6:
plt.figure()
ax = sub_global.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
150/7:
plt.figure()
ax = sub_usa.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
150/8:
fig = go.Figure()
for ct in sub_usa_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='log')
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
150/9:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    
    html.Label('Multi-Select State'),
    
    dcc.Dropdown(
        id='state_drop_down',
        options=[
            {'label': 'New York', 'value': 'New York'},
            {'label': 'California', 'value': 'California'},
            {'label': 'Michigan', 'value': 'Michigan'},
            {'label': 'Texas', 'value': 'Texas'},
            {'label': 'Florida', 'value': 'v'}
        ],
        value=['New York', 'California'], # which are pre-selected
        multi=True
    ),   
        
    dcc.Graph(figure=fig, id='main_window_slope')
])
150/10:
from dash.dependencies import Input, Output

@app.callback(
    Output('main_window_slope','figure'),
    [Input('state_drop_down', 'value')]
)
def update_figure(sub_usa_lst):
    traces = []
    for each in sub_usa_lst:
        traces.append(dict(x=df_usa.date,
                           y=df_usa[each],
                           mode='markers+lines',
                           opacity=0.9,
                           line_width=2,
                           marker_size=4, 
                           name=each
                        )
                )
    return {
        'data': traces,
            'layout': dict (
                width=1280,
                height=720,
                xaxis_title="Time",
                yaxis_title="Confirmed infected people in USA (log)",
                xaxis={'tickangle':-45,
                        'nticks':20,
                        'tickfont':dict(size=14,color="#7f7f7f"),
                        
                      },
                yaxis={'type':"log",
                       'range':'[1.1,5.5]'
                      }
        )
    }
139/1:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
import matplotlib as mpl
%matplotlib inline
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
139/2:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
139/3:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
139/4:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')


df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])


# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
139/5:
def div_two(df):
    """
    input: rearranged melt table - index: date, column: death, confirm
    output: death rate
    """
    return df['Death'].sum() / df['Confirm'].sum()
139/6:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
139/7:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
import matplotlib as mpl
%matplotlib inline
sns.set(style='darkgrid')
139/8:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
139/9:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
139/10:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')


df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])


# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
139/11:
def div_two(df):
    """
    input: rearranged melt table - index: date, column: death, confirm
    output: death rate
    """
    return df['Death'].sum() / df['Confirm'].sum()
139/12:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
139/13:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
%matplotlib inline
139/14:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
139/15:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
139/16:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')


df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])


# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
139/17:
def div_two(df):
    """
    input: rearranged melt table - index: date, column: death, confirm
    output: death rate
    """
    return df['Death'].sum() / df['Confirm'].sum()
139/18:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
139/19: df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
139/20: df_total.groupby(['Date']).apply(div_two)
139/21: df_total.groupby(['Date'])
139/22: df_total.groupby(['Date']).sum()
139/23: df_total.groupby(['Date'])['Death'].sum()/df_total.groupby(['Date'])['Confirm'].sum()
139/24:
def div_two(df):
    """
    input: rearranged melt table - index: date, column: death, confirm
    output: death rate
    """
    return df['Death'].sum() / df['Confirm'].sum()
139/25: df_total.groupby(['Date']).apply(div_two)
139/26:
def div_two(df):
    """
    input: rearranged melt table - index: date, column: death, confirm
    output: death rate
    """
    return np.array(df['Death'].sum() / df['Confirm'].sum())
139/27: df_total.groupby(['Date']).apply(div_two)
139/28: np.array(df_total.groupby(['Date'])['Death'].sum()/df_total.groupby(['Date'])['Confirm'].sum())
139/29:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
ax2.plot(df_new.index.values, death_rate, c = 'black')
#df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
139/30: death_rate = np.array(df_total.groupby(['Date'])['Death'].sum()/df_total.groupby(['Date'])['Confirm'].sum())
139/31:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
ax2.plot(df_new.index.values, death_rate, c = 'black')
#df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
139/32:
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn import preprocessing
%matplotlib inline
mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
139/33:
df_confirm = pd.read_csv('datasets/time_series_covid_19_confirmed_US.csv')
df_death = pd.read_csv('datasets/time_series_covid_19_deaths_US.csv')
139/34:
# extract 'UID', 'Province State', 'Date' from df_confirm
date_start_idx_confirm = df_confirm.columns.get_loc('1/22/20')
dates_confirm = df_confirm.columns[date_start_idx_confirm:-1]
df_confirm_new = df_confirm[['UID', 'Province_State'] +list(dates_confirm)]

# extract 'UID', 'Province State', 'Date', 'Population' from df_death
date_start_idx_death = df_death.columns.get_loc('1/22/20')
dates_death = df_death.columns[date_start_idx_death:-1]
df_death_new = df_death[['UID', 'Province_State','Population'] +list(dates_death)]
139/35:
# rearrange the df_confirm 
df_confirm_new = df_confirm_new.melt(id_vars=['UID', 'Province_State'],
                   var_name = 'Date', value_name ='Confirm').set_index('UID')

# rearrange the df_death
df_death_new = df_death_new.melt(id_vars=['UID', 'Province_State','Population'],
                   var_name = 'Date', value_name ='Death').set_index('UID')


df_total = df_confirm_new.merge(df_death_new, how='left', on=['UID', 'Date','Province_State'])
# change the object str to datetime dtype
df_total['Date'] = pd.to_datetime(df_total['Date'])


# sum_table
df_new = pd.DataFrame(df_total.groupby(['Date'])['Confirm'].sum())
df_new = df_new.set_index(pd.to_datetime(df_new.index))
df_new['Confirm_diff'] = df_new['Confirm'].diff()
df_new['Death'] = df_total.groupby(['Date'])['Death'].sum()
df_new['Death_diff'] = df_new['Death'].diff()
139/36:
def div_two(df):
    """
    input: rearranged melt table - index: date, column: death, confirm
    output: death rate
    """
    return np.array(df['Death'].sum() / df['Confirm'].sum())
139/37: death_rate = np.array(df_total.groupby(['Date'])['Death'].sum()/df_total.groupby(['Date'])['Confirm'].sum())
139/38:
# Create figure and plot space
fig, (ax0, ax1, ax2) = plt.subplots(nrows = 3, figsize=(10, 10))
#df_total.groupby(['Date'])['Confirm'].sum().plot(c ='black', ax = ax0, label ='Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm'], c = 'black', label = 'Cumulative')
ax0.plot(df_new.index.values, df_new['Confirm_diff'], c = 'red', label ='Daily')
#df_total.groupby(['Date'])['Death'].sum().plot(c ='black', ax = ax1,label ='Cumulative')
ax1.plot(df_new.index.values, df_new['Death'], c = 'black', label = 'Cumulative')
ax1.plot(df_new.index.values, df_new['Death_diff'], c = 'red', label = 'Daily')
ax2.plot(df_new.index.values, death_rate, c = 'black')
#df_total.groupby(['Date']).apply(div_two).plot(c ='black', ax = ax2)
ax0.set(xlabel="Date",ylabel="Confirmed Cases")
ax0.legend()
ax1.set(xlabel="Date",ylabel="Death Cases")
ax1.legend()
ax2.set(xlabel="Date",ylabel ='Death/Confirmed Ratio (Death Rate)')
139/39:
fig, ax = plt.subplots(figsize = (8,5))
ax.scatter(df_new['Confirm'], df_total.groupby(['Date']).apply(div_two))
ax.set(xlabel ='Confirm', ylabel = 'Death Rate')
139/40:
# Date starting from March 1st
index_mask = pd.to_datetime(df_new.index.values) > '2020-03-01'
# plot figure
fig, ax0 = plt.subplots( figsize=(8, 5))
ax0.plot(df_new[index_mask]['Confirm_diff'], c = 'black', label ='Confirmed Cases')
ax0.plot(df_new[index_mask]['Death_diff'], c = 'red', label = 'Death Cases')
ax0.set_xlabel('Date')
ax0.set_ylabel('Daily Case')
ax0.legend()
139/41:
# sum table based on date and states
df_new_state = pd.DataFrame(df_total.groupby(['Date','Province_State'])['Confirm'].sum())
df_new_state['Death'] = df_total.groupby(['Date','Province_State'])['Death'].sum()
df_new_state['Population'] = df_total.groupby(['Date','Province_State'])['Population'].sum()
139/42:
# function that sum all info based on alliance states
def alliance_states(df_new_state, states):
    """
    input: 1. multiindex table{index_1: Date, index_2: states info}
                            {column_1:confirm,  column_2: death}
           2. states name
    output: dic, a dictionary which key is every state,
            and corrsponding value is the normalized confirmed case over time 
    """
    dic ={}
    for idx, state in enumerate(states):
        df_new_ = df_new_state.loc(axis=0)[pd.IndexSlice[:,[state]]]
        df_new_['Confirm_diff'] = df_new_['Confirm'].diff()
        min_max_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))
        df_new_scaled = min_max_scaler.fit_transform(df_new_)
        dic[state] = [df_new_.index.get_level_values('Date'), df_new_scaled[:,-1]]
    return dic
139/43:
# Normalized time course of daily confirmation cased on different states
state_dic ={"Pacific States": ["Washington", "Oregon", "California"],
           "NorthEast States_1": ["Connecticut", "Massachusetts", "New Jersey", "New York", "Pennsylvania"],
           "NorthEast States_2": ["District of Columbia", "Virginia", "Maryland"],
           "Lake States":["Indiana", "Michigan", "Ohio", "Illinois"],
           "Southern States": ["Georgia", "Florida", "Texas", "Louisiana"]}

n_rows = len(state_dic)
fig, ax = plt.subplots(nrows = len(state_dic), figsize = (10, 10))
i = 0
for key, states in state_dic.items():
    states_data = alliance_states(df_new_state, states)
    for state in states_data:
        ax[i].plot(states_data[state][0], states_data[state][1], label = state)
    ax[i].set_prop_cycle
    ax[i].legend()
    i += 1
139/44:
# Add confirm case based on population - normalized the data
df_new_state['ConfirmVSPopulation'] = df_new_state['Confirm']/df_new_state['Population']
# only observe when pandamic occurs
dff = df_new_state[df_new_state.index.get_level_values(0) >= '2020-03-20']
139/45:
# generate pivot table and heatmap
df_new_state_pivot = dff.pivot_table(index = "Date", columns = "Population", values = "ConfirmVSPopulation")
sns.heatmap(df_new_state_pivot,vmin=0, vmax=0.01)
151/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly

mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
151/2:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
151/3:
global_confirm_path = 'datasets/time_series_covid19_confirmed_global.csv'
usa_confirm_path ='datasets/time_series_covid19_confirmed_US.csv'
df_usa, state_lst = clean_confirm(usa_confirm_path, 'usa')
df_global, country_lst = clean_confirm(global_confirm_path, 'global')
151/4:
sub_global_lst = ['Italy', 'US', 'China', 'Korea, South', 'Iran']
sub_usa_lst = ['New York', 'California', 'Michigan', 'Texas', 'Florida']
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas', 'Florida']]
151/5:
sub_usa.to_csv('datasets/sub_usa.csv', sep='\t',index=False)
sub_global.to_csv('datasets/sub_global.csv', sep = '\t',index=False)
151/6:
plt.figure()
ax = sub_global.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
151/7:
plt.figure()
ax = sub_usa.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
151/8:
fig = go.Figure()
for ct in sub_usa_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='log')
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
154/1:
import numpy as np
import matplotlib.pyplot as plt
from scipy import integrate
from scipy import linalg
from utils import *
154/2:
n = 1

def MMKinetics(t, x):
    Vmax, Km, k = 1.5, 0.3, 0.6
    return k - np.divide(Vmax*x, (Km+x))

tspan = np.linspace(0.01, 4, num=400)
dt = 0.01
ini = [0.5]
sol = integrate.solve_ivp(MMKinetics, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)

plt.plot(sol.y[0])
plt.show()
154/3:
sol_dx = MMKinetics(sol.t, sol.y)
plt.show()
term_lib, term_des = lib_terms(sol.y, 6, "")
term_lib = np.hstack((term_lib, term_lib * sol
154/4:
sol_dx = MMKinetics(sol.t, sol.y)
plt.show()
term_lib, term_des = lib_terms(sol.y, 6, "")
term_lib = np.hstack((term_lib, term_lib * sol_dx.T))
154/5:
tol, pflag = 1e-5,1
dic_Xi, dic_lib, dic_lambda, dic_num, dic_error = ADMpareto(term_lib,tol)
lambda_vec = list(dic_lambda.values())
terms_vec = list(dic_num.values())
err_vec = list(dic_error.values())
log_err_vec = np.log(err_vec)
log_lambda_vec = np.log(lambda_vec)

# plot
plt.figure(figsize=(10,6))
plt.subplot(1,2,1)
plt.scatter(log_lambda_vec, terms_vec)
plt.xlabel("Threshold (log_$\lambda$)")
plt.ylabel("Number of terms")
plt.subplot(1,2,2)
plt.scatter(terms_vec, log_err_vec)
plt.xlabel("Number of terms")
plt.ylabel("Error (log)")
plt.show()
154/6:
tol, pflag = 1e-5,1
dic_Xi, dic_lib, dic_lambda, dic_num, dic_error = ADMpareto(term_lib,tol)
lambda_vec = list(dic_lambda.values())
terms_vec = list(dic_num.values())
err_vec = list(dic_error.values())
log_err_vec = np.log(err_vec)
log_lambda_vec = np.log(lambda_vec)

# plot
plt.figure(figsize=(10,6))
plt.subplot(1,2,1)
plt.scatter(log_lambda_vec, terms_vec)
plt.xlabel("Threshold (log_$\lambda$)")
plt.ylabel("Number of terms")
plt.subplot(1,2,2)
plt.scatter(terms_vec, log_err_vec)
plt.xlabel("Number of terms")
plt.ylabel("Error (log)")
plt.show()
154/7:
import numpy as np
import matplotlib.pyplot as plt
from scipy import integrate
from scipy import linalg
from utils import *
154/8:
n = 1

def MMKinetics(t, x):
    Vmax, Km, k = 1.5, 0.3, 0.6
    return k - np.divide(Vmax*x, (Km+x))

tspan = np.linspace(0.01, 4, num=400)
dt = 0.01
ini = [0.5]
sol = integrate.solve_ivp(MMKinetics, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)

plt.plot(sol.y[0])
plt.show()
154/9:
sol_dx = MMKinetics(sol.t, sol.y)
plt.show()
term_lib, term_des = lib_terms(sol.y, 6, "")
term_lib = np.hstack((term_lib, term_lib * sol_dx.T))
154/10:
tol, pflag = 1e-5,1
dic_Xi, dic_lib, dic_lambda, dic_num, dic_error = ADMpareto(term_lib,tol)
lambda_vec = list(dic_lambda.values())
terms_vec = list(dic_num.values())
err_vec = list(dic_error.values())
log_err_vec = np.log(err_vec)
log_lambda_vec = np.log(lambda_vec)

# plot
plt.figure(figsize=(10,6))
plt.subplot(1,2,1)
plt.scatter(log_lambda_vec, terms_vec)
plt.xlabel("Threshold (log_$\lambda$)")
plt.ylabel("Number of terms")
plt.subplot(1,2,2)
plt.scatter(terms_vec, log_err_vec)
plt.xlabel("Number of terms")
plt.ylabel("Error (log)")
plt.show()
155/1:
import numpy as np
import matplotlib.pyplot as plt
from scipy import integrate
from scipy import linalg
from utils import *
155/2:
n = 1

def MMKinetics(t, x):
    Vmax, Km, k = 1.5, 0.3, 0.6
    return k - np.divide(Vmax*x, (Km+x))

tspan = np.linspace(0.01, 4, num=400)
dt = 0.01
ini = [0.5]
sol = integrate.solve_ivp(MMKinetics, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)

plt.plot(sol.y[0])
plt.show()
155/3:
sol_dx = MMKinetics(sol.t, sol.y)
plt.show()
term_lib, term_des = lib_terms(sol.y, 6, "")
term_lib = np.hstack((term_lib, term_lib * sol_dx.T))
155/4:
tol, pflag = 1e-5,1
dic_Xi, dic_lib, dic_lambda, dic_num, dic_error = ADMpareto(term_lib,tol)
lambda_vec = list(dic_lambda.values())
terms_vec = list(dic_num.values())
err_vec = list(dic_error.values())
log_err_vec = np.log(err_vec)
log_lambda_vec = np.log(lambda_vec)

# plot
plt.figure(figsize=(10,6))
plt.subplot(1,2,1)
plt.scatter(log_lambda_vec, terms_vec)
plt.xlabel("Threshold (log_$\lambda$)")
plt.ylabel("Number of terms")
plt.subplot(1,2,2)
plt.scatter(terms_vec, log_err_vec)
plt.xlabel("Number of terms")
plt.ylabel("Error (log)")
plt.show()
155/5:
add noise to 'x'
eps = 1e-2  #float; magnitude of noise
SIZE = np.size(sol.y[0])
sol_noise=sol.y[0]+eps*np.random.normal(0,1,(SIZE))#np.size=400; shape=(1,400); type=numpy.ndarray
# #calculate the corresponding derivative
# sol_noise_dx = MMKinetics(sol.t, sol_noise)
155/6:
#add noise to 'x'
eps = 1e-2  #float; magnitude of noise
SIZE = np.size(sol.y[0])
sol_noise=sol.y[0]+eps*np.random.normal(0,1,(SIZE))#np.size=400; shape=(1,400); type=numpy.ndarray
# #calculate the corresponding derivative
# sol_noise_dx = MMKinetics(sol.t, sol_noise)
155/7:
#denoise
# create an order 3 lowpass butterworth filter
b, a = signal.butter(3, 0.05)
#first time
zi = signal.lfilter_zi(b, a)
z,_=signal.lfilter(b, a, sol_noise, zi=zi*sol_noise[0])
#second time
z2, _ = signal.lfilter(b, a, z, zi=zi*z[0])
#third time
y = signal.filtfilt(b, a, sol_noise)
155/8:
import numpy as np
import matplotlib.pyplot as plt
from scipy import integrate
from scipy import linalg
from utils import *
from scipy import signal
155/9:
#denoise
# create an order 3 lowpass butterworth filter
b, a = signal.butter(3, 0.05)
#first time
zi = signal.lfilter_zi(b, a)
z,_=signal.lfilter(b, a, sol_noise, zi=zi*sol_noise[0])
#second time
z2, _ = signal.lfilter(b, a, z, zi=zi*z[0])
#third time
y = signal.filtfilt(b, a, sol_noise)
155/10:
plot noisy and denoised data
plt.figure
plt.plot( sol_noise, 'b', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/11:
#plot noisy and denoised data
plt.figure
plt.plot( sol_noise, 'b', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/12:
#denoised y and corresponding denoised dy
denoised_y=np.reshape(y,(1,len(y)))
denoised_dy = MMKinetics(sol.t, denoised_y)

term_lib, term_des = lib_terms(denoised_y, 6, "")
term_lib = np.hstack((term_lib, term_lib * denoised_dy.T))


tol, pflag = 1e-5,1
dic_Xi, dic_lib, dic_lambda, dic_num, dic_error = ADMpareto(term_lib,tol)
lambda_vec = list(dic_lambda.values())
terms_vec = list(dic_num.values())
err_vec = list(dic_error.values())
log_err_vec = np.log(err_vec)
log_lambda_vec = np.log(lambda_vec)

# plot
plt.figure(figsize=(10,6))
plt.subplot(1,2,1)
plt.scatter(log_lambda_vec, terms_vec)
plt.xlabel("Threshold (log_$\lambda$)")
plt.ylabel("Number of terms")
plt.subplot(1,2,2)
plt.scatter(terms_vec, log_err_vec)
plt.xlabel("Number of terms")
plt.ylabel("Error (log)")
plt.show()
155/13:
print(lambda_vec)
print(terms_vec)
155/14: dic_Xi[23]
155/15:
plot identified data with denoised data
identified_y=
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k',,'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/16:
#plot identified data with denoised data
identified_y=
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k',,'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/17:
#plot identified data with denoised data
identified_y= newmm(t,x)
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k',,'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/18:
#plot identified data with denoised data
identified_y= newmm(t,x)
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k','b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/19:
##identified system
def newmm(t,x):
    k1, k2, k3, k4 = 0.129, -0.643, 0.215, 0.717
    return np.divide((k1 + k2*x), (k3 + k4*x))
155/20:
#plot identified data with denoised data
identified_y= newmm(t,x)
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k',,'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/21:
#plot identified data with denoised data
identified_y= newmm(t,y)
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k', identified_y,'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/22:
#plot identified data with denoised data
identified_y= newmm(tspan,y)
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k', identified_y,'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/23:
#plot identified data with denoised data
identified_y= -newmm(tspan,y)
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k', identified_y,'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/24:
#plot identified data with denoised data
identified_y= integrate.solve_ivp(newmm, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k', identified_y.y,'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/25:
#plot identified data with denoised data
identified_y= integrate.solve_ivp(newmm, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k', identified_y.y,'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()

plt.plot(identified_y.y,'b')
155/26: plt.plot(identified_y.y,'b')
155/27:
plt.plot(identified_y.y,'b')
plt.show()
155/28:
#plot identified data with denoised data
identified_y= integrate.solve_ivp(newmm, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k', identified_y.y,'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/29: identified_y.y
155/30: plt.plot(identified_y.t, entified_y.y)
155/31: plt.plot(identified_y.t, ientified_y.y)
155/32: plt.plot(identified_y.t, identified_y.y)
155/33: plt.plot(identified_y.t, identified_y.y)
155/34: identified_y.t
155/35: identified_y.y
155/36: plt.plot(identified_y.t, identified_y.y[0])
155/37:
#plot identified data with denoised data
identified_y= integrate.solve_ivp(newmm, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)
plt.figure
plt.plot( sol_noise, 'y', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k')
plt.plot(identified_y.t, identified_y.y[0], 'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/38:
#plot noisy and denoised data
plt.figure
plt.plot( sol.t, sol_noise, 'b', alpha=0.75)
plt.plot( z, 'r--',  z2, 'r',  y, 'k')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/39:
#plot noisy and denoised data
plt.figure
plt.plot( sol.t, sol_noise, 'b', alpha=0.75)
plt.plot( sol.t, z, 'r--',  z2, 'r',  y, 'k')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/40:
#plot noisy and denoised data
plt.figure
plt.plot( sol.t, sol_noise, 'b', alpha=0.75)
plt.plot( sol.t, z, 'r--', sol.t, z2, 'r', sol.t, y, 'k')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/41:
#plot identified data with denoised data
identified_y= integrate.solve_ivp(newmm, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)
plt.figure
plt.plot(sol.t, sol_noise, 'y', alpha=0.75)
plt.plot(sol.t, z, 'r--', sol.t, z2, sol.t, 'r', sol.t, y, 'k')
plt.plot(identified_y.t, identified_y.y[0], 'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/42:
#plot identified data with denoised data
identified_y= integrate.solve_ivp(newmm, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)
plt.figure
plt.plot(sol.t, sol_noise, 'y', alpha=0.75)
plt.plot(sol.t, z, 'r--',  z2, 'r',  y, 'k')
plt.plot(identified_y.t, identified_y.y[0], 'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/43:
#plot identified data with denoised data
identified_y= integrate.solve_ivp(newmm, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)
plt.figure
plt.plot(sol.t, sol_noise, 'y', alpha=0.75)
plt.plot(sol.t, z, 'r--', sol.t, z2, 'r',  y, 'k')
plt.plot(identified_y.t, identified_y.y[0], 'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
155/44:
#plot identified data with denoised data
identified_y= integrate.solve_ivp(newmm, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)
plt.figure
plt.plot(sol.t, sol_noise, 'y', alpha=0.75)
plt.plot(sol.t, z, 'r--', sol.t, z2, 'r', sol.t, y, 'k')
plt.plot(identified_y.t, identified_y.y[0], 'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
157/1:
import numpy as np
import matplotlib.pyplot as plt
from scipy import integrate
from scipy import linalg
from utils import *
from scipy import signal
157/2:
n = 1

def MMKinetics(t, x):
    Vmax, Km, k = 1.5, 0.3, 0.6
    return k - np.divide(Vmax*x, (Km+x))

tspan = np.linspace(0.01, 4, num=400)
dt = 0.01
ini = [0.5]
sol = integrate.solve_ivp(MMKinetics, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)

plt.plot(sol.y[0])
plt.show()
157/3:
#add noise to 'x'
eps = 1e-2  #float; magnitude of noise
SIZE = np.size(sol.y[0])
sol_noise=sol.y[0]+eps*np.random.normal(0,1,(SIZE))#np.size=400; shape=(1,400); type=numpy.ndarray
# #calculate the corresponding derivative
# sol_noise_dx = MMKinetics(sol.t, sol_noise)
158/1:
import numpy as np
import matplotlib.pyplot as plt
from scipy import integrate
from scipy import linalg
from utils import *
from scipy import signal
158/2:
# simulated datasete
n = 1

def MMKinetics(t, x):
    Vmax, Km, k = 1.5, 0.3, 0.6
    return k - np.divide(Vmax*x, (Km+x))

tspan = np.linspace(0.01, 4, num=400)
dt = 0.01
ini = [0.5]
sol = integrate.solve_ivp(MMKinetics, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)

plt.plot(sol.y[0])
plt.show()
158/3:
sol_dx = MMKinetics(sol.t, sol.y)
plt.show()
term_lib, term_des = lib_terms(sol.y, 6, "")
term_lib = np.hstack((term_lib, term_lib * sol_dx.T))
158/4:
tol, pflag = 1e-5,1
dic_Xi, dic_lib, dic_lambda, dic_num, dic_error = ADMpareto(term_lib,tol)
lambda_vec = list(dic_lambda.values())
terms_vec = list(dic_num.values())
err_vec = list(dic_error.values())
log_err_vec = np.log(err_vec)
log_lambda_vec = np.log(lambda_vec)

# plot
plt.figure(figsize=(10,6))
plt.subplot(1,2,1)
plt.scatter(log_lambda_vec, terms_vec)
plt.xlabel("Threshold (log_$\lambda$)")
plt.ylabel("Number of terms")
plt.subplot(1,2,2)
plt.scatter(terms_vec, log_err_vec)
plt.xlabel("Number of terms")
plt.ylabel("Error (log)")
plt.show()
158/5:
#add noise to 'x'
eps = 1e-2  #float; magnitude of noise
SIZE = np.size(sol.y[0])
sol_noise=sol.y[0]+eps*np.random.normal(0,1,(SIZE))#np.size=400; shape=(1,400); type=numpy.ndarray
# #calculate the corresponding derivative
# sol_noise_dx = MMKinetics(sol.t, sol_noise)
158/6:
#denoise
# create an order 3 lowpass butterworth filter
b, a = signal.butter(3, 0.05)
#first time
zi = signal.lfilter_zi(b, a)
z,_=signal.lfilter(b, a, sol_noise, zi=zi*sol_noise[0])
#second time
z2, _ = signal.lfilter(b, a, z, zi=zi*z[0])
#third time
y = signal.filtfilt(b, a, sol_noise)
158/7:
#plot noisy and denoised data
plt.figure
plt.plot( sol.t, sol_noise, 'b', alpha=0.75)
plt.plot( sol.t, z, 'r--', sol.t, z2, 'r', sol.t, y, 'k')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
158/8:
#denoised y and corresponding denoised dy
denoised_y=np.reshape(y,(1,len(y)))
denoised_dy = MMKinetics(sol.t, denoised_y)

term_lib, term_des = lib_terms(denoised_y, 6, "")
term_lib = np.hstack((term_lib, term_lib * denoised_dy.T))


tol, pflag = 1e-5,1
dic_Xi, dic_lib, dic_lambda, dic_num, dic_error = ADMpareto(term_lib,tol)
lambda_vec = list(dic_lambda.values())
terms_vec = list(dic_num.values())
err_vec = list(dic_error.values())
log_err_vec = np.log(err_vec)
log_lambda_vec = np.log(lambda_vec)

# plot
plt.figure(figsize=(10,6))
plt.subplot(1,2,1)
plt.scatter(log_lambda_vec, terms_vec)
plt.xlabel("Threshold (log_$\lambda$)")
plt.ylabel("Number of terms")
plt.subplot(1,2,2)
plt.scatter(terms_vec, log_err_vec)
plt.xlabel("Number of terms")
plt.ylabel("Error (log)")
plt.show()
158/9:
print(lambda_vec)
print(terms_vec)
158/10: dic_Xi[23]
158/11:
##identified system
def newmm(t,x):
    k1, k2, k3, k4 = 0.129, -0.643, 0.215, 0.717
    return np.divide((k1 + k2*x), (k3 + k4*x))
158/12:
#plot identified data with denoised data
identified_y= integrate.solve_ivp(newmm, [tspan[0], tspan[-1]], ini, method='RK45', t_eval=tspan)
plt.figure
plt.plot(sol.t, sol_noise, 'y', alpha=0.75)
plt.plot(sol.t, z, 'r--', sol.t, z2, 'r', sol.t, y, 'k')
plt.plot(identified_y.t, identified_y.y[0], 'b')
plt.legend(('noisy data', 'lfilter, once', 'lfilter, twice', 'denoised data'), loc='best')
plt.grid(True)
plt.show()
160/1:
t_initial=28
t_intro_measures=14
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/2:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/3:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize, integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
160/4:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
ydata = np.array(df_global.US[20:])
t=np.arange(len(ydata))
160/5:
N0=1000000 #max susceptible population, fixed values
beta=0.4   # infection rate
gamma=0.1  # recovery rate

# initial populations I0+S0+R0=N0
I0=df_global.US[20]
S0=N0-I0
R0=0

# SIR model v1
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])

# SIR model v2, same setting as v1
# SIR_t only for optimization 
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt

# helper function to calculate SIR
def SIR_solver(x, beta, gamma):
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
160/6:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
160/7: fitted=SIR_solver(t, *popt)
160/8:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
160/9:
t_initial=28
t_intro_measures=14
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/10:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/11:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/12:
N0=1000000 #max susceptible population, fixed values
beta=0.4   # infection rate
gamma=0.1  # recovery rate

# initial populations I0+S0+R0=N0
I0=df_global.US[40]
S0=N0-I0
R0=0

# SIR model v1
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])

# SIR model v2, same setting as v1
# SIR_t only for optimization 
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt

# helper function to calculate SIR
def SIR_solver(x, beta, gamma):
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
160/13:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
160/14: fitted=SIR_solver(t, *popt)
160/15:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
160/16:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
ydata = np.array(df_global.US[30:])
t=np.arange(len(ydata))
160/17:
N0=1000000 #max susceptible population, fixed values
beta=0.4   # infection rate
gamma=0.1  # recovery rate

# initial populations I0+S0+R0=N0
I0=df_global.US[30]
S0=N0-I0
R0=0

# SIR model v1
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])

# SIR model v2, same setting as v1
# SIR_t only for optimization 
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt

# helper function to calculate SIR
def SIR_solver(x, beta, gamma):
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
160/18:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
160/19: fitted=SIR_solver(t, *popt)
160/20:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
160/21:
t_initial=28
t_intro_measures=14
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/22:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/23:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/24:
t_initial=30
t_intro_measures=14
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/25:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/26:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/27:
t_initial=28
t_intro_measures=28
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.1
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/28:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/29:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/30:
t_initial=28
t_intro_measures=28
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/31:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/32:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/33:
t_initial=28
t_intro_measures=28
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/34:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/35:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/36:
t_initial=20
t_intro_measures=28
t_hold=21
t_relax=21

beta_max=0.4
beta_min=0.11
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/37:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/38:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/39:
t_initial=20
t_intro_measures=28
t_hold=21
t_relax=21

beta_max=0.3
beta_min=0.11
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/40:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/41:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/42:
t_initial=20
t_intro_measures=28
t_hold=21
t_relax=21

beta_max=0.3
beta_min=0.1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/43:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/44:
t_initial=20
t_intro_measures=14
t_hold=21
t_relax=21

beta_max=0.3
beta_min=0.1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/45:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/46:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/47:
t_initial=30
t_intro_measures=14
t_hold=21
t_relax=21

beta_max=0.3
beta_min=0.1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/48:
t_initial=30
t_intro_measures=14
t_hold=21
t_relax=21

beta_max=0.3
beta_min=0.1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/49:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/50:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/51:
t_initial=30
t_intro_measures=20
t_hold=21
t_relax=21

beta_max=0.3
beta_min=0.1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/52:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/53:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/54:
t_initial=30
t_intro_measures=20
t_hold=30
t_relax=21

beta_max=0.3
beta_min=0.1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/55:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/56:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/57: pd_beta
160/58:
t_initial=30
t_intro_measures=20
t_hold=30
t_relax=30

beta_max=0.3
beta_min=0.1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/59: pd_beta
160/60:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/61:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/62:
t_initial=30
t_intro_measures=30
t_hold=30
t_relax=30

beta_max=0.3
beta_min=0.1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/63: pd_beta
160/64:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/65:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/66:
t_initial=30
t_intro_measures=20
t_hold=30
t_relax=30

beta_max=0.3
beta_min=0.2
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/67: pd_beta
160/68:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/69:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/70:
t_initial=30
t_intro_measures=20
t_hold= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.2
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/71: pd_beta
160/72:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/73:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/74:
t_initial=30
t_intro_measures=20
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.2
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/75: pd_beta
160/76:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/77:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/78:
t_initial=35
t_intro_measures=20
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.2
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/79: pd_beta
160/80:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/81:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/82:
t_initial=35
t_intro_measures=20
t_hold= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.2
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/83: pd_beta
160/84:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/85:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/86:
t_initial=28
t_intro_measures=30
t_hold= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.2
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/87: pd_beta
160/88:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/89:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/90:
t_initial=28
t_intro_measures=40
t_hold= 20
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.2
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/91: pd_beta
160/92:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/93:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/94:
t_initial=28
t_intro_measures=40
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.2
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/95: pd_beta
160/96:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/97:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/98:
t_initial=28
t_intro_measures=30
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.2
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/99: pd_beta
160/100:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/101:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/102:
t_initial=28
t_intro_measures=30
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.3
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/103: pd_beta
160/104:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/105:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/106:
t_initial=28
t_intro_measures=30
t_hold= 20
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.3
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_max]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/107: pd_beta
160/108:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/109:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/110:
t_initial=28
t_intro_measures=30
t_hold= 20
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_max]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/111: pd_beta
160/112:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/113:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/114:
t_initial=28
t_intro_measures=30
t_hold= 20
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/115: pd_beta
160/116:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/117:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/118:
t_initial=28
t_intro_measures=30
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/119: pd_beta
160/120:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/121:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/122:
t_initial=28
t_intro_measures=30
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.6
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/123: pd_beta
160/124:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/125:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/126:
t_initial=28
t_intro_measures=30
t_hold= 20
t_relax=21

beta_max=0.3
beta_min=0.2
beta_mid = 0.6
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_min]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/127: pd_beta
160/128:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/129:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/130:
t_initial=28
t_intro_measures=50
t_hold= 20
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_min,beta_max,t_relax),
                       ))
160/131: pd_beta
160/132:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/133:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/134:
t_initial=28
t_intro_measures=28
t_hold= 20
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_mid,beta_min,t_relax),
                       ))
160/135: pd_beta
160/136:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/137:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/138:
t_initial=28
t_intro_measures=28
t_hold= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_mid,beta_min,t_relax),
                       ))
160/139: pd_beta
160/140:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/141:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/142:
t_initial=28
t_intro_measures=28
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_mid,beta_max,t_relax),
                       ))
160/143: pd_beta
160/144:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/145:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/146:
t_initial=28
t_intro_measures=28
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_max,beta_mid,t_relax),
                       ))
160/147: pd_beta
160/148:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/149:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/150:
t_initial=28
t_intro_measures=28
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_max2,beta_mid,t_relax),
                       ))
160/151: pd_beta
160/152:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/153:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/154:
t_initial=28
t_intro_measures=28
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                        np.linspace(beta_max2,beta_mid,t_relax),
                         np.array(t_hold*[beta_mid]),
                       ))
160/155: pd_beta
160/156:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/157:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/158:
t_initial=28
t_intro_measures=28
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_max,beta_mid,t_relax),
                       ))
160/159: pd_beta
160/160:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/161:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/162:
t_initial=28
t_intro_measures=38
t_hold= 40
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                        np.linspace(beta_max,beta_mid,t_relax),
                       ))
160/163: pd_beta
160/164:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/165:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
160/166:
t_initial=28
t_intro_measures=28
t_hold= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                       np.linspace(beta_max,beta_min,t_intro_measures),
                       np.array(t_hold*[beta_mid]),
                       ))
160/167: pd_beta
160/168:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
160/169:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
161/1:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize, integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
161/2:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
ydata = np.array(df_global.US[30:])
t=np.arange(len(ydata))
161/3:
N0=1000000 #max susceptible population, fixed values
beta=0.4   # infection rate
gamma=0.1  # recovery rate

# initial populations I0+S0+R0=N0
I0=df_global.US[30]
S0=N0-I0
R0=0

# SIR model v1
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])

# SIR model v2, same setting as v1
# SIR_t only for optimization 
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt

# helper function to calculate SIR
def SIR_solver(x, beta, gamma):
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
161/4:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
161/5: fitted=SIR_solver(t, *popt)
161/6:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
161/7:
t_initial=28
t_intro_measures=28
t_hold= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_mid = 0.5
beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_hold*[beta_mid]),
                        np.linspace(beta_max,beta_mid,t_relax),
                       ))
161/8:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
161/9:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
161/10:
t_initial=28
t_intro_measures=28
t_hold= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_hold*[beta_rebound]),
                        np.linspace(beta_max,beta_mid,t_relax),
                       ))
161/11:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
161/12:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
161/13:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_rebound*[beta_rebound]),
                        np.linspace(t_hold * [beta_min]),
                       ))
161/14:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
161/15:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='relax measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='repead hard measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
161/16:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
161/17:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_rebound*[beta_rebound]),
                        np.linspace(t_hold *[beta_min])
                       ))
161/18:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
161/19:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
161/20:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_rebound*[beta_rebound]),
                        np.array(t_hold*[beta_min]),
                       ))
161/21:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
161/22:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
161/23:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize, integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
162/1:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize, integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
162/2:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
ydata = np.array(df_global.US[30:])
t=np.arange(len(ydata))
162/3:
N0=1000000 #max susceptible population, fixed values
beta=0.4   # infection rate
gamma=0.1  # recovery rate

# initial populations I0+S0+R0=N0
I0=df_global.US[30]
S0=N0-I0
R0=0

# SIR model v1
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])

# SIR model v2, same setting as v1
# SIR_t only for optimization 
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt

# helper function to calculate SIR
def SIR_solver(x, beta, gamma):
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
162/4:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
162/5: fitted=SIR_solver(t, *popt)
162/6:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
162/7:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_rebound*[beta_rebound]),
                        np.array(t_hold*[beta_min]),
                       ))
162/8:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_relax=21

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_rebound*[beta_rebound]),
                        np.array(t_relax*[beta_min]),
                       ))
162/9:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
162/10:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_hold,t_relax]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/11:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/12:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_hold=21

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_rebound*[beta_rebound]),
                        np.array(t_hold*[beta_min]),
                       ))
162/13:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
162/14:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/15:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_hold=21

beta_max=0.3
beta_min=0.1
beta_rebound= 0.3
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_rebound*[beta_rebound]),
                        np.array(t_hold*[beta_min]),
                       ))
162/16:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
162/17:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/18:
t_initial=40
t_intro_measures=28
t_rebound= 30
t_hold=21

beta_max=0.3
beta_min=0.1
beta_rebound= 0.3
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_rebound*[beta_rebound]),
                        np.array(t_hold*[beta_min]),
                       ))
162/19:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
162/20:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/21:
t_initial=30
t_intro_measures=28
t_rebound= 30
t_hold=21

beta_max=0.3
beta_min=0.1
beta_rebound= 0.3
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_rebound*[beta_rebound]),
                        np.array(t_hold*[beta_min]),
                       ))
162/22:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
162/23:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/24:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_hold=21

beta_max=0.3
beta_min=0.1
beta_rebound= 0.3
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_rebound*[beta_rebound]),
                        np.array(t_hold*[beta_min]),
                       ))
162/25:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
162/26:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/27:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_hold=10

beta_max=0.3
beta_min=0.1
beta_rebound= 0.3
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_hold*[beta_min]),
                        np.array(t_rebound*[beta_rebound]),
                       ))
162/28:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
162/29:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/30:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_hold=5

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_hold*[beta_min]),
                        np.array(t_rebound*[beta_rebound]),
                       ))
162/31:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
162/32:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/33:
N0=2000000 #max susceptible population, fixed values
beta=0.4   # infection rate
gamma=0.1  # recovery rate

# initial populations I0+S0+R0=N0
I0=df_global.US[30]
S0=N0-I0
R0=0

# SIR model v1
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])

# SIR model v2, same setting as v1
# SIR_t only for optimization 
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt

# helper function to calculate SIR
def SIR_solver(x, beta, gamma):
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
162/34:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
162/35: fitted=SIR_solver(t, *popt)
162/36:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
162/37:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_hold=5

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_hold*[beta_min]),
                        np.array(t_rebound*[beta_rebound]),
                       ))
162/38:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
162/39:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/40:
N0=1000000 #max susceptible population, fixed values
beta=0.4   # infection rate
gamma=0.1  # recovery rate

# initial populations I0+S0+R0=N0
I0=df_global.US[30]
S0=N0-I0
R0=0

# SIR model v1
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])

# SIR model v2, same setting as v1
# SIR_t only for optimization 
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt

# helper function to calculate SIR
def SIR_solver(x, beta, gamma):
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
162/41:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
162/42: fitted=SIR_solver(t, *popt)
162/43:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
162/44:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_hold=5

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_hold*[beta_min]),
                        np.array(t_rebound*[beta_rebound]),
                       ))
162/45:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
162/46:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='hold measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/47:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize, integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
162/48:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
ydata = np.array(df_global.US[30:])
t=np.arange(len(ydata))
162/49:
N0=1000000 #max susceptible population, fixed values
beta=0.4   # infection rate
gamma=0.1  # recovery rate

# initial populations I0+S0+R0=N0
I0=df_global.US[30]
S0=N0-I0
R0=0

# SIR model v1
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])

# SIR model v2, same setting as v1
# SIR_t only for optimization 
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt

# helper function to calculate SIR
def SIR_solver(x, beta, gamma):
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
162/50:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
162/51: fitted=SIR_solver(t, *popt)
162/52:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
162/53:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_hold=5

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_hold*[beta_min]),
                        np.array(t_rebound*[beta_rebound]),
                       ))
162/54:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
162/55:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='rebound measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Szenario SIR simulations  (demonstration purposes only)',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
162/56:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='rebound measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Demo Simulation',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
163/1:
import numpy as np
from datetime import datetime
import pandas as pd 
from scipy import optimize, integrate

import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline
sns.set(style="darkgrid")
mpl.rcParams['figure.figsize'] = (16, 9)
163/2:
df_global=pd.read_csv('datasets/sub_global.csv',sep='\t')  
df_global.sort_values('date',ascending=True).head()
ydata = np.array(df_global.US[30:])
t=np.arange(len(ydata))
163/3:
N0=1000000 #max susceptible population, fixed values
beta=0.4   # infection rate
gamma=0.1  # recovery rate

# initial populations I0+S0+R0=N0
I0=df_global.US[30]
S0=N0-I0
R0=0

# SIR model v1
def SIR_model(SIR,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    
    S,I,R=SIR
    dSdt= -beta*S*I/N0 
    dIdt= beta*S*I/N0-gamma*I
    dRdt= gamma*I
    return([dSdt,dIdt,dRdt])

# SIR model v2, same setting as v1
# SIR_t only for optimization 
def SIR_model_t(SIR,t,beta,gamma):
    ''' Simple SIR model
        S: susceptible population
        I: infected population
        R: recovered population
        S+I+R= N (fix limited poplation siz)
        params: beta infection rate
        params: gamma recovery rate
    '''
    S,I,R=SIR
    dSdt=-beta*S*I/N0
    dIdt=beta*S*I/N0-gamma*I
    dRdt=gamma*I
    return dSdt,dIdt,dRdt

# helper function to calculate SIR
def SIR_solver(x, beta, gamma):
    return integrate.odeint(SIR_model_t, (S0, I0, R0), t, args=(beta, gamma))[:,1]
163/4:
"""
optimize.curve_fit
return: popt optimal values for the parameters
return: The estimated covariance of popt.
To compute one standard deviation errors on the parameters use perr = np.sqrt(np.diag(pcov))
"""
popt, pcov = optimize.curve_fit(SIR_solver, t, ydata)
perr = np.sqrt(np.diag(pcov))

print('standard deviation errors : ',str(perr), ' start infect:',ydata[0])
print("Optimal parameters: beta =", popt[0], " and gamma = ", popt[1])
163/5: fitted=SIR_solver(t, *popt)
163/6:
plt.semilogy(t, ydata, 'o', label = 'true')
plt.semilogy(t, fitted, label ='simulated')
plt.title("SIR MODEL based on the data from confirmation cases in USA", fontsize=28)
plt.ylabel("Population infected", fontsize=24)
plt.xlabel("Days", fontsize=24)
plt.legend(fontsize=20)
plt.show()
print("Optimal parameters: beta =", round(popt[0],2), " and gamma = ", round(popt[1],2))
163/7:
t_initial=28
t_intro_measures=28
t_rebound= 30
t_hold=5

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_initial*[beta_max]),
                        np.linspace(beta_max,beta_min,t_intro_measures),
                        np.array(t_hold*[beta_min]),
                        np.array(t_rebound*[beta_rebound]),
                       ))
163/8:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
163/9:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='no measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hard measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='rebound measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Demo Simulation',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
163/10:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='hard measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='hold measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='rebound measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='unknown measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Demo Simulation',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
163/11:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_initial,t_intro_measures,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='hard measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='intro measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='rebound measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Demo Simulation',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
163/12:
t_hard=28
t_relax=28
t_rebound= 30
t_hold=5

beta_max=0.3
beta_min=0.1
beta_rebound= 0.5
# beta_max2 = 1
gamma=0.01
pd_beta=np.concatenate((np.array(t_hard*[beta_max]),
                        np.linspace(beta_max,beta_min,t_relax),
                        np.array(t_hold*[beta_min]),
                        np.array(t_rebound*[beta_rebound]),
                       ))
163/13:
SIR=np.array([S0,I0,R0])
propagation_rates=pd.DataFrame(columns={'susceptible':S0,
                                        'infected':I0,
                                        'recoverd':R0})



for each_beta in pd_beta:
   
    new_delta_vec=SIR_model(SIR,each_beta,gamma)
   
    SIR=SIR+new_delta_vec
    
    propagation_rates=propagation_rates.append({'susceptible':SIR[0],
                                                'infected':SIR[1],
                                                'recovered':SIR[2]}, ignore_index=True)
163/14:
fig, ax1 = plt.subplots(1, 1)

ax1.plot(propagation_rates.index,propagation_rates.infected,label='infected',linewidth=3)

t_phases=np.array([t_hard,t_relax,t_rebound,t_hold]).cumsum()
ax1.bar(np.arange(len(ydata)),ydata, width=0.8,label=' current infected Germany',color='r')
ax1.axvspan(0,t_phases[0], facecolor='b', alpha=0.2,label='hard measures')
ax1.axvspan(t_phases[0],t_phases[1], facecolor='b', alpha=0.3,label='relax measures introduced')
ax1.axvspan(t_phases[1],t_phases[2], facecolor='b', alpha=0.4,label='hold measures')
ax1.axvspan(t_phases[2],t_phases[3], facecolor='b', alpha=0.5,label='rebound measures')
ax1.axvspan(t_phases[3],len(propagation_rates.infected), facecolor='b', alpha=0.6,label='relax measures')

ax1.set_ylim(10, 1.5*max(propagation_rates.infected))
ax1.set_yscale('log')
ax1.set_title('Demo Simulation',size=16)
ax1.set_xlabel('time in days',size=16)
ax1.legend(loc='best',
           prop={'size': 16});
164/1:
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
import matplotlib as mpl
%matplotlib inline
import seaborn as sns
import plotly.graph_objs as go
import plotly

mpl.rcParams['figure.figsize'] = (16, 9)
sns.set(style='darkgrid')
164/2:
def clean_confirm(data_path, settp):
    """
    clean confirmed data
    :param data_path: raw data
    :param settp: 'usa_confirm' or 'global_confirm'
    :return df_new: a summarized dataframe
    :return loc_lst: unique location list
    """
    df_raw = pd.read_csv(data_path)
    if settp == 'usa':
        pos, str_ = 11, 'Province_State'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    if settp == 'global':
        pos, str_ = 4, 'Country/Region'
        time_idx = df_raw.columns[pos:]
        loc_lst = df_raw[str_].unique()

    df_new = pd.DataFrame({'date': time_idx})
    df_new['date'] = df_new.date.astype('datetime64[ns]')

    for loc in loc_lst:
        df_new[loc] = np.array(
            df_raw[df_raw[str_] == loc].iloc[:, pos:].sum(axis=0)
        )

    return df_new, loc_lst
164/3:
global_confirm_path = 'datasets/time_series_covid19_confirmed_global.csv'
usa_confirm_path ='datasets/time_series_covid19_confirmed_US.csv'
df_usa, state_lst = clean_confirm(usa_confirm_path, 'usa')
df_global, country_lst = clean_confirm(global_confirm_path, 'global')
164/4:
sub_global_lst = ['Italy', 'US', 'China', 'Korea, South', 'Iran']
sub_usa_lst = ['New York', 'California', 'Michigan', 'Texas', 'Florida']
sub_global = df_global[['date', 'Italy', 'US', 'China', 'Korea, South', 'Iran']]
sub_usa = df_usa[['date', 'New York', 'California', 'Michigan', 'Texas', 'Florida']]
164/5:
sub_usa.to_csv('datasets/sub_usa.csv', sep='\t',index=False)
sub_global.to_csv('datasets/sub_global.csv', sep = '\t',index=False)
164/6:
plt.figure()
ax = sub_global.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
164/7:
plt.figure()
ax = sub_usa.iloc[:,:].set_index('date').plot()
ax.set_yscale('log')
164/8:
fig = go.Figure()
for ct in sub_usa_lst:  
    fig.add_trace(go.Scatter(x = df_usa.date, 
                             y = df_usa[ct], 
                             mode ='markers+lines',
                             opacity = 0.9, 
                             line_width = 2, 
                             marker_size = 4,
                             name = ct))

fig.update_layout(width= 800, 
                 height= 600,
                 xaxis_title = 'Time',
                 yaxis_title = 'Confirmed infected people in usa (log)')
fig.update_yaxes(type='log')
fig.update_layout(xaxis_rangeslider_visible = True)
fig.show()
164/9:
import dash 
import dash_core_components as dcc
import dash_html_components as html

app = dash.Dash()
app.layout = html.Div([
    
    html.Label('Multi-Select State'),
    
    dcc.Dropdown(
        id='state_drop_down',
        options=[
            {'label': 'New York', 'value': 'New York'},
            {'label': 'California', 'value': 'California'},
            {'label': 'Michigan', 'value': 'Michigan'},
            {'label': 'Texas', 'value': 'Texas'},
            {'label': 'Florida', 'value': 'v'}
        ],
        value=['New York', 'California'], # which are pre-selected
        multi=True
    ),   
        
    dcc.Graph(figure=fig, id='main_window_slope')
])
164/10:
from dash.dependencies import Input, Output

@app.callback(
    Output('main_window_slope','figure'),
    [Input('state_drop_down', 'value')]
)
def update_figure(sub_usa_lst):
    traces = []
    for each in sub_usa_lst:
        traces.append(dict(x=df_usa.date,
                           y=df_usa[each],
                           mode='markers+lines',
                           opacity=0.9,
                           line_width=2,
                           marker_size=4, 
                           name=each
                        )
                )
    return {
        'data': traces,
            'layout': dict (
                width=1280,
                height=720,
                xaxis_title="Time",
                yaxis_title="Confirmed infected people in USA (log)",
                xaxis={'tickangle':-45,
                        'nticks':20,
                        'tickfont':dict(size=14,color="#7f7f7f"),
                        
                      },
                yaxis={'type':"log",
                       'range':'[1.1,5.5]'
                      }
        )
    }
164/11:
if __name__ == '__main__':
    app.run_server(host='127.0.0.1', debug=True, use_reloader=False)
165/1: import psycopg2 as pg2
166/1:
class Solution:

    def findKthLargest(self, nums, k):
        self.quicksort(nums, 0, len(nums) -1, len(nums)-k)
        return nums[-k]
    
    def quicksort(self, nums, l, r, K):
        if l < r:
            p = self.partition(nums, l, r)
            if p == K:
                return 
            elif p < K:
                self.quicksort(nums, p + 1, r, K)
            else:
                self.quicksort(nums, l, p - 1, K)
                
    def partition(self, nums, l, r):
        pivot = nums[r]
        i = l - 1
        
        for j in range(l, r):
            if nums[j] <= pivot:
                i += 1
                self.swap(nums, i, j)
        self.swap(nums, i + 1, r)
        return i + 1
    
    def swap(self, nums, st, ed):
        nums[st], nums[ed] = nums[ed], nums[st]
        return


sol = Solution()
sol.findKthLargest([3,4,2,5,1,6],2)
166/2:
class Solution:

    def findKthLargest(self, nums, k):
        self.quicksort(nums, 0, len(nums) -1, len(nums)-k)
        return nums[-k]
    
    def quicksort(self, nums, l, r, K):
        if l < r:
            p = self.partition(nums, l, r)
            if p == K:
                return 
            elif p < K:
                self.quicksort(nums, p + 1, r, K)
            else:
                self.quicksort(nums, l, p - 1, K)
                
    def partition(self, nums, l, r):
        pivot = nums[r]
        i = l - 1
        
        for j in range(l, r):
            if nums[j] <= pivot:
                i += 1
                self.swap(nums, i, j)
        self.swap(nums, i + 1, r)
        return i + 1
    
    def swap(self, nums, st, ed):
        nums[st], nums[ed] = nums[ed], nums[st]
        return


sol = Solution()
sol.findKthLargest([3,4,2,5,1,6],2)
168/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
% matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
168/2: portfolio.head()
168/3:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
168/4: portfolio.head()
168/5:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
168/6: profile.head()
168/7: sns.distplot(profile['age'], kde=False)
168/8: pd.DataFrame(profile.groupby(by='age', as_index=False)['id'].count()).sort_values(by = 'age', ascending=False).head()
168/9:
print(profile.isnull().sum())
print('/')
print(sum(profile['age'] ==118))
168/10:
profile['age'].replace(118, np.nan,inplace=True)
profile.head()
168/11:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile)
# mice_imputer = IterativeImputer()
# profile_mice = profile.copy(deep=True)
# profile_mice.iloc[:,:] = mice_imputer.fit_transform(profile)
168/12:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile)
168/13:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile['age'])
168/14:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age','income']])
168/15:
!pip install fancyimpute
from fancyimpute import IterativeImputer
mice_imputer = IterativeImputer()
profile_mice = profile.copy(deep=True)
profile_mice.iloc[:,:] = mice_imputer.fit_transform(profile)
168/16:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age','income']])
print(imp.transform(profile[['age','income']]))
168/17:
from fancyimpute import IterativeImputer
mice_imputer = IterativeImputer()
profile_mice = profile.copy(deep=True)
profile_mice.iloc[:,:] = mice_imputer.fit_transform(profile[['age','income']])
168/18:

from fancyimpute import IterativeImputer
mice_imputer = IterativeImputer()
profile_mice = profile.copy(deep=True)
profile_mice.iloc[:,:] = mice_imputer.fit_transform(profile[['age','income']])
168/19:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age','income']])
print(imp.transform(profile[['age','income']]))
168/20:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age','income']])
print(round(imp.transform(profile[['age','income']])))
168/21:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age','income']])
print(imp.transform(profile[['age','income']]))
168/22:
profile['age'].replace(118, np.nan,inplace=True)
profile.head()
168/23:
profile['age'].replace(118, np.nan,inplace=True)
profile.head()
168/24:
profile['age'].replace(118, np.nan,inplace=True)
profile.head()
168/25:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age','income']])
print(imp.transform(profile[['age','income']]))
168/26:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age','income']])
print(imp.transform(profile[['age','income']]))
168/27:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age','income']])
print(imp.transform(profile[['age','income']]))
168/28: pd.DataFrame(profile.groupby(by='age', as_index=False)['id'].count()).sort_values(by = 'age', ascending=False).head()
168/29:
print(profile.isnull().sum())
print('/')
print(sum(profile['age'] ==118))
168/30:
profile['age'].replace(118, np.nan,inplace=True)
profile['gender'].get_dummies()
168/31:
profile['age'].replace(118, np.nan,inplace=True)
pd.get_dummies(profile['gender'])
168/32: sns.distplot(profile['age'], kde=False)
168/33:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
168/34:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
168/35: portfolio.head()
169/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
169/2:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
169/3: portfolio.head()
169/4:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
169/5:
portfolio_new = clean_portfolio()
portfolio_new.head()
169/6: profile.head()
169/7: sns.distplot(profile['age'], kde=False)
169/8: pd.DataFrame(profile.groupby(by='age', as_index=False)['id'].count()).sort_values(by = 'age', ascending=False).head()
169/9:
print(profile.isnull().sum())
print('/')
print(sum(profile['age'] ==118))
169/10:
from sklearn.impute import KNNImputer
X = [[1, 2, np.nan], [1, 2, 1], [np.nan, 1, 1], [2, 1, np.nan]]
imputer = KNNImputer(n_neighbors=2)
print(imputer.fit_transform(X))
169/11:
from sklearn.impute import KNNImputer
X = [[1, 0, np.nan], [1, 0, 1], [np.nan, 1, 1], [0, 1, np.nan]]
imputer = KNNImputer(n_neighbors=2)
print(imputer.fit_transform(X))
169/12:
from sklearn.impute import KNNImputer
X = [[1, 0], [ 0, 1], [np.nan, 1], [1, np.nan]]
imputer = KNNImputer(n_neighbors=2)
print(imputer.fit_transform(X))
169/13:
from sklearn.impute import KNNImputer
X = [[1, 0], [ 0, 1], [np.nan, 1], [1, np.nan]]
imputer = KNNImputer(n_neighbors=3)
print(imputer.fit_transform(X))
169/14:
from sklearn.impute import KNNImputer
X = [[1, 0], [ 0, 1], [np.nan, 1], [1, np.nan]]
imputer = KNNImputer(n_neighbors=2)
print(imputer.fit_transform(X))
169/15: pd.get_dummies(profile['age'])
169/16: pd.get_dummies(profile['age'], dummy_na=True)
169/17: pd.get_dummies(profile['gender'], dummy_na=True)
169/18: pd.get_dummies(profile['gender'])
169/19:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age','income']])
print(pd.Series(imp.transform(profile[['age','income']])))
169/20:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age','income']])
print(imp.transform(profile[['age','income']]))
169/21:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
print(imp.transform(profile[['age']))
169/22:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
print(imp.transform(profile['age']))
169/23:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile['age'])
print(imp.transform(profile['age']))
169/24:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile['age'])
# print(imp.transform(profile['age']))
169/25:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
# print(imp.transform(profile['age']))
169/26:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
print(imp.transform(profile[['age']]))
169/27:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
print(imp.transform(profile[['age']][0]))
169/28:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
print(imp.transform(profile[['age']]))
169/29:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
print(pd.DataFrame(imp.transform(profile[['age']])))
169/30:
from sklearn.impute import SimpleImputer
profile['age'].replace(118, np.nan,inplace=True)
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
print(pd.DataFrame(imp.transform(profile[['age']])))
169/31:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
print(pd.DataFrame(imp.transform(profile[['age']])))
169/32:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile['age'])
print(pd.DataFrame(imp.transform(profile['age'])))
169/33:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile['age']))
pd.cut(age, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/34:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile['age']))
pd.cut(age.vaules, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/35:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/36:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
print(age)
pd.cut(age, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/37:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/38:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age.vaules, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/39:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age.vaule, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/40:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(np.array(age) , bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/41:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age.vaule() , bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/42:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age.vaules() , bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/43:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age.value , bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/44:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age.values , bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/45:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age.value(), bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/46:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(pd.Series(age), bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/47:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age.to_numpy(), bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/48: age.to_numpy()
169/49: age.to_numpy()[0]
169/50: list(age.to_numpy())
169/51: age.to_list()
169/52: age.tolist()
169/53: age..values.tolist()
169/54: age.values.tolist()
169/55: first = [x[0] for x in zip(*age)]
169/56: first = [x[0] for x in zip(*age.to_numpy())]
169/57:
first = [x[0] for x in zip(*age.to_numpy())]
print(first)
169/58:
first = [x[0] for x in zip(*age.to_numpy())]
print(first)
169/59:
first = [x for x in zip(*age.to_numpy())]
print(first)
169/60:
first = [x for x in zip(*age.to_numpy())]
len(first)
169/61: first = [x[0] for x in age]
169/62: first = [x[0] for x in age.to_numpy()]
169/63:
first = [x[0] for x in age.to_numpy()]
print(first)
169/64:
first = age.to_numpy()[:,0]
print(first)
169/65:
first = age.to_numpy()[:,0]
print(len(first))
169/66:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s', '100s'])
169/67:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
169/68:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
age_cat = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
age_cat = pd.get_dummies(age_cat)
169/69:
from sklearn.impute import SimpleImputer
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp.fit(profile[['age']])
age = pd.DataFrame(imp.transform(profile[['age']]))
age_cat = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
age_cat = pd.get_dummies(age_cat)
age_cat
169/70: pd.to_datetime(clean_profile['became_member_on'],format='%Y%m%d')
169/71: pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
169/72:
date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
date.year
169/73: date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d').dt
169/74:
date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d').dt
date.year
169/75:
date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
date.dt.year
169/76: profile.info()
169/77: profile.describe()
169/78: profile['became_member_on'].min
169/79: profile['became_member_on'].min()
169/80: profile['became_member_on'].min(), profile['became_member_on'].max()
169/81: print(profile['became_member_on'].min(), profile['became_member_on'].max())
169/82:
member_date = pd.to_datetime(clean_profile['became_member_on'],format='%Y%m%d')
member_year = pd.get_dummies(member_date.dt.year)
member_month = pd.get_dummies(member_date.dt.month)
169/83:
member_date = pd.to_datetime(clean_profile['become_member_on'],format='%Y%m%d')
member_year = pd.get_dummies(member_date.dt.year)
member_month = pd.get_dummies(member_date.dt.month)
169/84:
member_date = pd.to_datetime(clean_profile['became_member_on'],format='%Y%m%d')
member_year = pd.get_dummies(member_date.dt.year)
member_month = pd.get_dummies(member_date.dt.month)
169/85:
member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
member_year = pd.get_dummies(member_date.dt.year)
member_month = pd.get_dummies(member_date.dt.month)
169/86:
member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
member_year = pd.get_dummies(member_date.dt.year)
member_month = pd.get_dummies(member_date.dt.month)
member_year
169/87:
profile_new =clean_profile()
profile_new.head()
169/88:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer, KNNImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        impleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]))

    # get dummies to age
    age_bins = pd.cut(age, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year)
    member_month = pd.get_dummies(member_date.dt.month)


    age,id,became_member_on,income]

    profile_new = pd.concat([gender,profile['id'],member_year,member_month, income])
    

    return profile_new
169/89:
profile_new =clean_profile()
profile_new.head()
169/90:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer, KNNImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        impleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]))

    # get dummies to age
    age_bins = pd.cut(age, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year)
    member_month = pd.get_dummies(member_date.dt.month)


    age,id,became_member_on,income]

    profile_new = pd.concat([gender,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
169/91:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer, KNNImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        impleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]))

    # get dummies to age
    age_bins = pd.cut(age, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year)
    member_month = pd.get_dummies(member_date.dt.month)


    age,id,became_member_on,income]

    profile_new = pd.concat([gender,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
169/92:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer, KNNImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        impleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]))

    # get dummies to age
    age_bins = pd.cut(age, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year)
    member_month = pd.get_dummies(member_date.dt.month)


    profile_new = pd.concat([gender,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
169/93:
profile_new =clean_profile()
profile_new.head()
169/94:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), impleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]))

    # get dummies to age
    age_bins = pd.cut(age, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year)
    member_month = pd.get_dummies(member_date.dt.month)


    profile_new = pd.concat([gender,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
169/95:
profile_new =clean_profile()
profile_new.head()
169/96:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        SimpleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]))

    # get dummies to age
    age_bins = pd.cut(age, bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year)
    member_month = pd.get_dummies(member_date.dt.month)


    profile_new = pd.concat([gender,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
169/97:
profile_new =clean_profile()
profile_new.head()
169/98:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        SimpleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]))

    # get dummies to age
    age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year)
    member_month = pd.get_dummies(member_date.dt.month)


    profile_new = pd.concat([gender,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
169/99:
profile_new =clean_profile()
profile_new.head()
169/100:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        SimpleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']])).columns({'income'})

    # get dummies to age
    age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year)
    member_month = pd.get_dummies(member_date.dt.month)


    profile_new = pd.concat([gender,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
169/101:
profile_new =clean_profile()
profile_new.head()
169/102:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        SimpleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year)
    member_month = pd.get_dummies(member_date.dt.month)


    profile_new = pd.concat([gender,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
169/103:
profile_new =clean_profile()
profile_new.head()
169/104:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        SimpleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
169/105:
profile_new =clean_profile()
profile_new.head()
169/106: sns.barplot(profile_new.loc[:,year_2013:year_2018])
169/107: sns.barplot(profile_new.loc[:,'year_2013':'year_2018'])
169/108: profile_new.loc[:,'year_2013':'year_2018']
169/109: sns.barplot(profile_new.loc[:,'year_2013':'year_2018'].sum())
169/110: profile_new.loc[:,'year_2013':'year_2018'].sum()
169/111: profile_new.loc[:,'year_2013':'year_2018'].sum()
169/112: bar(profile_new.loc[:,'year_2013':'year_2018'].sum())
169/113: sns.barplot(profile_new.loc[:,'year_2013':'year_2018'].sum())
169/114: profile_new.loc[:,'year_2013':'year_2018'].index
169/115: profile_new.loc[['year_2013':'year_2018']]
169/116: profile_new.loc['year_2013':'year_2018']
169/117: profile_new.loc[:,'year_2013':'year_2018']
169/118: profile_new.loc[:,'year_2013':'year_2018'].sum()
169/119: profile_new.loc[:,'year_2013':'year_2018'].plot(kbind='bar')
169/120: profile_new.loc[:,'year_2013':'year_2018'].plot
169/121: profile_new.loc[:,'year_2013':'year_2018'].plot(kind='bar')
170/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
170/2: portfolio.head()
170/3:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
170/4:
portfolio_new = clean_portfolio()
portfolio_new.head()
170/5: profile.head()
170/6: sns.distplot(profile['age'], kde=False)
170/7: pd.DataFrame(profile.groupby(by='age', as_index=False)['id'].count()).sort_values(by = 'age', ascending=False).head()
170/8:
print(profile.isnull().sum())
print('/')
print(sum(profile['age'] ==118))
170/9: print(profile['became_member_on'].min(), profile['became_member_on'].max())
170/10:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        SimpleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
170/11:
profile_new =clean_profile()
profile_new.head()
170/12: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
170/13: profile_new.loc[:,'10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'].sum().plot.bar()
170/14: profile_new.loc[:,['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s']].sum().plot.bar()
170/15:
age_label = ['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s']
profile_new.loc[:,age_label].sum().plot.bar()
170/16:
age_label = ['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s']
profile_new.loc[:,age_label]
170/17:
age_label = ['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s']
profile_new.loc
170/18:
age_label = ['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s']
profile_new.columns
170/19:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        SimpleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
170/20:
profile_new =clean_profile()
profile_new.head()
170/21:
def clean_profile(df=profile):
    from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''
    # age 118 makes no sense, replace 118 with unknown number
    profile['age'].replace(118, np.nan,inplace=True)

    # gender: assign unknown to other
    profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile['gender'])

    # impute with mean
    imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
        SimpleImputer(missing_values=np.nan, strategy='mean')
    imp_age.fit(profile[['age']])
    age = pd.DataFrame(imp_age.transform(profile[['age']]))

    imp_income.fit(profile[['income']])
    income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile['id'],member_year,member_month,income], axis=1)
    

    return profile_new
170/22:
profile_new =clean_profile()
profile_new.head()
170/23: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
170/24:
age_label = ['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s']
profile_new.loc[:,age_label].sum.plot()
170/25:
age_label = ['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s']
profile_new.loc[:,age_label].sum().plot.bar()
170/26: profile_new.loc[:,'10s':'90s'].sum().plot.bar()
170/27: profile_new.loc[:,'month_1':'month_12'].sum().plot.bar()
170/28: profile_new.loc['income']
170/29: profile_new['income']
170/30: profile_new['income'].plot()
170/31: sns.distplot(profile_new['income'],kde=False)
170/32:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
170/33:
profile_new =clean_profile()
profile_new.head()
170/34:
profile_new =clean_profile()
profile_new.head()
170/35:
profile_new =clean_profile()
profile_new.head()
170/36: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
170/37: profile_new.loc[:,'10s':'90s'].sum().plot.bar()
170/38: sns.distplot(profile_new['income'],kde=False)
170/39: sns.distplot(profile_new['income'])
170/40:
profile_new =clean_profile()
profile_new.shape
170/41: profile.shape
170/42: profile.shape[0] -2175
170/43:
profile_new =clean_profile()
profile_new.head()
170/44: profile_new.groupby(by=['F','M','O'])
170/45: profile_new.groupby(by=['F','M','O']).head()
170/46: profile_new.groupby(by=['F','M','O']).loc[:,'year_2013':'year_2017']
170/47: profile_new.loc[:,'year_2013':'year_2017'].groupby(by=['F','M','O'])
170/48: profile_new.loc[:,'year_2013':'year_2017']
170/49: profile_new.loc[:,['F':'O','year_2013':'year_2017']]
170/50: profile_new.loc[:,['F':'O','year_2013':'year_2017']]
170/51: profile_new.loc[:,['F':'O'],['year_2013':'year_2017']]
170/52: profile_new.loc[:,['F':'O']]
170/53: profile_new.loc[:,'F':'O']
170/54: profile_new.loc[:,'F':'O','year_2013']
170/55: profile_new.loc[:,'F':'O'] + profile_new.loc[:,'year_2013':'year_2018']
170/56: profile_new.loc[:,'F':'O']
170/57: pd.concat([profile_new.loc[:,'F':'O'], profile_new.loc[:,'year_2013':'year_2018']],axis=1)
170/58:
gender_year = pd.concat([profile_new.loc[:,'F':'O'], profile_new.loc[:,'year_2013':'year_2018']],axis=1)
gender_year.groupby(by=['F','M','O')).head()
170/59:
gender_year = pd.concat([profile_new.loc[:,'F':'O'], profile_new.loc[:,'year_2013':'year_2018']],axis=1)
gender_year.groupby(by=['F','M','O']).head()
170/60: gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
170/61:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
gender_year.shape
170/62:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
gender_year.head()
170/63:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
gender_year.groupby('gender')
170/64:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
gender_year.groupby('gender').head()
170/65:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
gender_year.groupby('gender').sum().unstack()
170/66:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
gender_year.groupby('gender').sum().unstack().heatmap()
170/67:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
gender_year.groupby('gender').sum().unstack()
170/68:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.index
170/69:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.index.level(0)
170/70:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.index
170/71:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.index['None']
170/72:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.index
170/73:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.index['gender']
170/74:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.get_level_values(0)
170/75:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.index.get_level_values(0)
170/76:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.index.get_level_values(0)
x.index.get_level_values(1)
170/77:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.plot.bar()
170/78:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.column
170/79:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.columns
170/80:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
170/81:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.head()
170/82:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.columns = ['counts']
170/83:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.columns = ['counts']
x.head()
170/84:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.columns = 'counts'
x.head()
170/85:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = gender_year.groupby('gender').sum().unstack()
x.columns()
170/86:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack())
x.columns()
170/87:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack())
170/88:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
170/89:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
x.head()
170/90:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],data = x)
170/91:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue ='gender',data = x)
170/92:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue = index.get_level_values(1),data = x)
170/93:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue = x.index.get_level_values(1),data = x)
170/94: transcript.head()
170/95:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile_new[profile_new['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
170/96:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile_new[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
170/97:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile_new[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
transcript_new.shape, transcript.shape
170/98:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile_new[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
transcript_new['id'].isin(age_118_label).sum()
170/99:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile_new[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
age_118_label
170/100:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
age_118_label
170/101:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
profile[profile['age'] == 118]
170/102: profile['age']
170/103:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
170/104: portfolio.head()
170/105:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
170/106:
portfolio_new = clean_portfolio()
portfolio_new.head()
170/107: profile.head()
170/108: sns.distplot(profile['age'], kde=False)
170/109: pd.DataFrame(profile.groupby(by='age', as_index=False)['id'].count()).sort_values(by = 'age', ascending=False).head()
170/110:
print(profile.isnull().sum())
print('/')
print(sum(profile['age'] ==118))
170/111: print(profile['became_member_on'].min(), profile['became_member_on'].max())
170/112:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
170/113:
profile_new =clean_profile()
profile_new.head()
170/114: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
170/115:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue = x.index.get_level_values(1),data = x)
170/116: profile_new.loc[:,'10s':'90s'].sum().plot.bar()
170/117: sns.distplot(profile_new['income'])
170/118: transcript.head()
170/119:
def clean_transcript(df=transcript):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]

    return transcript_new
170/120:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
profile[profile['age'] == 118]
170/121:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
profile[profile['age'] == 118]['id']
170/122:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
transcript_new.shape, transcript.shape
172/1: transcript.event.unique()
172/2:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
172/3: portfolio.head()
172/4:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
172/5:
portfolio_new = clean_portfolio()
portfolio_new.head()
172/6: profile.head()
172/7: sns.distplot(profile['age'], kde=False)
172/8: pd.DataFrame(profile.groupby(by='age', as_index=False)['id'].count()).sort_values(by = 'age', ascending=False).head()
172/9:
print(profile.isnull().sum())
print('/')
print(sum(profile['age'] ==118))
172/10: print(profile['became_member_on'].min(), profile['became_member_on'].max())
172/11:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
172/12:
profile_new =clean_profile()
profile_new.head()
172/13: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
172/14:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue = x.index.get_level_values(1),data = x)
172/15: profile_new.loc[:,'10s':'90s'].sum().plot.bar()
172/16: sns.distplot(profile_new['income'])
172/17:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
172/18:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
174/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
174/2:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
174/3: portfolio.head()
174/4:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
174/5:
portfolio_new = clean_portfolio()
portfolio_new.head()
174/6: profile.head()
174/7: sns.distplot(profile['age'], kde=False)
174/8: pd.DataFrame(profile.groupby(by='age', as_index=False)['id'].count()).sort_values(by = 'age', ascending=False).head()
174/9:
print(profile.isnull().sum())
print('/')
print(sum(profile['age'] ==118))
174/10: print(profile['became_member_on'].min(), profile['became_member_on'].max())
174/11:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
174/12:
profile_new =clean_profile()
profile_new.head()
174/13: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
174/14:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue = x.index.get_level_values(1),data = x)
174/15: profile_new.loc[:,'10s':'90s'].sum().plot.bar()
174/16: sns.distplot(profile_new['income'])
174/17: transcript.event.unique()
174/18: transcript.head()
174/19:
df_transcation = transcript_new[transcript_new['event'] == 'transcation']
amount = df_transcation['value'].apply(lambda x:df_transcation['amount'])
174/20:
df_transcation = transcript[transcript['event'] == 'transcation']
amount = df_transcation['value'].apply(lambda x:df_transcation['amount'])
174/21:
df_transcation = transcript[transcript['event'] == 'transcation']
amount = df_transcation['value'].apply(lambda x:df_transcation['amount'])
amount
174/22:
df_transcation = transcript[transcript['event'] == 'transcation']
amount = df_transcation['value'].apply(lambda x:x['amount'])
174/23:
df_transcation = transcript[transcript['event'] == 'transcation']
amount = df_transcation['value'].apply(lambda x:x['amount'])
amount
174/24:

x = transcript['value'].apply(lambda x: x['amount'])
x
174/25:
amount = transcript['value'].apply(lambda x: x['amount'])
amount
174/26:
df_transcation = transcript[transcript['event'] == 'transcation']
df_transcation['value']
174/27: transcript['event']
174/28: transcript['event'].unique()
174/29:
df_transaction = transcript[transcript['event'] == 'transaction']
amount = df_transaction['value'].apply(lambda x:x['amount'])
amount
174/30: transcript['offer_received'] = transcript['event'].apply(lambda x:1 if x == "offer received")
174/31: transcript['offer_received'] = transcript['event'].apply(lambda x:[1] if x == "offer received")
174/32: transcript['event'].apply(lambda x:1 if x == "offer received" else 0)
174/33: transcript['value'][1]
174/34: transcript['value'][1][0:10]
174/35: transcript['value'][1]
174/36: transcript['value']['offer_id']
174/37:
def clean_transcript(df=transcript):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed')]
    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)

    for i in range(len(df_offer)):
        try:
            o_id = df_offer.iloc[i]['value']['offer id']
        except:
            o_id = df_offer.iloc[i]['value']['offer_id']
        df_offer.iloc[i]['offer_id'] = o_id

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])
    return transcript_new, df_offer, df_transaction
174/38: df_offer = transcript[transcript['event'].isin(['offer received', 'offer viewed', 'offer completed'])]
174/39:
def clean_transcript(df=transcript):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)

    for i in range(len(df_offer)):
        try:
            o_id = df_offer.iloc[i]['value']['offer id']
        except:
            o_id = df_offer.iloc[i]['value']['offer_id']
        df_offer.iloc[i]['offer_id'] = o_id

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])
    return transcript_new, df_offer, df_transaction
174/40: transcript_new, df_offer, df_transaction = clean_transcript()
174/41: transcript_new.head()
174/42: df_offer.head()
174/43:
def clean_transcript(df=transcript):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_id = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer_completed':
            o_id.append(df_offer.iloc[i]['value']['offer_id'])
        else:
            o_id.append(df_offer.iloc[i]['value']['offer id'])
    df_offer['offer_id'] = o_id

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])
    return transcript_new, df_offer, df_transaction
174/44: transcript_new, df_offer, df_transaction = clean_transcript()
174/45: df_offer.head()
174/46:
def clean_transcript(df=transcript):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_id = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer_completed':
            print(df_offer.iloc[i]['value'])
            o_id.append(df_offer.iloc[i]['value']['offer_id'])
        else:
            print(df_offer.iloc[i]['value'])
            o_id.append(df_offer.iloc[i]['value']['offer id'])
    df_offer['offer_id'] = o_id

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])
    return transcript_new, df_offer, df_transaction
174/47: transcript_new, df_offer, df_transaction = clean_transcript()
174/48:
def clean_transcript(df=transcript):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_id = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer_completed':
            print(df_offer.iloc[i]['value'])
            o_id.append(df_offer.iloc[i]['value']['offer_id'])
        else:
            print(df_offer.iloc[i]['value'])
            o_id.append(df_offer.iloc[i]['value']['offer id'])
    df_offer['offer_id'] = o_id

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])
    return transcript_new, df_offer, df_transaction
174/49:
def clean_transcript(df=transcript):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_id = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer_completed':
            o_id.append(df_offer.iloc[i]['value']['offer_id'])
        else:
            o_id.append(df_offer.iloc[i]['value']['offer id'])
    df_offer['offer_id'] = o_id

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])
    return transcript_new, df_offer, df_transaction
174/50: transcript_new, df_offer, df_transaction = clean_transcript()
174/51: df_offer.iloc[0]['value']['offer id']
174/52: df_offer.iloc[0]['value']['offer_id']
174/53: df_offer.iloc[0]['value']['offer id']
174/54:
o_id = []
for i in range(len(df_offer)):
    if df_offer.iloc[i]['event'] == 'offer_completed':
        o_id.append(df_offer.iloc[i]['value']['offer_id'])
    else:
        o_id.append(df_offer.iloc[i]['value']['offer id'])
174/55: df_offer['event'].unique()
174/56:
def clean_transcript(df=transcript):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_id = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id.append(df_offer.iloc[i]['value']['offer_id'])
        else:
            o_id.append(df_offer.iloc[i]['value']['offer id'])
    df_offer['offer_id'] = o_id

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])
    return transcript_new, df_offer, df_transaction
174/57: transcript_new, df_offer, df_transaction = clean_transcript()
174/58: df_offer.head()
174/59: df_offer = df_offer[~df_offer[['value', 'event']]]
174/60: df_offer = df_offer.drop(['event', 'value'], axis = 0)
174/61: df_offer = df_offer.drop(['event', 'value'], axis = 1)
174/62: df_transcation.head()
174/63: df_transaction.head()
174/64:
def clean_transcript(df=transcript):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_id = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id.append(df_offer.iloc[i]['value']['offer_id'])
        else:
            o_id.append(df_offer.iloc[i]['value']['offer id'])
    df_offer['offer_id'] = o_id

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transcation.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/65: transcript_new, df_offer, df_transaction = clean_transcript()
174/66:
def clean_transcript(df1=transcript, df2 = portfolio_new):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_id = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id.append(df_offer.iloc[i]['value']['offer_id'])
        else:
            o_id.append(df_offer.iloc[i]['value']['offer id'])
    df_offer['offer_id'] = o_id
    df_offer['offer_type'] = portfolio[portfolio['offer_id'] == o_id]['offer_type'].values[0]

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transcation.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/67: transcript_new, df_offer, df_transaction = clean_transcript()
174/68:
def clean_transcript(df1=transcript, df2 = portfolio_new):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_id = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id.append(df_offer.iloc[i]['value']['offer_id'])
        else:
            o_id.append(df_offer.iloc[i]['value']['offer id'])
    df_offer['offer_id'] = o_id
    df_offer['offer_type'] = portfolio_new[portfolio_new['offer_id'] == o_id]['offer_type'].values[0]

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transcation.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/69: transcript_new, df_offer, df_transaction = clean_transcript()
174/70:
portfolio_new.head()
profile_new.head()
174/71:
portfolio_new.head()
profile_new.head()
df_offer.head()
df_transcation.head()
174/72: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio_new)
174/73:
def clean_transcript(transcript, portfolio_new):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_id = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id.append(df_offer.iloc[i]['value']['offer_id'])
        else:
            o_id.append(df_offer.iloc[i]['value']['offer id'])
    df_offer['offer_id'] = o_id
    df_offer['offer_type'] = portfolio_new[portfolio_new['offer_id'] == o_id]['offer_type'].values[0]

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transcation.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/74: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio_new)
174/75:
def clean_transcript(transcript, portfolio_new):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_id = []
    o_type = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_id.append(o_id)
            o_type.append(portfolio_new[portfolio_new['offer_id'] == o_id]['offer_type'].values[0])
    

    df_offer['offer_id'] = o_id
    df_offer['offer_type'] = o_type

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transcation.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/76: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio_new)
174/77:
def clean_transcript(transcript, portfolio_new):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio_new[portfolio_new['offer_id'] == o_id]['offer_type'].values[0])
    

    df_offer['offer_id'] = o_ids
    df_offer['offer_type'] = o_types

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transcation.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/78: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio_new)
174/79: portfolio_new.head()
174/80:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio_new[portfolio_new['offer_id'] == o_id]['offer_type'].values[0])
    

    df_offer['offer_id'] = o_ids
    df_offer['offer_type'] = o_types

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transcation.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/81:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio[portfolio['offer_id'] == o_id]['offer_type'].values[0])
    

    df_offer['offer_id'] = o_ids
    df_offer['offer_type'] = o_types

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transcation.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/82: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
174/83: portfolio.head()
174/84:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    

    df_offer['offer_id'] = o_ids
    df_offer['offer_type'] = o_types

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transcation.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/85: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
174/86:
df_offer = transcript[transcript['event'].isin(['offer received', 'offer viewed', 'offer completed'])]
o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
174/87:
df_offer = transcript[transcript['event'].isin(['offer received', 'offer viewed', 'offer completed'])]
o_ids = []
o_types = []
for i in range(len(df_offer)):
    if df_offer.iloc[i]['event'] == 'offer completed':
        o_id = df_offer.iloc[i]['value']['offer_id']
    else:
        o_id = df_offer.iloc[i]['value']['offer id']
    o_ids.append(o_id)
    o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
174/88: o_ids
174/89: list(o_ids)
174/90: list(o_ids)[0]
174/91: list(o_ids).shape
174/92: o_ids.shape
174/93: df_offer['offer_id'] = pd.Series(o_ids)
174/94:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    

    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transcation.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/95: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
174/96:
portfolio_new.head()
profile_new.head()
df_offer.head()
df_transcation.head()
174/97:
portfolio_new.head()
profile_new.head()
df_offer.head()
df_transaction.head()
174/98: df_transaction.head()
174/99: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
174/100:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    df_offer['offer_received'] = df_offer['event'].apply(lambda x:1 if x == "offer received" else 0)
    df_offer['offer_viewed'] = df_offer['event'].apply(lambda x:1 if x == "offer viewed" else 0)
    df_offer['offer_completed'] = df_offer['event'].apply(lambda x:1 if x == "offer completed" else 0)
    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    

    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/101: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
174/102: df_transaction.head()
174/103: df_offer.head()
174/104:
user_id = profile_new.unique()
u_id = user_id[0]
transaction = df_transcation[df_transcation['id']==u_id].sort_values(by = 'time')
174/105: profile_new.head()
174/106: profile_new['id']
174/107:
user_id = profile_new['id'].unique()
u_id = user_id[0]
transaction = df_transcation[df_transcation['id']==u_id].sort_values(by = 'time')
174/108: profile_new['id'].unique()
174/109:
user_id = profile_new['id'].unique()
u_id = user_id[0]
transaction = df_transcation[df_transcation['id']==u_id].sort_values(by = 'time')
174/110: profile_new['id'].unique()
174/111: user_id = profile_new['id'].unique()
174/112: df_transaction.head()
174/113: user_id[0]
174/114: df_transcation['id']==u_id]user_id[0]
174/115: df_transcation['id']== user_id[0]
174/116: df_transcation['id']
174/117: df_transaction['id']  == user_id[0]
174/118: df_transactoin[df_transaction['id']  == user_id[0]]
174/119: df_transaction[df_transaction['id']  == user_id[0]]
174/120:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
174/121:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
offer.head()
174/122: df_offer.head()
174/123:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/124: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
174/125: df_offer.head()
174/126:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
offer.head()
174/127: profile[profile['id'] == '0610b486422d4921ae7d2bf64640c50b']
174/128: portfolio.head()
174/129: transcript[transcript['id'] == '0610b486422d4921ae7d2bf64640c50b']
174/130: transcript.head()
174/131: profile[profile['person'] == '0610b486422d4921ae7d2bf64640c50b']
174/132: profile['person']
174/133: transcription['person']
174/134: transcript['person'] == '0610b486422d4921ae7d2bf64640c50b'
174/135: transcript[transcript['person'] == '0610b486422d4921ae7d2bf64640c50b']
174/136:
age_118_label = profile[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
df_offer_tst = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]
174/137: df_offer_tst.head()
174/138:
transcript_new = transcript.rename(columns={'person':'id'})
age_118_label = profile[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
df_offer_tst = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])]
174/139: df_offer_tst.head()
174/140: df_offer_tst[df_offer_tst['id'] == '0610b486422d4921ae7d2bf64640c50b']
174/141:
new_x = df_offer_tst[df_offer_tst['id'] == '0610b486422d4921ae7d2bf64640c50b']

for i in range(len(new_x)):
    if new_x.iloc[i]['event'] == 'offer completed':
        o_id = new_x.iloc[i]['value']['offer_id']
        print(o_id)
    else:
        o_id = new_x.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
174/142:
new_x = df_offer_tst[df_offer_tst['id'] == '0610b486422d4921ae7d2bf64640c50b']
o_ids = []
o_types = []
for i in range(len(new_x)):
    if new_x.iloc[i]['event'] == 'offer completed':
        o_id = new_x.iloc[i]['value']['offer_id']
        print(o_id)
    else:
        o_id = new_x.iloc[i]['value']['offer id']
    o_ids.append(o_id)
    o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
174/143: print(o_ids)
174/144:
new_x['offer_id']  = pd.Series(o_ids)
new_x['offer_type'] = pd.Series(o_types)
new_x.head()
174/145:
new_x['offer_id']  = pd.Series(o_ids)
new_x['offer_type'] = pd.Series(o_types)
pd.Series(o_ids)
174/146:
new_x = df_offer_tst[df_offer_tst['id'] == '0610b486422d4921ae7d2bf64640c50b'].reset_index()
o_ids = []
o_types = []
for i in range(len(new_x)):
    if new_x.iloc[i]['event'] == 'offer completed':
        o_id = new_x.iloc[i]['value']['offer_id']
        print(o_id)
    else:
        o_id = new_x.iloc[i]['value']['offer id']
    o_ids.append(o_id)
    o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
174/147:
new_x['offer_id']  = pd.Series(o_ids)
new_x['offer_type'] = pd.Series(o_types)
new_x.head()
174/148:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
174/149: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
174/150:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
offer.head()
174/151:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
pd.join(transcation, offer, on = 'time')
174/152:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
174/153: transcation.groupby('time')
174/154: transaction.groupby('time')
174/155: transaction.groupby('time').sum()
174/156: transaction.head()
174/157: transaction.groupby('time')['amount'].cumsum()
174/158: transaction.groupby('time').cumsum()
174/159: transaction.groupby('time')['time'].cumsum()
174/160: transaction.groupby('time').cumsum()
174/161: transaction.groupby('time')
174/162: transaction.groupby('time').sum()
174/163: transaction.groupby('time').sum().reset_index()
174/164: transaction.groupby('time').sum().reset_index().cumsum()
174/165: transaction.groupby('time').sum().reset_index()
174/166:
transaction.groupby('time').sum().reset_index()
transaction['cum'] = transaction['amount'].cumsum()
174/167:
transaction.groupby('time').sum().reset_index()
transaction['cum'] = transaction['amount'].cumsum()
transaction.head()
174/168: offer.head()
174/169: pd.merge([transcation,offer], how ='outer')
174/170: pd.merge([transaction,offer], how ='outer')
174/171: offer.merge(transcation, how ='left')
174/172: offer.merge(transaction, how ='left')
174/173: offer.merge(transaction, how ='outer)
174/174: offer.merge(transaction, how ='outer')
174/175: pd.merge(offer, transcation, how = 'right')
174/176: pd.merge(offer, transcation)
174/177: pd.merge(offer, transaction)
174/178: pd.merge(offer, transaction, how = 'outer')
174/179: offer & transcation
174/180: offer  + transcation
174/181: offer  + transaction
174/182: offer  & transaction
174/183: transcation
174/184: transaction
174/185: transaction['time']
174/186: offer['time']
174/187: offer.head()
174/188: offer.iloc[0]['cum'] = 53.79
174/189:
offer.iloc[0]['cum'] = 53.79
offer.head()
174/190: offer.iloc[0]
174/191: offer.iloc[0]['cum'] = 21
174/192:
offer.iloc[0]['cum'] = 21
offer.iloc
174/193:
offer.iloc[0]['cum'] = 21
offer.iloc[0]
174/194:
offer['cum'] = 0
offer.iloc[0]['cum'] = 10
offer.head()
174/195:
offer['cum'] = 0
offer.get_index()
offer.iloc[0]['cum'] = 10
offer.head()
174/196:
offer['cum'] = 0
offer.get_index
174/197:
offer['cum'] = 0
offer.index
174/198:
offer['cum'] = 0
offer.index.value
174/199:
offer['cum'] = 0
offer.index.values
174/200:
offer.index.values
offer.iloc[71645]['new'] = 10
offer.head()
174/201:
offer.index.values
offer.iloc[71645]['sum'] = 10
offer.head()
174/202:
offer.index.values
offer.iloc[71645]['cum'] = 10
offer.head()
174/203:
offer.iloc[71645]['cum'] = 10
offer.head()
174/204:
offer.loc[71645]['cum'] = 10
offer.head()
174/205:
offer.iloc[71645]['cum'] = 10
offer.head()
174/206:
offer.loc[71645]['cum'] = 10
offer.head()
174/207:
offer.loc[71645, 'cum'] = 10
offer.head()
174/208:
offer.iloc[71645, 'cum'] = 10
offer.head()
174/209:
offer.iloc[0]['cum'] = 20
offer.head()
174/210:
offer.loc[71645,'cum'] = 20
offer.head()
174/211:
offer.loc[71645,'sum'] = 20
offer.head()
174/212: pd.merge(offer, transcation, how = 'outer')
174/213: pd.merge(offer, transaction, how = 'outer')
174/214: pd.merge(offer, transaction, how = 'outer').sort_values(by = 'time')
174/215: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offery_id'])
174/216: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id'])
174/217: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'cum'])
174/218: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_tyoe'])
174/219: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_type'])
174/220: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_type'])
174/221: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_type'])
174/222: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_type']).head()
174/223: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time'])
174/224: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time'])
174/225: pd.merge(offer, transaction, how = 'outer')
175/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
175/2: portfolio.head()
175/3:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
175/4:
portfolio_new = clean_portfolio()
portfolio_new.head()
175/5: profile.head()
175/6: sns.distplot(profile['age'], kde=False)
175/7: pd.DataFrame(profile.groupby(by='age', as_index=False)['id'].count()).sort_values(by = 'age', ascending=False).head()
175/8:
print(profile.isnull().sum())
print('/')
print(sum(profile['age'] ==118))
175/9: print(profile['became_member_on'].min(), profile['became_member_on'].max())
175/10:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
175/11:
profile_new =clean_profile()
profile_new.head()
175/12: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
175/13:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue = x.index.get_level_values(1),data = x)
175/14: profile_new.loc[:,'10s':'90s'].sum().plot.bar()
175/15: sns.distplot(profile_new['income'])
175/16:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
175/17: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
175/18:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
175/19: transaction.head()
175/20:
transaction.groupby('time').sum().reset_index()
transaction['cum'] = transaction['amount'].cumsum()
transaction.head()
175/21: user_id = profile['id'].unique()
175/22:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
175/23: pd.merge(offer, transaction, how = 'outer').sortV_vaules(by = ['time', 'offer_id'])
175/24: pd.merge(offer, transaction, how = 'outer').sort_vaules(by = ['time', 'offer_id'])
175/25: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id'])
175/26: transaction.head()
175/27: df_transaction
175/28: user_id[0]
175/29: df_transaction[df_transaction['id']  == user_id[0]]
175/30: user_id = transcript['id'].unique()
175/31: user_id = transcript['person'].unique()
175/32:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
175/33: df_transaction[df_transaction['id']  == user_id[0]]
175/34: transcation.head()
175/35: transaction.head()
175/36: offer.head()
175/37: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id'])
175/38:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
transcation.loc[15, 'time'] = 510
offer = df_offer[df_offer['id']  == user_id[0]]
175/39:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
transaction.loc[15, 'time'] = 510
offer = df_offer[df_offer['id']  == user_id[0]]
175/40: offer.head()
175/41: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id'])
175/42:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
transaction.iloc[15]['time'] = 510
offer = df_offer[df_offer['id']  == user_id[0]]
175/43:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
175/44: user_id = profile['person'].unique()
175/45: user_id = profile['id'].unique()
175/46:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
175/47: offer.head()
175/48: profile.head()
175/49: user_id = profile_new['id'].unique()
175/50:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
175/51: offer.head()
175/52: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id'])
175/53: transaction.head()
175/54: pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id']).reset_index()
175/55:
x  = pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id']).reset_index()
x[amount].fillna('ffill')
175/56:
x  = pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id']).reset_index()
x['amount'].fillna('ffill')
x.head()
175/57:
x  = pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id']).reset_index()
x['amount'].fillna(method='ffill')
x.head()
175/58:
x  = pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id']).reset_index()
x['amount'].fillna(value =np.nan, method='ffill')
x.head()
175/59:
x  = pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id']).reset_index()
x_new = x['amount'].fillna(method='ffill')
x_new
175/60:
x  = pd.merge(offer, transaction, how = 'outer').sort_values(by = ['time', 'offer_id']).reset_index()
x['amount'].fillna(method='ffill', inplace =True)
x
175/61:
transaction.groupby('time').sum().reset_index()
transaction['cum'] = transaction['amount'].cumsum()
175/62:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
175/63: transaction.head()
175/64:
transaction.groupby('time').sum().reset_index()
#transaction['cum'] = transaction['amount'].cumsum()
175/65: transaction.head()
175/66: transaction['cum'] = transaction['amount'].cumsum()
175/67:
transaction['cum'] = transaction['amount'].cumsum()
transaction.head()
175/68: offer.merge(transaction, how = 'outer')
175/69:
offer.merge(transaction, how = 'outer')
offer.head()
175/70:
offer.merge(transaction, how = 'outer')
offer.head()
175/71:
offer.merge(transaction, how = 'outer')
offer
175/72:
transaction = df_transaction[df_transaction['id']  == user_id[0]]
offer = df_offer[df_offer['id']  == user_id[0]]
175/73:
transaction['cum'] = transaction['amount'].cumsum()
transaction.head()
175/74:
offer.merge(transaction, how = 'outer')
offer
175/75:
user_id = profile_new['id'].unique()
for u_id in user_id:
    # offer info
    rcd = df_offer[df_offer['id'] == u_id]
    # transaction info
    transaction = df_transaction[df_transaction['id']==u_id]
    transaction['cum'] = transaction['amount'].cumsum()
    
    # merge and take 
    rcd.merge(transaction, how = 'outer').sort_values(by ="time")
    df_merge['cum'] = pd.fillna(method='ffill')
    rcd['cum'] = df_merge.dropna(subset=['offer_id'])['cum']

    # compare time to fill offer and transcation info
175/76:
user_id = profile_new['id'].unique()
for u_id in user_id:
    # offer info
    rcd = df_offer[df_offer['id'] == u_id]
    # transaction info
    transaction = df_transaction[df_transaction['id']==u_id]
    transaction['cum'] = transaction['amount'].cumsum()
    
    # merge and take 
    rcd.merge(transaction, how = 'outer').sort_values(by ="time")
    df_merge['cum'].fillna(method='ffill', inplace = True)
    rcd['cum'] = df_merge.dropna(subset=['offer_id'])['cum']

    # compare time to fill offer and transcation info
175/77:
user_id = profile_new['id'].unique()
for u_id in user_id:
    # offer info
    rcd = df_offer[df_offer['id'] == u_id]
    # transaction info
    transaction = df_transaction[df_transaction['id']==u_id]
    transaction['cum'] = transaction['amount'].cumsum()
    
    # merge and take 
    df_merge = pd.merge(rcd, transaction, how = 'outer').sort_values(by ="time")
    df_merge['cum'].fillna(method='ffill', inplace = True)
    rcd['cum'] = df_merge.dropna(subset=['offer_id'])['cum']

    # compare time to fill offer and transcation info
175/78:
df_offer_tst = df_offer
df_transaction_tst = df_transaction

len(df_transaction_tst.groupby('id').sum().unstack())
175/79: len(df_transaction_tst)
175/80:
df_offer_tst = df_offer
df_transaction_tst = df_transaction

df_transaction_tst.sort_values(by= 'id')
175/81:
df_offer_tst = df_offer
df_transaction_tst = df_transaction

df_transaction_tst.sort_values(by= ['id', 'time'])
175/82: df_transaction_tst.head()
175/83:
df_offer_tst = df_offer
df_transaction_tst = df_transaction

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(rcd, transaction, how ='outer').sort_values(by = 'time')
df_merge.head()
175/84:
df_offer_tst = df_offer
df_transaction_tst = df_transaction

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(rcd, transaction, how ='outer').sort_values(by = 'time')
df_merge['cum'].fillna(method='ffill', inplace = True)
175/85:
df_offer_tst = df_offer
df_transaction_tst = df_transaction

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(rcd, transaction, how ='outer').sort_values(by = 'time')
df_merge['cum'].fillna(method='ffill', inplace = True)
df_merge.head()
175/86:
df_offer_tst = df_offer
df_transaction_tst = df_transaction

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(rcd, transaction, how ='outer').sort_values(by = 'time')
df_merge['cum'].fillna(method='ffill', inplace = True)
df_merge.dropna(subset='offer_id', inplace=True)
df_merge.head()
175/87:
df_offer_tst = df_offer
df_transaction_tst = df_transaction

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(rcd, transaction, how ='outer').sort_values(by = 'time')
df_merge['cum'].fillna(method='ffill', inplace = True)
df_merge.dropna(subset=['offer_id'], inplace=True)
df_merge.head()
175/88:
df_offer_tst = df_offer
df_transaction_tst = df_transaction

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(rcd, transaction, how ='outer').sort_values(by = 'time')
df_merge['cum'].fillna(method='ffill', inplace = True)
#df_merge.dropna(subset=['offer_id'], inplace=True)
df_merge.head()
176/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
176/2: portfolio.head()
176/3:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
176/4:
portfolio_new = clean_portfolio()
portfolio_new.head()
176/5: profile.head()
176/6: sns.distplot(profile['age'], kde=False)
176/7: pd.DataFrame(profile.groupby(by='age', as_index=False)['id'].count()).sort_values(by = 'age', ascending=False).head()
176/8:
print(profile.isnull().sum())
print('/')
print(sum(profile['age'] ==118))
176/9: print(profile['became_member_on'].min(), profile['became_member_on'].max())
176/10:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
176/11:
profile_new =clean_profile()
profile_new.head()
176/12: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
176/13:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue = x.index.get_level_values(1),data = x)
176/14: profile_new.loc[:,'10s':'90s'].sum().plot.bar()
176/15: sns.distplot(profile_new['income'])
176/16:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction']
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
176/17: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
176/18: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
176/19:
df_offer_tst = df_offer.copy
df_transaction_tst = df_transaction.copy

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True).reset_index()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True).reset_index()

df_merge = pd.merge(rcd, transaction, how ='outer')
df_merge.head()
# df_merge['cum'].fillna(method='ffill', inplace = True)
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/20:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True).reset_index()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True).reset_index()

df_merge = pd.merge(rcd, transaction, how ='outer')
df_merge.head()
# df_merge['cum'].fillna(method='ffill', inplace = True)
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/21:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(rcd.reset_index(), transaction.reset_index(), how ='outer')
df_merge.head()
# df_merge['cum'].fillna(method='ffill', inplace = True)
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/22:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(df_offer_test.reset_index(), df_transaction_tst.reset_index(), how ='outer')
df_merge.head()
# df_merge['cum'].fillna(method='ffill', inplace = True)
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/23:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(df_offer_tst.reset_index(), df_transaction_tst.reset_index(), how ='outer')
df_merge.head()
# df_merge['cum'].fillna(method='ffill', inplace = True)
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/24:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(df_offer_tst.reset_index(), df_transaction_tst.reset_index(), how ='outer')
df_merge.head()
df_merge['cum'].fillna(method='ffill', inplace = True)
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/25:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()

df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(df_offer_tst.reset_index(), df_transaction_tst.reset_index(), how ='outer')
df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/26:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(df_offer_tst.reset_index(), df_transaction_tst.reset_index(), how ='outer')
df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/27:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer')
df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/28:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/29:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge.fillna(method='ffill', inplace = True)
df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/30:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
#df_merge.fillna(method='ffill', inplace = True)
df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/31:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge.fillna(method='ffill', inplace = True)
df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/32:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)

df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)
df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/33:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_transaction_tst['total'] = df_transaction_tst.groupby(by='id')['cum'].max().unstack()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)

df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/34: df_transaction_tst.groupby(by='id')['cum'].max().unstack()
176/35: df_transaction_tst.groupby(by='id')['cum'].max()
176/36: df_transaction_tst.groupby(by='id')['cum'].max().values
176/37:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_transaction_tst['total'] = df_transaction_tst.groupby(by='id')['cum'].max().values
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)

df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/38:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_transaction_tst['total'] = df_transaction_tst.groupby(by='id')['cum'].max()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)

df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/39:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True).reset_index
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_transaction_tst['total'] = df_transaction_tst.groupby(by='id')['cum'].max()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)

df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/40:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True).reset_index()
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_transaction_tst['total'] = df_transaction_tst.groupby(by='id')['cum'].max()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)

df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/41: df_transaction_tst.groupby(by='id')['cum'].max()
176/42: df_transaction_tst.groupby(by='id')
176/43: df_transaction_tst.head()
176/44:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)

df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/45: df_transaction_tst.head()
176/46: df_transaction_tst.groupby(by='id').max().unstack()
176/47: df_transaction_tst.groupby(by='id')['cum'].max().unstack()
176/48: df_transaction_tst.groupby(by='id')['cum']
176/49: df_transaction_tst.groupby(by='id')['cum'].max()
176/50: pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max())
176/51:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max())
x.columns()
176/52:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max())
x.columns
176/53: x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max())
176/54:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max())
x.head()
176/55:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max(), columns={'cum':'total'})
x.head()
176/56:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max(), columns={'total'})
x.head()
176/57:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max(), columns={'cum':'total'})
x.head()
176/58:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max(), columns={'cum':'total'}, inplace= True)
x.head()
176/59:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'})
x.head()
176/60:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'})
pd.merge(df_transaction_tst, x, how ='left')
176/61:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'})
pd.merge(df_transaction_tst, x, left_on ='id')
176/62:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'})
x.head()
176/63: x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
176/64:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
x.head()
176/65:
x = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
pd.merge(df_transaction_tst,x, how ='left')
176/66:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


# df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
# df_merge['cum'].fillna(method='ffill', inplace = True)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/67: df_transaction_tst.head()
176/68:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True).reset_index
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


# df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
# df_merge['cum'].fillna(method='ffill', inplace = True)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/69:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True).reset_index()
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


# df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
# df_merge['cum'].fillna(method='ffill', inplace = True)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/70: df_offer_tst.head()
176/71:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst.sort_values(by= ['id', 'time'], inplace = True).reset_index


# df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
# df_merge['cum'].fillna(method='ffill', inplace = True)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/72: df_offer_tst.head()
176/73:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst = df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True).reset_index()
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()
df_offer_tst = df_offer_tst.sort_values(by= ['id', 'time'], inplace = True).reset_index()


# df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
# df_merge['cum'].fillna(method='ffill', inplace = True)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/74:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transcation_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
# df_merge['cum'].fillna(method='ffill', inplace = True)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/75:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transcation_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge.head()
# df_merge['cum'].fillna(method='ffill', inplace = True)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/76:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transaction_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge.head()
# df_merge['cum'].fillna(method='ffill', inplace = True)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/77:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transaction_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge.head()
df_merge[['cum', 'total']].fillna(method='ffill', inplace = True)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/78:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transaction_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge[['cum', 'total']].fillna(method='ffill', inplace = True)
df_merge.head()

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/79:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transaction_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum','total'].fillna(method='ffill', inplace = True)
df_merge.head()

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/80:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transaction_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)
df_merge['total'].fillna(method='ffill', inplace = True)
df_merge.head()

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/81:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transaction_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)
df_merge['total'].fillna(method='ffill', inplace = True)
df_merge.head(10)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/82:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transaction_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)
df_merge['total'].fillna(method='ffill', inplace = True)
df_merge.head(20)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/83:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
176/84: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
176/85:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transaction_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)
df_merge['total'].fillna(method='ffill', inplace = True)
df_merge.head(20)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/86:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
176/87: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
176/88:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transaction_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)
df_merge['total'].fillna(method='ffill', inplace = True)
df_merge.head(20)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/89: df_offer_tst.head()
176/90: df_offer_tst.head(20)
176/91: df_offer_tst[df_offer_tst[id] == '0009655768c64bdeb2e877511632db8f']
176/92: df_offer_tst[df_offer_tst[id] == '0009655768c64bdeb2e877511632db8f']
176/93: df_offer_tst[df_offer_tst['id'] == '0009655768c64bdeb2e877511632db8f']
176/94: df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
176/95:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]

df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

# o_ids = []
# o_types = []
# for i in range(len(df_offer)):
#     if df_offer.iloc[i]['event'] == 'offer completed':
#         o_id = df_offer.iloc[i]['value']['offer_id']
#     else:
#         o_id = df_offer.iloc[i]['value']['offer id']
#         o_ids.append(o_id)
#         o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])

# df_offer['offer_id'] = pd.Series(o_ids)
# df_offer['offer_type'] = pd.Series(o_types)

# event_cat = pd.get_dummies(df_offer['event'])
# df_offer = pd.concat([df_offer, event_cat], axis=1)

# df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
# df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

# df_offer = df_offer.drop(['value', 'event'], axis = 1)
# df_transaction = df_transaction.drop(['value', 'event'], axis =1)
176/96: df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
176/97: df_offer.head()
176/98:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]

df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

o_ids = []
o_types = []
for i in range(len(df_offer)):
    if df_offer.iloc[i]['event'] == 'offer completed':
        o_id = df_offer.iloc[i]['value']['offer_id']
    else:
        o_id = df_offer.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])

df_offer['offer_id'] = pd.Series(o_ids)
df_offer['offer_type'] = pd.Series(o_types)

# event_cat = pd.get_dummies(df_offer['event'])
# df_offer = pd.concat([df_offer, event_cat], axis=1)

# df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
# df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

# df_offer = df_offer.drop(['value', 'event'], axis = 1)
# df_transaction = df_transaction.drop(['value', 'event'], axis =1)
176/99: df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
176/100: df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
176/101: len(df_offer), len(o_types)
176/102: df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
176/103:
x = df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
o_ids, o_types = [], []
for i in range(len(df_offer)):
    if df_offer.iloc[i]['event'] == 'offer completed':
        o_id = df_offer.iloc[i]['value']['offer_id']
    else:
        o_id = df_offer.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
176/104:
x = df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
o_ids, o_types = [], []
for i in range(len(x)):
    if x.iloc[i]['event'] == 'offer completed':
        o_id = x.iloc[i]['value']['offer_id']
    else:
        o_id = x.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
176/105: o_ids
176/106: len(o_ids)
176/107: len(o_ids), len(x)
176/108:
x = df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
o_ids, o_types = [], []
for i in range(9,len(x)):
    print(x.iloc[i]['event'])
    if x.iloc[i]['event'] == 'offer completed':
        o_id = x.iloc[i]['value']['offer_id']
    else:
        o_id = x.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
176/109:
x = df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
o_ids, o_types = [], []
for i in range(9,len(x)):
    print(x.iloc[i]['event'])
    if x.iloc[i]['event'] == 'offer completed':
        o_id = x.iloc[i]['value']['offer_id']
    else:
        o_id = x.iloc[i]['value']['offer id']
    o_ids.append(o_id)
    o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
176/110: len(o_ids), len(x)
176/111:
x = df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
o_ids, o_types = [], []
for i in range(len(x)):
    if x.iloc[i]['event'] == 'offer completed':
        o_id = x.iloc[i]['value']['offer_id']
    else:
        o_id = x.iloc[i]['value']['offer id']
    o_ids.append(o_id)
    o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
176/112: len(o_ids), len(x)
176/113:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]

df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

o_ids = []
o_types = []
for i in range(len(df_offer)):
    if df_offer.iloc[i]['event'] == 'offer completed':
        o_id = df_offer.iloc[i]['value']['offer_id']
    else:
        o_id = df_offer.iloc[i]['value']['offer id']
        
    o_ids.append(o_id)
    o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])

df_offer['offer_id'] = pd.Series(o_ids)
df_offer['offer_type'] = pd.Series(o_types)

# event_cat = pd.get_dummies(df_offer['event'])
# df_offer = pd.concat([df_offer, event_cat], axis=1)

# df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
# df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

# df_offer = df_offer.drop(['value', 'event'], axis = 1)
# df_transaction = df_transaction.drop(['value', 'event'], axis =1)
176/114: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
176/115: df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
176/116:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
            o_ids.append(o_id)
            o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
176/117: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
176/118: df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
176/119:
# rename person_id to id
transcript_new = transcript.rename(columns={'person':'id'})
# remove the person who age 118 (those data points were removed)
age_118_label = profile[profile['age'] == 118]['id']
transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]

df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

o_ids = []
o_types = []
# for i in range(len(df_offer)):
#     if df_offer.iloc[i]['event'] == 'offer completed':
#         o_id = df_offer.iloc[i]['value']['offer_id']
#     else:
#         o_id = df_offer.iloc[i]['value']['offer id']

#     o_ids.append(o_id)
#     o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])

# df_offer['offer_id'] = pd.Series(o_ids)
# df_offer['offer_type'] = pd.Series(o_types)

# event_cat = pd.get_dummies(df_offer['event'])
# df_offer = pd.concat([df_offer, event_cat], axis=1)

# df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
# df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

# df_offer = df_offer.drop(['value', 'event'], axis = 1)
# df_transaction = df_transaction.drop(['value', 'event'], axis =1)
176/120:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
176/121: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
176/122: df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']
176/123:
df_offer_tst = df_offer.copy()
df_transaction_tst = df_transaction.copy()


df_transaction_tst.sort_values(by= ['id', 'time'], inplace = True)
df_transaction_tst['cum'] = df_transaction_tst['amount'].cumsum()

total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
df_transaction_tst = pd.merge(df_transaction_tst,total_spend, how ='left')


df_offer_tst.sort_values(by= ['id', 'time'], inplace = True)


df_merge = pd.merge(df_offer_tst, df_transaction_tst, how ='outer').sort_values(by=['id','time'])
df_merge['cum'].fillna(method='ffill', inplace = True)
df_merge['total'].fillna(method='ffill', inplace = True)
df_merge.head(20)

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/124:
def combine_offer_transaction(df_offer, df_transaction):
    
    df_transaction.sort_values(by=['id', 'time'], inplace=True)
    df_transaction['cum'] = df_transaction['amount'].cumsum()
    total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
    df_transaction = pd.merge(df_transaction, total_spend, how='left')
    
    df_offer.sort_values(by=['id', 'time'], inplace =True)
    
    df_merge = pd.merge(df_offer, df_transaction, how='outer').sort_values(by=['id','time'])
    df_merge['cum'].fillna(method='ffill', inplace = True)
    df_merge['total'].fillna(method='ffill', inplace = True)

    return df_merge

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/125: df_merge = combine_offer_transaction(df_offer, df_transaction)
176/126: df_merge.head()
176/127:
def combine_offer_transaction(df_offer, df_transaction):
    
    df_transaction.sort_values(by=['id', 'time'], inplace=True)
    df_transaction['cum'] = df_transaction['amount'].cumsum()
    total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
    df_transaction = pd.merge(df_transaction, total_spend, how='left')
    
    df_offer.sort_values(by=['id', 'time'], inplace =True)
    
    df_merge = pd.merge(df_offer, df_transaction, how='outer').sort_values(by=['id','time'])
    df_merge['cum'].fillna(method='ffill', inplace = True)
    df_merge['total'].fillna(method='ffill', inplace = True)

    df_merge.dropna(subset=['offer_id'], inplace=True)
    return df_merge

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/128: df_merge = combine_offer_transaction(df_offer, df_transaction)
176/129: df_merge.head()
176/130: df_merge.head(20)
176/131:
def combine_offer_transaction(df_offer, df_transaction):
    
    df_transaction.sort_values(by=['id', 'time'], inplace=True)
    df_transaction['cum'] = df_transaction['amount'].cumsum()
    total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
    df_transaction = pd.merge(df_transaction, total_spend, how='left')
    
    df_offer.sort_values(by=['id', 'time'], inplace =True)
    
    df_merge = pd.merge(df_offer, df_transaction, how='outer').sort_values(by=['id','time'])
    df_merge['cum'].fillna(method='ffill', inplace = True)
    df_merge['total'].fillna(method='ffill', inplace = True)

    # df_merge.dropna(subset=['offer_id'], inplace=True)
    return df_merge

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/132: df_merge = combine_offer_transaction(df_offer, df_transaction)
176/133: df_merge.head(20)
176/134:
def combine_offer_transaction(df_offer, df_transaction):
    
    df_transaction.sort_values(by=['id', 'time'], inplace=True)
    df_transaction['cum'] = df_transaction['amount'].cumsum()
    total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
    df_transaction = pd.merge(df_transaction, total_spend, how='left')
    
    df_offer.sort_values(by=['id', 'time'], inplace =True)
    
    df_merge = pd.merge(df_offer, df_transaction, how='outer').sort_values(by=['id','time'])
    df_merge['cum'].fillna(method='ffill', inplace = True)
    df_merge['total'].fillna(method='ffill', inplace = True)

    df_merge.dropna(subset=['offer_id'], inplace=True)
    return df_merge

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/135: df_merge = combine_offer_transaction(df_offer, df_transaction)
176/136: df_merge.head(20)
176/137: portfolio.head()
176/138: profile.head()
176/139: profile_new.head()
176/140: protfolio.head()
176/141: portfolio_new.head()
176/142: df_merge.head()
176/143: portfolio_new.head()
176/144:
def combine_offer_transaction(df_offer, df_transaction):
    
    df_transaction.sort_values(by=['id', 'time'], inplace=True)
    df_transaction['cum'] = df_transaction['amount'].cumsum()
    total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
    df_transaction = pd.merge(df_transaction, total_spend, how='left')
    
    df_offer.sort_values(by=['id', 'time'], inplace =True)
    
    df_merge = pd.merge(df_offer, df_transaction, how='outer').sort_values(by=['id','time'])
    df_merge['cum'].fillna(method='ffill', inplace = True)
    df_merge['total'].fillna(method='ffill', inplace = True)
    
    df_merge.dropna(subset=['offer_id'], inplace=True)
    df_merge.drop('index', axis =1, inplace=True)
    return df_merge

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
176/145: df_merge = combine_offer_transaction(df_offer, df_transaction)
176/146: df_merge.head()
176/147: df_merge.shape, profile_new.shape, portfolio_new.shape
176/148: len(portfolio_new)
176/149:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
176/150:
portfolio_new = clean_portfolio()
portfolio_new.head()
176/151: portfolio_new.shape
176/152:
portfolio_new = portfolio.copy()
portfolio_new['duration'] = portfolio['duration']*24
# rename id -> offer_id
len(portfolio_new)
portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)

# get dummies to channel and offer_type
channels = portfolio['channels'].apply(lambda x: ','.join(x))
channels = pd.Series(channels).str.get_dummies(',')

offer_type = pd.get_dummies(portfolio['offer_type'])

# new cleaned portfolio
portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
176/153:
portfolio_new = portfolio.copy()
portfolio_new['duration'] = portfolio['duration']*24
# rename id -> offer_id
print(len(portfolio_new))
portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)

# get dummies to channel and offer_type
channels = portfolio['channels'].apply(lambda x: ','.join(x))
channels = pd.Series(channels).str.get_dummies(',')

offer_type = pd.get_dummies(portfolio['offer_type'])

# new cleaned portfolio
portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
176/154:
print(len(portfolio))
portfolio_new = portfolio.copy()
portfolio_new['duration'] = portfolio['duration']*24
# rename id -> offer_id
print(len(portfolio_new))
portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)

# get dummies to channel and offer_type
channels = portfolio['channels'].apply(lambda x: ','.join(x))
channels = pd.Series(channels).str.get_dummies(',')

offer_type = pd.get_dummies(portfolio['offer_type'])

# new cleaned portfolio
portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
176/155: len(portfolio)
178/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
178/2: portfolio.shape
178/3: portfolio.head()
178/4: profile.head()
178/5:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
178/6:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
178/7: len(portfolio)
178/8:
print(len(portfolio))
portfolio_new = portfolio.copy()
portfolio_new['duration'] = portfolio['duration']*24
# rename id -> offer_id
print(len(portfolio_new))
portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)

# get dummies to channel and offer_type
channels = portfolio['channels'].apply(lambda x: ','.join(x))
channels = pd.Series(channels).str.get_dummies(',')

offer_type = pd.get_dummies(portfolio['offer_type'])

# new cleaned portfolio
portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
178/9:
portfolio_new = clean_portfolio()
portfolio_new.head()
178/10: portfolio_new.shape
178/11: sns.distplot(profile['age'], kde=False)
178/12: pd.DataFrame(profile.groupby(by='age', as_index=False)['id'].count()).sort_values(by = 'age', ascending=False).head()
178/13:
print(profile.isnull().sum())
print('/')
print(sum(profile['age'] ==118))
178/14: print(profile['became_member_on'].min(), profile['became_member_on'].max())
178/15:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
178/16:
profile_new =clean_profile()
profile_new.head()
178/17: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
178/18:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue = x.index.get_level_values(1),data = x)
178/19: profile_new.loc[:,'10s':'90s'].sum().plot.bar()
178/20: sns.distplot(profile_new['income'])
178/21:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
178/22:
# # rename person_id to id
# transcript_new = transcript.rename(columns={'person':'id'})
# # remove the person who age 118 (those data points were removed)
# age_118_label = profile[profile['age'] == 118]['id']
# transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]

# df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

# o_ids = []
# o_types = []

# for i in range(len(df_offer)):
#     if df_offer.iloc[i]['event'] == 'offer completed':
#         o_id = df_offer.iloc[i]['value']['offer_id']
#     else:
#         o_id = df_offer.iloc[i]['value']['offer id']

#     o_ids.append(o_id)
#     o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])

# df_offer['offer_id'] = pd.Series(o_ids)
# df_offer['offer_type'] = pd.Series(o_types)

# event_cat = pd.get_dummies(df_offer['event'])
# df_offer = pd.concat([df_offer, event_cat], axis=1)

# df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
# df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

# df_offer = df_offer.drop(['value', 'event'], axis = 1)
# df_transaction = df_transaction.drop(['value', 'event'], axis =1)
178/23:
x = df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']

for i in range(len(x)):
    if x.iloc[i]['event'] == 'offer completed':
        o_id = x.iloc[i]['value']['offer_id']
    else:
        o_id = df_offer.iloc[i]['value']['offer id']
    o_ids.append(o_id)
    o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
178/24: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
178/25:
x = df_offer[df_offer['id'] == '0009655768c64bdeb2e877511632db8f']

for i in range(len(x)):
    if x.iloc[i]['event'] == 'offer completed':
        o_id = x.iloc[i]['value']['offer_id']
    else:
        o_id = df_offer.iloc[i]['value']['offer id']
    o_ids.append(o_id)
    o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
178/26:
def combine_offer_transaction(df_offer, df_transaction):
    
    df_transaction.sort_values(by=['id', 'time'], inplace=True)
    df_transaction['cum'] = df_transaction['amount'].cumsum()
    total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
    df_transaction = pd.merge(df_transaction, total_spend, how='left')
    
    df_offer.sort_values(by=['id', 'time'], inplace =True)
    
    df_merge = pd.merge(df_offer, df_transaction, how='outer').sort_values(by=['id','time'])
    df_merge['cum'].fillna(method='ffill', inplace = True)
    df_merge['total'].fillna(method='ffill', inplace = True)
    
    df_merge.dropna(subset=['offer_id'], inplace=True)
    df_merge.drop('index', axis =1, inplace=True)
    return df_merge

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
178/27: df_merge = combine_offer_transaction(df_offer, df_transaction)
178/28: df_merge.head()
178/29: df_merge.shape, profile_new.shape, portfolio_new.shape
179/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
179/2:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
179/3:
print(len(portfolio))
portfolio_new = portfolio.copy()
portfolio_new['duration'] = portfolio['duration']*24
# rename id -> offer_id
print(len(portfolio_new))
portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)

# get dummies to channel and offer_type
channels = portfolio['channels'].apply(lambda x: ','.join(x))
channels = pd.Series(channels).str.get_dummies(',')

offer_type = pd.get_dummies(portfolio['offer_type'])

# new cleaned portfolio
portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
179/4:
portfolio_new = clean_portfolio()
portfolio_new.head()
179/5: sns.distplot(profile['age'], kde=False)
179/6: pd.DataFrame(profile.groupby(by='age', as_index=False)['id'].count()).sort_values(by = 'age', ascending=False).head()
179/7:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
179/8:
profile_new =clean_profile()
profile_new.head()
179/9: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
179/10:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue = x.index.get_level_values(1),data = x)
179/11: profile_new.loc[:,'10s':'90s'].sum().plot.bar()
179/12: sns.distplot(profile_new['income'])
179/13:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
179/14: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
179/15:
def combine_offer_transaction(df_offer, df_transaction):
    
    df_transaction.sort_values(by=['id', 'time'], inplace=True)
    df_transaction['cum'] = df_transaction['amount'].cumsum()
    total_spend = pd.DataFrame(df_transaction_tst.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
    df_transaction = pd.merge(df_transaction, total_spend, how='left')
    
    df_offer.sort_values(by=['id', 'time'], inplace =True)
    
    df_merge = pd.merge(df_offer, df_transaction, how='outer').sort_values(by=['id','time'])
    df_merge['cum'].fillna(method='ffill', inplace = True)
    df_merge['total'].fillna(method='ffill', inplace = True)
    
    df_merge.dropna(subset=['offer_id'], inplace=True)
    df_merge.drop('index', axis =1, inplace=True)
    return df_merge

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
179/16: df_merge = combine_offer_transaction(df_offer, df_transaction)
179/17: df_merge.head()
179/18:
def combine_offer_transaction(df_offer, df_transaction):
    
    df_transaction.sort_values(by=['id', 'time'], inplace=True)
    df_transaction['cum'] = df_transaction['amount'].cumsum()
    total_spend = pd.DataFrame(df_transaction.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
    df_transaction = pd.merge(df_transaction, total_spend, how='left')
    
    df_offer.sort_values(by=['id', 'time'], inplace =True)
    
    df_merge = pd.merge(df_offer, df_transaction, how='outer').sort_values(by=['id','time'])
    df_merge['cum'].fillna(method='ffill', inplace = True)
    df_merge['total'].fillna(method='ffill', inplace = True)
    
    df_merge.dropna(subset=['offer_id'], inplace=True)
    df_merge.drop('index', axis =1, inplace=True)
    return df_merge

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
179/19: df_merge = combine_offer_transaction(df_offer, df_transaction)
179/20: df_merge.head()
179/21:
df_master = pd.merge(df_merge, profile_new, left_on= 'id')
df_master = pd.merge(df_master, portfolio_new, left_on='offer_id')
df_master.head()
179/22: profile_new.head()
179/23: profile.head()
179/24: profile_new['id']
179/25: pd.merge(df_merge, profile_new)
179/26: pd.merge(df_merge, profile_new, how='left', left_on='id')
179/27: df_merge['id']
179/28: df_merge.head()
179/29: sorted(df_merge['id'].unique())
179/30: len(sorted(df_merge['id'].unique())), len(sorted(profile_new['id'].unique()))
179/31: len(pd.merge(df_merge, profile_new))
179/32: len(pd.merge(df_merge, profile_new, how ='left'))
179/33:
len(sorted(df_merge['id'].unique())), len(sorted(profile_new['id'].unique()))
np.intersect1d(df_merge['id'].unique(), profile_new['id'].unique())
179/34:
len(sorted(df_merge['id'].unique())), len(sorted(profile_new['id'].unique()))
len(np.intersect1d(df_merge['id'].unique(), profile_new['id'].unique()))
179/35: len(pd.merge(df_merge, profile_new, how ='outer', on='id'))
179/36: len(pd.merge(df_merge, profile_new, how ='outer'))
179/37: pd.merge(df_merge, profile_new, how = 'inner')
179/38: len(pd.merge(df_merge, profile_new, how = 'inner'))
179/39: pd.merge(df_merge, profile_new, on ='id')
179/40:
df_master = pd.merge(df_merge, profile_new, on ='id')
df_master = pd.merge(df_master, portfolio_new, on='offer_id')
df_master.head()
179/41:
df_master = pd.merge(df_merge, profile_new, on ='id')
df_master = pd.merge(df_master, portfolio_new, left_on='offer_id')
df_master.head()
179/42:
df_master = pd.merge(df_merge, profile_new, on ='id')
df_master['offer_id'].unique()
179/43:
df_master = pd.merge(df_merge, profile_new, on ='id')
len(df_master['offer_id'].unique())
179/44:
df_master = pd.merge(df_merge, profile_new, on ='id')
df_master = pd.merge(df_master, portfolio_new, how ='left', on='offer_id')
df_master.head()
179/45:
df_master = pd.merge(df_merge, profile_new, on ='id')
df_master = pd.merge(df_master, portfolio_new, how ='left')
df_master.head()
179/46:
df_master = pd.merge(df_merge, profile_new, on ='id')
#df_master = pd.merge(df_master, portfolio_new, how ='left')
df_master.head()
179/47: df_merge = combine_offer_transaction(df_offer, df_transaction)
179/48:
df_master = pd.merge(df_merge, profile_new, on ='id')
df_master = pd.merge(df_master, portfolio_new, how ='left')
df_master.head()
179/49: df_merge = combine_offer_transaction(df_offer, df_transaction)
179/50: df_merge.head()
179/51: df_merge.shap
179/52: df_merge.shape
179/53:
df_master = pd.merge(df_merge, profile_new, on ='id')
len(df_master)
179/54:
df_master_new = pd.merge(df_master, portfolio_new, how ='left')
df_master_new.head()
179/55: df_master.head()
179/56:
df_master = pd.merge(df_merge, profile_new, on ='id')
len(df_master)
179/57: df_master.head()
179/58:
df_master_new = pd.merge(df_master, portfolio_new, how ='left')
len(df_master_new)
179/59:
df_master = pd.merge(df_master, portfolio_new, how ='left')
len(df_master)
179/60:
df_master = pd.merge(df_merge, profile_new, on ='id')
df_master = pd.merge(df_master, portfolio_new, how ='left')
len(df_master)
179/61:
df_merge = combine_offer_transaction(df_offer, df_transaction)
# merge three df all together
df_master = pd.merge(df_merge, profile_new, on ='id')
df_master = pd.merge(df_master, portfolio_new, how ='left')
179/62: df_master.head()
179/63: df_master.groupby(by='id')
179/64: df_master.groupby(by='id')['offer_id']
179/65: df_master.groupby(by='id').sum()
179/66: df_master.groupby(by=['id', 'offer_id']).sum()
179/67: df_master.groupby(by=['id', 'offer_id']).sum().index
179/68: df_master.groupby(by=['id', 'offer_id']).sum().unstack()
179/69: df_master.groupby(by=['id', 'offer_id']).sum()
179/70: df_master.groupby(by=['id', 'offer_id']).sum().unstack()
179/71: df_response = df_master.groupby(by=['id', 'offer_id']).sum()
179/72: df_response['cum']
179/73: df_response.loc[:,'offer completed': 'offer viewed']
179/74: df_response = df_master.groupby(by=['id', 'offer_id', 'offer_type']).sum()
179/75: df_response.loc[:,'offer completed': 'offer viewed']
179/76: df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
179/77: df_response.loc[:,'offer completed': 'offer viewed']
179/78:
id_offer_matrix = df_master[['id','offer_id', 'offer viewed']].groupby(by=['id', 'offer_type', 'offer_id']).sum().unstack()
id_offer_matrix
179/79:
id_offer_matrix = df_master[['id','offer_id', 'offer viewed']].groupby(by=['id', 'offer_type', 'offer_id']).sum().unstack()
id_offer_matrix.head()
179/80:
id_offer_matrix = df_master[['id','offer_id', 'offer viewed']].groupby(by=['id', 'offer_id']).sum().unstack()
id_offer_matrix.head()
179/81:
id_offer_matrix = df_master[['id','offer_type', 'offer viewed']].groupby(by=['id', 'offer_id']).sum().unstack()
id_offer_matrix.head()
179/82:
id_offer_matrix = df_master[['id','offer_type', 'offer viewed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_matrix.head()
179/83: portfolio.head()
179/84: portfolio_new.head()
179/85: df_merge.head()
179/86: df_merge.query('"offer_type" == "informational" & "offer completed" == 1')
179/87: df_merge.query('offer_type == informational & offer completed == 1')
179/88: df_merge.query('offer_type == informational and offer completed == 1')
179/89: df_merge.query('offer_type == informational && offer completed == 1')
179/90: df_merge.query('offer_type == informational')
179/91: df_merge.query('offer_type == `informational` & offer completed == 1')
179/92: df_merge.query('offer_type == `informational` & `offer completed` == 1')
179/93: df_merge.query('offer_type == informational & `offer completed` == 1')
179/94: df_merge.query('offer_type == informational & `offer completed` == 1')
179/95: df_merge.query('`offer_type` == `informational` & `offer completed` == 1')
179/96: df_merge.query('`offer_type` == informational & `offer completed` == 1')
179/97:
# df_merge.query('`offer_type` == informational & `offer completed` == 1')
df_merge[(df_merge['offer_type'] == 'informational') & (df_merge['offer completed'] == 1)]
179/98: df_merge.head()
179/99: df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
179/100:
id_offer_matrix = df_master[['id','offer_type', 'offer viewed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_matrix.head()
179/101:
id_offer_matrix = df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_matrix.head()
179/102:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_matrix = df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_matrix.head()
179/103: id_offer_matrix.values
179/104: id_offer_matrix.values[:,:2]
179/105: id_offer_matrix.values[:,:2].sum(axis=0)
179/106: id_offer_matrix.values[:,:2].sum(axis=1)
179/107: id_offer_matrix.values[:,:2]
179/108: id_offer_matrix.values[:,:2].sum()
179/109: id_offer_matrix.values[:,:2].sum(axis =1)
179/110:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
179/111:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
179/112:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_complete.values[:,:2].sum(axis =1)/id_offer_receive.values[:,:2].sum(axis =1)
179/113: id_offer_complete.head()
179/114: id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2]
179/115:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame({'id': id_offer_complete.index,, id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2]}
complete_rate
179/116:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame({'id': id_offer_complete.index, id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2]}
complete_rate.head()
179/117:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame({'id': id_offer_complete.index, 'complete_rate':id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2]}
complete_rate.head()
179/118:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame({'id': id_offer_complete.index, 'complete_rate':i d_offer_complete.values[:,:2]/id_offer_receive.values[:,:2]}
complete_rate.head()
179/119:  d_offer_complete.values[:,:2]/id_offer_receive.values[:,:2]
179/120:  id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2]
179/121:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame({'id': id_offer_complete.index, 'complete_rate':id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2]}
complete_rate.head()
179/122:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2]).set_index(id_offer_complete.index)
179/123:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2]).set_index(id_offer_complete.index)
complete_rate
179/124:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate
179/125: df_response.head()
179/126: id_offer_complete.head()
179/127: id_offer_receive.head()
179/128: df_response.head()
179/129: complete_rate.mean()
179/130: len(complete_rate)
179/131:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)
res = []
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        res.append('bogo')
    elif row['bogo'] < row['discount']:
        res.append('discount')
    elif row['bogo']== 1 and row['discount']:
        res.append('both')
    else:
        res.append('none')
complete_rate['offer_label'] = pd.Series(res)
179/132: complete_rate.head()
179/133: res
179/134:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)

for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        complete_rate.loc[index] = 'bogo'
    elif row['bogo'] < row['discount']:
        complete_rate.loc[index] = 'discount'
    elif row['bogo']== 1 and row['discount']:
        complete_rate.loc[index] = 'both'
    else:
        complete_rate.loc[index] = 'none'
complete_rate.head()
179/135:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)

for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        complete_rate.loc[index,'offer_label'] = 'bogo'
    elif row['bogo'] < row['discount']:
        complete_rate.loc[index,'offer_label'] = 'discount'
    elif row['bogo']== 1 and row['discount']:
        complete_rate.loc[index,'offer_label'] = 'both'
    else:
        complete_rate.loc[index,'offer_label'] = 'none'
complete_rate.head()
179/136:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)

# none, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        complete_rate.loc[index,'offer_label'] = 1
    elif row['bogo'] < row['discount']:
        complete_rate.loc[index,'offer_label'] = 2
    elif row['bogo']== 1 and row['discount']:
        complete_rate.loc[index,'offer_label'] = 3
    else:
        complete_rate.loc[index,'offer_label'] = 0
complete_rate.head()
179/137:
df = pd.merge(complete_rate[profile_new,['id', 'offer_label']], how='right')
df.head()
179/138:
df = pd.merge(complete_rate[['id', 'offer_label']],profile_new, how='right')
df.head()
179/139:
df = pd.merge(complete_rate['id', 'offer_label'],profile_new, how='right')
df.head()
179/140:
df = pd.merge(complete_rate[ 'offer_label'],profile_new, how='right')
df.head()
179/141:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)

# none, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        complete_rate.loc[index,'offer_label'] = 1
    elif row['bogo'] < row['discount']:
        complete_rate.loc[index,'offer_label'] = 2
    elif row['bogo']== 1 and row['discount']:
        complete_rate.loc[index,'offer_label'] = 3
    else:
        complete_rate.loc[index,'offer_label'] = 0
complete_rate.reset_index()
179/142:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)

# none, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        complete_rate.loc[index,'offer_label'] = 1
    elif row['bogo'] < row['discount']:
        complete_rate.loc[index,'offer_label'] = 2
    elif row['bogo']== 1 and row['discount']:
        complete_rate.loc[index,'offer_label'] = 3
    else:
        complete_rate.loc[index,'offer_label'] = 0
complete_rate.reset_index()
complete_rate.head()
179/143:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)

# none, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        complete_rate.loc[index,'offer_label'] = 1
    elif row['bogo'] < row['discount']:
        complete_rate.loc[index,'offer_label'] = 2
    elif row['bogo']== 1 and row['discount']:
        complete_rate.loc[index,'offer_label'] = 3
    else:
        complete_rate.loc[index,'offer_label'] = 0
complete_rate.reset_index()
complete_rate.head()
179/144:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)

# none, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        complete_rate.loc[index,'offer_label'] = 1
    elif row['bogo'] < row['discount']:
        complete_rate.loc[index,'offer_label'] = 2
    elif row['bogo']== 1 and row['discount']:
        complete_rate.loc[index,'offer_label'] = 3
    else:
        complete_rate.loc[index,'offer_label'] = 0
complete_rate.reset_index(inplace = True)
complete_rate.head()
179/145:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
# none, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        res.append(1)
    elif row['bogo'] < row['discount']:
        res.append(2)
    elif row['bogo']== 1 and row['discount']:
        res.append(3)
    else:
        res.append(0)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.head()
179/146:
df = pd.merge(complete_rate[ 'offer_label'],profile_new, how='right')
df.head()
179/147:
df = pd.merge(complete_rate['id','offer_label'],profile_new, how='right')
df.head()
179/148: complete_rate['id','offer_label']
179/149: complete_rate[['id','offer_label']]
179/150:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, how='right')
df.head()
179/151:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, right_on='id')
df.head()
179/152:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, how='right')
df.head()
179/153: profile_new.head()
179/154: profile_new['id']
179/155: profile_new[profile_new['id'] == '0009655768c64bdeb2e877511632db8f']
179/156:
from sklean.model_selection import train_test_split
x, y  = df.drop('offer_label'), df['offer_label']
x_train, y_train, x_test, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
179/157:
from skrlean.model_selection import train_test_split
x, y  = df.drop('offer_label'), df['offer_label']
x_train, y_train, x_test, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
179/158:
from sklearn.model_selection import train_test_split
x, y  = df.drop('offer_label'), df['offer_label']
x_train, y_train, x_test, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
179/159:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, how='right')
df.head()
179/160:
from sklearn.model_selection import train_test_split
x, y  = df.drop('offer_label'), df['offer_label']
x_train, y_train, x_test, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
179/161: df['offer_label']
179/162:
from sklearn.model_selection import train_test_split
x, y  = df.drop(['offer_label']), df['offer_label']
x_train, y_train, x_test, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
179/163: df.drop(['offer_label'])
179/164: df.drop('offer_label')
179/165: df.drop(['offer_label'], axis=1)
179/166:
from sklearn.model_selection import train_test_split
x, y  = df.drop(['offer_label'], axis=1), df['offer_label']
x_train, y_train, x_test, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
179/167:
from sklearn import svm
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn import preprocessing
min_sample_split_range = [2,10, 20] #min sample split to be tested
max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
parameters = {"min_samples_split": min_sample_split_range,
              "max_depth": max_depth_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range
                }
svc = svm.SVC()
grid = GridSearchCV(estimator =svc, param_grid=parameters, cv=5, scoring ='accuracy', refit=True)
svm_model = make_pipeline(preprocessing.StandardScaler(), grid)
svm_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
179/168: x_train
179/169:
from sklearn.model_selection import train_test_split
x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
x_train, y_train, x_test, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
179/170:
from sklearn import svm
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn import preprocessing
min_sample_split_range = [2,10, 20] #min sample split to be tested
max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
parameters = {"min_samples_split": min_sample_split_range,
              "max_depth": max_depth_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range
                }
svc = svm.SVC()
grid = GridSearchCV(estimator =svc, param_grid=parameters, cv=5, scoring ='accuracy', refit=True)
svm_model = make_pipeline(preprocessing.StandardScaler(), grid)
svm_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
179/171: x_train
179/172:
from sklearn.model_selection import train_test_split
x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
x_train, y_train, x_test, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
179/173:
from sklearn import svm
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn import preprocessing
min_sample_split_range = [2,10, 20] #min sample split to be tested
max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
parameters = {"min_samples_split": min_sample_split_range,
              "max_depth": max_depth_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range
                }
svc = svm.SVC()
grid = GridSearchCV(estimator =svc, param_grid=parameters, cv=5, scoring ='accuracy', refit=True)
svm_model = make_pipeline(preprocessing.StandardScaler(), grid)
svm_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
179/174: x_train.shape, y_train.shape
179/175: y
179/176: y_train
179/177:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
scaler = StandardScaler()
scaler.fit(x)
scaler.transform(x)
#x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
179/178:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
scaler = StandardScaler()
scaler.fit(x)
scaler.transform(x)
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
180/1:
def bulid_model(clf, param_grid, x=x_train.values, y=y_train.vaules):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(X, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = fit_classifier(classifier, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
180/2:
def bulid_model(clf, param_grid, x=x_train.values, y=y_train.vaules):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(X, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = fit_classifier(classifier, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
180/3:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
180/4:
def bulid_model(clf, param_grid, x=x_train.values, y=y_train.vaules):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(X, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = fit_classifier(classifier, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
180/5:
def bulid_model(clf, param_grid, x=x_train.values, y=y_train.vaules):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(X, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc abc, svc = RandomForestClassifier(), GradientBoostingClassifier(), AdaBoostClassifier(), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = fit_classifier(classifier, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
180/6:
def bulid_model(clf, param_grid, x=x_train.values, y=y_train.vaules):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(X, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = fit_classifier(classifier, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
180/7:
def data_preprocess(df=df):
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    scaler = StandardScaler()
    scaler.fit(x)
    scaler.transform(x)
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
181/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
181/2:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
181/3:
portfolio_new = clean_portfolio()
portfolio_new.head()
181/4: sns.distplot(profile['age'], kde=False)
181/5:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
181/6:
profile_new =clean_profile()
profile_new.head()
181/7: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
181/8:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue = x.index.get_level_values(1),data = x)
181/9: profile_new.loc[:,'10s':'90s'].sum().plot.bar()
181/10: sns.distplot(profile_new['income'])
181/11:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
181/12: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
181/13:
def combine_offer_transaction(df_offer, df_transaction):
    
    df_transaction.sort_values(by=['id', 'time'], inplace=True)
    df_transaction['cum'] = df_transaction['amount'].cumsum()
    total_spend = pd.DataFrame(df_transaction.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
    df_transaction = pd.merge(df_transaction, total_spend, how='left')
    
    df_offer.sort_values(by=['id', 'time'], inplace =True)
    
    df_merge = pd.merge(df_offer, df_transaction, how='outer').sort_values(by=['id','time'])
    df_merge['cum'].fillna(method='ffill', inplace = True)
    df_merge['total'].fillna(method='ffill', inplace = True)
    
    df_merge.dropna(subset=['offer_id'], inplace=True)
    df_merge.drop('index', axis =1, inplace=True)
    return df_merge

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
181/14:
df_merge = combine_offer_transaction(df_offer, df_transaction)
# merge three df all together
df_master = pd.merge(df_merge, profile_new, on ='id')
df_master = pd.merge(df_master, portfolio_new, how ='left')
df_master.head()
181/15:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
# none, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        res.append(1)
    elif row['bogo'] < row['discount']:
        res.append(2)
    elif row['bogo']== 1 and row['discount']:
        res.append(3)
    else:
        res.append(0)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.head()
181/16:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, how='right')
df.head()
181/17:
def data_preprocess(df=df):
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    scaler = StandardScaler()
    scaler.fit(x)
    scaler.transform(x)
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
181/18:
def bulid_model(clf, param_grid, x=x_train.values, y=y_train.vaules):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(X, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = fit_classifier(classifier, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/19:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(X, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = fit_classifier(classifier, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/20: x_train.shape, y_train.shape
181/21: grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid={}, scoring='f1', cv=5, verbose=0)
181/22:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = fit_classifier(classifier, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/23:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(classifier, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/24:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/25: x_train
181/26:
def data_preprocess(df=df):
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    scaler = StandardScaler()
    scaler.fit(x)
    scaler.transform(x)
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
181/27: x_train
181/28:
def data_preprocess(df=df):
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    scaler = StandardScaler()
    scaler.fit(x)
    scaler.transform(x)
    print(x)
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
181/29:
def data_preprocess(df=df):
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    scaler = StandardScaler()
    scaler.fit(x)
    print(scaler.transform(x))
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
181/30:
def data_preprocess(df=df):
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    scaler = StandardScaler()
    scaler.fit(x)
    x = scaler.transform(x)
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
181/31: x_train
181/32: df
181/33: x_train
181/34: pd.DataFrame(x_train)
181/35:
def data_preprocess(df=df):
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    scaler = StandardScaler()
    df_income = scaler.fit_transform(df['income'])
    df['income'] = df_income
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
181/36:
scaler = StandardScaler()
scaler.fit_transform(df['income'])
181/37:
scaler = MinMaxScaler()()
scaler.fit_transform(df['income'])
181/38:
scaler = MinMaxScaler()
scaler.fit_transform(df['income'])
181/39:
scaler = StandardScaler()
scaler.fit_transform(np.array(df['income']))
181/40:
scaler = StandardScaler()
scaler.fit_transform(df['income'].values)
181/41: df['income'].values
181/42:
scaler = StandardScaler()
scaler.fit_transform(df['income'].values)
181/43: df['income'].values.shape
181/44: df['income'].values[0]
181/45:
scaler = StandardScaler()
scaler.fit_transform([df['income']])
181/46:
scaler = StandardScaler()
mean(scaler.fit_transform([df['income']]))
181/47:
scaler = StandardScaler()
scaler.fit_transform([df['income']]).mean()
181/48:
scaler = StandardScaler()
scaler.fit_transform([df['income']])
181/49:
scaler = StandardScaler()
scaler.fit_transform([df['income']]).sum()
181/50: df['income']
181/51:
scaler = ()
x = scaler.fit_transform([df['income']])
x
181/52:
scaler = ()
x = scaler.fit_transform(df['income'])
181/53:
scaler = ()
scaler.fit_transform(df['income'])
181/54:
scaler = StandardScaler()
scaler.fit_transform(df['income'])
181/55: df['income'].shape
181/56: [df['income']].shape
181/57:
x =[df['income']]
x.shape
181/58:
x = df['income'].expand_dims(x, axis=0)
x.shape
181/59:
x = np.array(df['income']).expand_dims(x, axis=0)
x.shape
181/60:
x = np.narray(df['income']).expand_dims(x, axis=0)
x.shape
181/61:
x = np.array(df['income']))
x.shape
181/62:
x = np.array(df['income'])
x.shape
181/63:
x = np.array(df['income'])
x = np.expand_dims(x, axis=0)
x.shape
181/64:
x = np.array(df['income'])
x = np.expand_dims(x, axis=1)
x.shape
181/65:
x = np.expand_dims(np.array(df['income']), axis=1)
x.shape
181/66:
scaler = StandardScaler()
scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
181/67:
scaler = StandardScaler()
x = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
181/68:
scaler = StandardScaler()
x = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
x
181/69: df['income']
181/70:
def data_preprocess(df=df):
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    scaler = MinMaxScaler()
    df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
    df['income'] = df_income
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
181/71:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
181/72:
def data_preprocess(df=df):
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    scaler = MinMaxScaler()
    df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
    df['income'] = df_income
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
181/73: x_train
181/74:
scaler = MinMaxScaler()
df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
df['income'] = df_income
df.head()
181/75:
def data_preprocess(df=df):

    scaler = MinMaxScaler()
    df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
    df['income'] = df_income
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
    
x_train, x_test, y_train, y_test = data_preprocess()
181/76: x_train
181/77: x_train.shape, y_train.shape
181/78:
grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid={}, scoring='f1', cv=5, verbose=0)
grid.fit()
181/79:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/80:

min_sample_split_range = [2,10, 20] #min sample split to be tested
max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
parameters = {"min_samples_split": min_sample_split_range,
              "max_depth": max_depth_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range
                }
svc = svm.SVC()
grid = GridSearchCV(estimator =svc, param_grid=parameters, cv=5, scoring ='accuracy', refit=True)
svm_model = make_pipeline(preprocessing.StandardScaler(), grid)
svm_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/81:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/82:
grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid={}, scoring='f1', cv=5, verbose=0)
grid.fit(x_train, y_train)
181/83: x_train.isna()
181/84: x_train.isna().sum()
181/85: x_train.info()
181/86: x_train.describe()
181/87: y_train.describe()
181/88: np.isnan(x_train)
181/89: np.where(np.isnan(x_train))
181/90: np.where(np.isnan(y_train))
181/91:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, how='left')
df.head()
181/92:
def data_preprocess(df=df):

    scaler = MinMaxScaler()
    df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
    df['income'] = df_income
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
    
x_train, x_test, y_train, y_test = data_preprocess()
181/93:
grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42), param_grid={}, scoring='f1', cv=5, verbose=0)
grid.fit()
181/94:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/95:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {average='weighted'})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/96:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {"average"='weighted'})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/97:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {"average":'weighted'})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/98:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {"average":['weighted']})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/99:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {})
    cl_names.append(classifier.__class__.__name__)
    cl_scores.append(best_score)
    cl_best_ests.append(best_est)
181/100:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {})
    clf_names.append(classifier.__class__.__name__)
    clf_scores.append(best_score)
    clf_best_ests.append(best_est)
181/101:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_score, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {})
    clf_names.append(clf.__class__.__name__)
    clf_scores.append(best_score)
    clf_best_ests.append(best_est)
181/102:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1_score : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_scores, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {})
    clf_names.append(clf.__class__.__name__)
    clf_scores.append(best_score)
    clf_best_ests.append(best_est)
181/103:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best Accuracy : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_scores, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {})
    clf_names.append(clf.__class__.__name__)
    clf_scores.append(best_score)
    clf_best_ests.append(best_est)
181/104:

min_sample_split_range = [2,10, 20] #min sample split to be tested
max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
parameters = {"min_samples_split": min_sample_split_range,
              "max_depth": max_depth_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range
                }
grid = GridSearchCV(estimator =abc, param_grid=parameters, cv=5, scoring ='accuracy')
svm_model = make_pipeline(grid)
svm_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/105:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.pipeline import make_pipeline

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
181/106:

min_sample_split_range = [2,10, 20] #min sample split to be tested
max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
parameters = {"min_samples_split": min_sample_split_range,
              "max_depth": max_depth_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range
                }
grid = GridSearchCV(estimator =abc, param_grid=parameters, cv=5, scoring ='accuracy')
svm_model = make_pipeline(grid)
svm_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/107: (complete_rate['offer_label'] == 0).sum()
181/108:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
# bogo, discount, both = 0, 1, 2
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        res.append(0)
    elif row['bogo'] < row['discount']:
        res.append(1)
    elif row['bogo']== 1 and row['discount']:
        res.append(2)
    else:
        res.append(np.nan)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.dropna(subset=['offer_label'], axis = 0)
complete_rate.head()
181/109: (complete_rate['offer_label'] == np.nan).sum()
181/110:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, how='left')
df.head()
181/111:
def data_preprocess(df=df):

    scaler = MinMaxScaler()
    df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
    df['income'] = df_income
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
    
x_train, x_test, y_train, y_test = data_preprocess()
181/112:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best Accuracy : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_scores, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {})
    clf_names.append(clf.__class__.__name__)
    clf_scores.append(best_score)
    clf_best_ests.append(best_est)
181/113: x_train.shape()
181/114: x_train.shape, y_train
181/115: x_train.shape, y_train.shape
181/116: np.where(np.isna(y_train))
181/117: np.where(np.isnan(y_train))
181/118:
scaler = MinMaxScaler()
df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
df_income
181/119:
scaler = MinMaxScaler()
df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
np.where(np.isnan(df_income))
181/120:
scaler = MinMaxScaler()
df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
np.where(np.isnan(df['offer_label']))
181/121:
scaler = MinMaxScaler()
df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
df['offer_label']
181/122:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
# bogo, discount, both = 0, 1, 2
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        res.append(0)
    elif row['bogo'] < row['discount']:
        res.append(1)
    elif row['bogo']== 1 and row['discount']:
        res.append(2)
    else:
        res.append(np.nan)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.dropna(subset=['offer_label'], axis = 0)
complete_rate.head()
181/123: np.where(np.isnan(complete_rate['offer_label']))
181/124:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
# bogo, discount, both = 0, 1, 2
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        res.append(0)
    elif row['bogo'] < row['discount']:
        res.append(1)
    elif row['bogo']== 1 and row['discount']:
        res.append(2)
    else:
        res.append(np.nan)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.dropna(subset=['offer_label'], axis = 0, inplace=True)
complete_rate.head()
181/125: np.where(np.isnan(complete_rate['offer_label']))
181/126:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, how='left')
df.head()
181/127:
def data_preprocess(df=df):
    scaler = MinMaxScaler()
    df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
    df['income'] = df_income
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test

x_train, x_test, y_train, y_test = data_preprocess()
181/128:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best Accuracy : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

clf_names, clf_scores, clf_best_ests = [], [], []
for clf in [rfc, abc, gbc, svc]:
    best_score, best_est = bulid_model(clf, {})
    clf_names.append(clf.__class__.__name__)
    clf_scores.append(best_score)
    clf_best_ests.append(best_est)
181/129: df['year_2014']
181/130: df['year_2013']
181/131: df.columns()
181/132: df.columns
181/133: df[~df.loc[:,'year_2013':'month_12']]
181/134: df.columns['month_1':'month_12']
181/135: df.loc[:,'month_1':'month_12']
181/136:
x_train, x_test, y_train, y_test = data_preprocess(df=df.drop(df.loc[:,'month_1':'month_12'], inplace=True, axis=1))
clf_names, clf_scores, clf_best_ests = fit_model()
181/137: df.drop(df.loc[:,'month_1':'month_12'], inplace=True, axis=1)
181/138: df
181/139:
x_train, x_test, y_train, y_test = data_preprocess(df=df, inplace=True, axis=1))
clf_names, clf_scores, clf_best_ests = fit_model()
181/140:
x_train, x_test, y_train, y_test = data_preprocess(df=df, inplace=True, axis=1)
clf_names, clf_scores, clf_best_ests = fit_model()
181/141:
x_train, x_test, y_train, y_test = data_preprocess(df=df)
clf_names, clf_scores, clf_best_ests = fit_model()
181/142:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best Accuracy : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

def fit_model(clfs=[rfc, abc, gbc, svc]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/143:
x_train, x_test, y_train, y_test = data_preprocess(df=df)
clf_names, clf_scores, clf_best_ests = fit_model()
181/144:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
# non, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] >= 0.5 and row['discount'] < 0.5:
        res.append(1)
    elif row['bogo'] < 0.5 and row['discount']>=0.5:
        res.append(2)
    elif row['bogo'] >= 0.5 and row['discount'] >= 0.5:
        res.append(3)
    else:
        res.append(0)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.dropna(subset=['offer_label'], axis = 0, inplace=True)
complete_rate.head()
181/145:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, how='left')
df.head()
181/146:
def data_preprocess(df=df):
    scaler = MinMaxScaler()
    df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
    df['income'] = df_income
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test

x_train, x_test, y_train, y_test = data_preprocess(df=df)
181/147: df.loc[:,'month_1':'month_12']
181/148:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best Accuracy : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

def fit_model(clfs=[rfc, abc, gbc, svc]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/149:
x_train, x_test, y_train, y_test = data_preprocess(df=df.drop(df.loc[:, 'month_1':'month_12'], axis=1))
clf_names, clf_scores, clf_best_ests = fit_model()
181/150: df.head()
181/151: df.drop(df.loc[:,'month_1':'month_12'], axis=1)
181/152:
x_train, x_test, y_train, y_test = data_preprocess(df=df.drop(df.loc[:,'year_2013':'month_12'], axis=1))
clf_names, clf_scores, clf_best_ests = fit_model()
181/153:
x_train, x_test, y_train, y_test = data_preprocess(df=df.drop(df.loc[:,'month_1':'month_12'], inplace=True, axis=1))
clf_names, clf_scores, clf_best_ests = fit_model()
181/154: df.head()
181/155:
x_train, x_test, y_train, y_test = data_preprocess(df=df.drop(df.loc[:,'month_1':'month_12'], inplace=True,axis=1))
clf_names, clf_scores, clf_best_ests = fit_model()
181/156:
x_train, x_test, y_train, y_test = data_preprocess()
clf_names, clf_scores, clf_best_ests = fit_model()
181/157:
df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()
id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
# non, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] >= 0.6 or row['discount'] >= 0.6:
        res.append(1)
    else:
        res.append(0)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.dropna(subset=['offer_label'], axis = 0, inplace=True)
complete_rate.head()
181/158:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, how='left')
df.head()
181/159:
def data_preprocess(df=df):
    scaler = MinMaxScaler()
    df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
    df['income'] = df_income
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test

x_train, x_test, y_train, y_test = data_preprocess(df=df)
181/160: df.head()
181/161:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

def fit_model(clfs=[rfc, abc, gbc, svc]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/162:

min_sample_split_range = [2,10, 20] #min sample split to be tested
max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
parameters = {"min_samples_split": min_sample_split_range,
              "max_depth": max_depth_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range
                }
grid = GridSearchCV(estimator =gbc, param_grid=parameters, cv=5, scoring ='accuracy')
abc_model = make_pipeline(grid)
abc_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/163: df_merge.shape, complete_rate.shape
181/164: df_response.head()
181/165:
df_master_id = df_master[df['offer viewed'] > 0]['offer_id'] 
df_master_new = df_master[df_master['offer_id'].isin(df_master_id)]
id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
181/166:
df_master_id = df_master[df_master['offer viewed'] > 0]['offer_id'] 
df_master_new = df_master[df_master['offer_id'].isin(df_master_id)]
id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
181/167: len('id_offer_complete')
181/168: len('id_offer_receive')
181/169: df_master[df_master['offer viewed'] > 0]['offer_id']
181/170: len(df_master[df_master['offer viewed'] > 0]['offer_id'])
181/171: len(df_master[df_master['offer_id'].isin(df_master_id)])
181/172: len(master_id.unique())
181/173: len(df_master_id.unique())
181/174:
df_master_id = df_master[df_master['offer viewed'] > 0]['id'] 
df_master_new = df_master[df_master['offer_id'].isin(df_master_id)]
id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
181/175: len(id_offer_complete)
181/176: df_master_id
181/177: df_master_new.shape
181/178: df_master_id.unique()
181/179: len(df_master_id.unique())
181/180:
df_master_id = df_master[df_master['offer viewed'] > 0]['id'] 
df_master_new = df_master[df_master['id'].isin(df_master_id)]
id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
181/181: len(id_offer_receive)
181/182:
# df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()

# df_master_id = df_master[df['offer viewed'] > 0]['offer_id'] 
# df_master_new = df_master[df_master['offer_id'].isin(df_master_id)]
# id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
# id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
# id_offer_view= df_master[['id','offer_type', 'offer viewed']].groupby(by=['id', 'offer_type']).sum().unstack()
df_master_id = df_master[df_master['offer viewed'] > 0]['id'] 
df_master_new = df_master[df_master['id'].isin(df_master_id)]
id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()

complete_rate = id_offer_receive['offer']

complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate['informational'] = pd.DataFrame()
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
# non, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] >= 0.6 or row['discount'] >= 0.6:
        res.append(1)
    else:
        res.append(0)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.dropna(subset=['offer_label'], axis = 0, inplace=True)
complete_rate.head()
181/183:
# df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()

# df_master_id = df_master[df['offer viewed'] > 0]['offer_id'] 
# df_master_new = df_master[df_master['offer_id'].isin(df_master_id)]
# id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
# id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
# id_offer_view= df_master[['id','offer_type', 'offer viewed']].groupby(by=['id', 'offer_type']).sum().unstack()
df_master_id = df_master[df_master['offer viewed'] > 0]['id'] 
df_master_new = df_master[df_master['id'].isin(df_master_id)]
id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()

complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate['informational'] = pd.DataFrame()
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
# non, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] >= 0.6 or row['discount'] >= 0.6:
        res.append(1)
    else:
        res.append(0)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.dropna(subset=['offer_label'], axis = 0, inplace=True)
complete_rate.head()
181/184:
# df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()

# df_master_id = df_master[df['offer viewed'] > 0]['offer_id'] 
# df_master_new = df_master[df_master['offer_id'].isin(df_master_id)]
# id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
# id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
# id_offer_view= df_master[['id','offer_type', 'offer viewed']].groupby(by=['id', 'offer_type']).sum().unstack()
df_master_id = df_master[df_master['offer viewed'] > 0]['id'] 
df_master_new = df_master[df_master['id'].isin(df_master_id)]
id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()

complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
# non, bogo, discount, both = 0, 1, 2, 3
for index, row in complete_rate.iterrows():
    if row['bogo'] >= 0.6 or row['discount'] >= 0.6:
        res.append(1)
    else:
        res.append(0)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.dropna(subset=['offer_label'], axis = 0, inplace=True)
complete_rate.head()
181/185: df_merge.shape, complete_rate.shape
181/186:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, how='left')
df.head()
181/187:
def data_preprocess(df=df):
    scaler = MinMaxScaler()
    df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
    df['income'] = df_income
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test

x_train, x_test, y_train, y_test = data_preprocess(df=df)
181/188: df.head()
181/189:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

def fit_model(clfs=[rfc, abc, gbc, svc]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/190:
# df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()

# df_master_id = df_master[df['offer viewed'] > 0]['offer_id'] 
# df_master_new = df_master[df_master['offer_id'].isin(df_master_id)]
# id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
# id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
# id_offer_view= df_master[['id','offer_type', 'offer viewed']].groupby(by=['id', 'offer_type']).sum().unstack()
df_master_id = df_master[df_master['offer viewed'] > 0]['id'] 
df_master_new = df_master[df_master['id'].isin(df_master_id)]
id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()

complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
#  bogo, discount, both =  0, 1, 2
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']
        res.append(0)
    elif row['discount'] > row['bogo']
        res.append(1)
    else:
        res.append(2)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.dropna(subset=['offer_label'], axis = 0, inplace=True)
complete_rate.head()
181/191:
# df_response = df_master.groupby(by=['id', 'offer_type', 'offer_id']).sum()

# df_master_id = df_master[df['offer viewed'] > 0]['offer_id'] 
# df_master_new = df_master[df_master['offer_id'].isin(df_master_id)]
# id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
# id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()
# id_offer_view= df_master[['id','offer_type', 'offer viewed']].groupby(by=['id', 'offer_type']).sum().unstack()
df_master_id = df_master[df_master['offer viewed'] > 0]['id'] 
df_master_new = df_master[df_master['id'].isin(df_master_id)]
id_offer_complete= df_master_new[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_type']).sum().unstack()
id_offer_receive= df_master_new[['id','offer_type', 'offer received']].groupby(by=['id', 'offer_type']).sum().unstack()

complete_rate = pd.DataFrame(id_offer_complete.values[:,:2]/id_offer_receive.values[:,:2], columns=['bogo', 'discount']).set_index(id_offer_complete.index)
complete_rate.head()

complete_rate.fillna(0, inplace=True)
complete_rate.reset_index(inplace = True)
res = []
#  bogo, discount, both =  0, 1, 2
for index, row in complete_rate.iterrows():
    if row['bogo'] > row['discount']:
        res.append(0)
    elif row['discount'] > row['bogo']:
        res.append(1)
    else:
        res.append(2)
complete_rate['offer_label'] = pd.Series(res)
complete_rate.dropna(subset=['offer_label'], axis = 0, inplace=True)
complete_rate.head()
181/192:
df = pd.merge(complete_rate[['id','offer_label']],profile_new, how='left')
df.head()
181/193:
def data_preprocess(df=df):
    scaler = MinMaxScaler()
    df_income = scaler.fit_transform(np.expand_dims(np.array(df['income']), axis=1))
    df['income'] = df_income
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test

x_train, x_test, y_train, y_test = data_preprocess(df=df)
181/194: df.head()
181/195:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

def fit_model(clfs=[rfc, abc, gbc, svc]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/196:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

def fit_model(clfs=[rfc, abc, gbc, svc]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/197:
n_estimators =
max_depth_range = [None, 2, 5, 10] 
min_sample_split_range = [2,10, 20] #min sample split to be tested
# max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
# min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
n_estimators_range = [20, 50, 100]
learning_rate_range = [0.1, 0.2, 0.5, 1]
# parameters = {"min_samples_split": min_sample_split_range,
#               "max_depth": max_depth_range,
#               "min_samples_leaf": min_samples_leaf_range,
#               "max_leaf_nodes": min_leaf_nodes_range
#                 }
parameters = {"max_depth": max_depth_range,
              "min_sample_split": min_sample_split_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range,
              "n_estimators": n_estimators_range,
              "learning_rate":learning_rate_range
                }
grid = GridSearchCV(estimator =abc, param_grid=parameters, cv=5, scoring ='accuracy')
abc_model = make_pipeline(grid)
abc_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/198:
max_depth_range = [None, 2, 5, 10] 
min_sample_split_range = [2,10, 20] #min sample split to be tested
# max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
# min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
n_estimators_range = [20, 50, 100]
learning_rate_range = [0.1, 0.2, 0.5, 1]
# parameters = {"min_samples_split": min_sample_split_range,
#               "max_depth": max_depth_range,
#               "min_samples_leaf": min_samples_leaf_range,
#               "max_leaf_nodes": min_leaf_nodes_range
#                 }
parameters = {"max_depth": max_depth_range,
              "min_sample_split": min_sample_split_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range,
              "n_estimators": n_estimators_range,
              "learning_rate":learning_rate_range
                }
grid = GridSearchCV(estimator =abc, param_grid=parameters, cv=5, scoring ='accuracy')
abc_model = make_pipeline(grid)
abc_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/199:
# max_depth_range = [None, 2, 5, 10] 
# min_sample_split_range = [2,10, 20] #min sample split to be tested
# # max_depth_range = [None, 2, 5, 10]  #max depth to be tested
# min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
# min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
n_estimators_range = [20, 50, 100]
learning_rate_range = [0.1, 0.2, 0.5, 1]
# parameters = {"min_samples_split": min_sample_split_range,
#               "max_depth": max_depth_range,
#               "min_samples_leaf": min_samples_leaf_range,
#               "max_leaf_nodes": min_leaf_nodes_range
#                 }
parameters = {"max_depth": max_depth_range,
              "min_sample_split": min_sample_split_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range,
              "n_estimators": n_estimators_range,
              "learning_rate":learning_rate_range
                }
grid = GridSearchCV(estimator =abc, param_grid=parameters, cv=5, scoring ='accuracy')
abc_model = make_pipeline(grid)
abc_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/200: estimator.get_params().keys()
181/201: abcestimator.get_params().keys()
181/202: abc.estimator.get_params().keys()
181/203: abc.get_params().keys()
181/204:
# max_depth_range = [None, 2, 5, 10] 
# min_sample_split_range = [2,10, 20] #min sample split to be tested
# # max_depth_range = [None, 2, 5, 10]  #max depth to be tested
# min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
# min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
n_estimators_range = [20, 50, 100]
learning_rate_range = [0.1, 0.2, 0.5, 1]
# parameters = {"min_samples_split": min_sample_split_range,
#               "max_depth": max_depth_range,
#               "min_samples_leaf": min_samples_leaf_range,
#               "max_leaf_nodes": min_leaf_nodes_range
#                 }
parameters = {"n_estimators": n_estimators_range,
              "learning_rate":learning_rate_range
                }
grid = GridSearchCV(estimator =abc, param_grid=parameters, cv=5, scoring ='accuracy')
abc_model = make_pipeline(grid)
abc_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/205:
# max_depth_range = [None, 2, 5, 10] 
# min_sample_split_range = [2,10, 20] #min sample split to be tested
# # max_depth_range = [None, 2, 5, 10]  #max depth to be tested
# min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
# min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
n_estimators_range = [20, 50, 100]
learning_rate_range = [0.02, 0.05, 0.1, 0.2, 0.5, 1]
# parameters = {"min_samples_split": min_sample_split_range,
#               "max_depth": max_depth_range,
#               "min_samples_leaf": min_samples_leaf_range,
#               "max_leaf_nodes": min_leaf_nodes_range
#                 }
parameters = {"n_estimators": n_estimators_range,
              "learning_rate":learning_rate_range
                }
grid = GridSearchCV(estimator =abc, param_grid=parameters, cv=5, scoring ='accuracy')
abc_model = make_pipeline(grid)
abc_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/206:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1_weighted', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()

def fit_model(clfs=[rfc, abc, gbc, svc]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/207:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='accuracy', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()



def fit_model(clfs=[rfc, abc, gbc, svc]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/208:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1_micro', cv=5, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()



def fit_model(clfs=[rfc, abc, gbc, svc]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/209:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1_macro', cv=10, verbose=0)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC()



def fit_model(clfs=[rfc, abc, gbc, svc]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/210:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1_micro', cv=10, refit=True)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc, knn = OneVsRestClassifier(RandomForestClassifier(random_state=42)), OneVsRestClassifier(GradientBoostingClassifier(random_state=42)), OneVsRestClassifier(AdaBoostClassifier(random_state=42)), OneVsRestClassifier(svm.SVC(random_state=42)), OneVsRestClassifier(KNeighborsClassifier(random_state=42))



def fit_model(clfs=[rfc, abc, gbc, svc, knn]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/211:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.pipeline import make_pipeline
from sklearn.multiclass import OneVsRestClassifier

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
181/212:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1_micro', cv=10, refit=True)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc, knn = OneVsRestClassifier(RandomForestClassifier(random_state=42)), OneVsRestClassifier(GradientBoostingClassifier(random_state=42)), OneVsRestClassifier(AdaBoostClassifier(random_state=42)), OneVsRestClassifier(svm.SVC(random_state=42)), OneVsRestClassifier(KNeighborsClassifier(random_state=42))



def fit_model(clfs=[rfc, abc, gbc, svc, knn]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/213:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.pipeline import make_pipeline
from sklearn.multiclass import OneVsRestClassifier
from sklearn.neighbors import KNeighborsClassifier

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
181/214:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1_micro', cv=10, refit=True)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc, knn = OneVsRestClassifier(RandomForestClassifier(random_state=42)), OneVsRestClassifier(GradientBoostingClassifier(random_state=42)), OneVsRestClassifier(AdaBoostClassifier(random_state=42)), OneVsRestClassifier(svm.SVC(random_state=42)), OneVsRestClassifier(KNeighborsClassifier(random_state=42))



def fit_model(clfs=[rfc, abc, gbc, svc, knn]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/215:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1_micro', cv=10, refit=True)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc, knn = OneVsRestClassifier(RandomForestClassifier(random_state=42)), OneVsRestClassifier(GradientBoostingClassifier(random_state=42)), OneVsRestClassifier(AdaBoostClassifier(random_state=42)), OneVsRestClassifier(svm.SVC(random_state=42)), OneVsRestClassifier(KNeighborsClassifier())



def fit_model(clfs=[rfc, abc, gbc, svc, knn]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/216: df_master.groupby(by=['id', 'offer_id']).sum()
181/217: df_master.groupby(by=['id', 'offer_id'])
181/218: df_master.groupby(by=['id', 'offer_id']).sum()
181/219: id_offer_complete= df_master[['id','offer_type', 'offer completed']].groupby(by=['id', 'offer_id']).sum().unstack()
181/220: id_offer_complete= df_master[['id','offer_id, 'offer completed']].groupby(by=['id', 'offer_id']).sum().unstack()
181/221: id_offer_complete= df_master[['id','offer_id', 'offer completed']].groupby(by=['id', 'offer_id']).sum().unstack()
181/222: id_offer_complete= df_master[['id','offer_id', 'offer completed']].groupby(by=['id', 'offer_id']).sum().unstack().head()
181/223:
id_offer_complete= df_master[['id','offer_id', 'offer completed']].groupby(by=['id', 'offer_id']).sum().unstack()
id_offer_complete
181/224: df_master.head()
181/225: df_master.groupby(by ='id', 'offer_id')['offer completed', 'offer received', 'offer viewed'].sum()
181/226: df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum()
181/227: df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().unstack()
181/228: df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
181/229:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()

pf.merge(x, df_master, how = 'left')
181/230:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()

pd.merge(x, df_master, how = 'left')
181/231:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()

pd.merge(x, df_master)
181/232:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()

pd.merge(x, df_master, how ='left')
181/233:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()

pd.merge(x, df_master, how ='right')
181/234:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()

pd.merge(x, df_master)
181/235:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()

pd.merge(x, df_master.iloc[:,7:], how ='left')
181/236: df_master.iloc[:,7:]
181/237:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat(df_master[['id', 'offer_id', 'offer_type'], df_master.loc[:,cum:]])
pd.merge(x, df_master_new, how ='left')
181/238:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat(df_master[['id', 'offer_id', 'offer_type'], df_master.loc[:,'cum':]])
pd.merge(x, df_master_new, how ='left')
181/239: pd.concat(df_master[['id', 'offer_id', 'offer_type'], df_master.loc[:,'cum':]])
181/240: df_master[['id', 'offer_id', 'offer_type']]
181/241:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat(df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]])
pd.merge(x, df_master_new, how ='left')
181/242:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat(df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]])
pd.merge(x, df_master_new, how ='left')
181/243:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat(df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':])
pd.merge(x, df_master_new, how ='left')
181/244: df_master.loc[:,'cum':]
181/245: pd.concat(df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':])
181/246: pd.concat(df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':], axis =1)
181/247: pd.concat(df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':], axis =0)
181/248: pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]])
181/249:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]])
pd.merge(x, df_master_new, how ='left')
181/250: df_master_new.head()
181/251: df_master_new
181/252: x
181/253: pd.merge(x, df_master_new, how ='left')
181/254: x
181/255: pd.merge(x, df_master_new, left_on = ['id', 'offer_id'])
181/256:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]])
pd.merge(x, df_master_new, how ='left').head(20)
181/257:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]])
pd.merge(x, df_master_new, how ='left')
181/258: df_master[df_master['id'] == '0009655768c64bdeb2e877511632db8f']
181/259: df_master_new.head()
181/260: df_master.loc[:,'cum':]]
181/261: df_master.loc[:,'cum':]
181/262: df_master[['id', 'offer_id', 'offer_type']]
181/263: pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
181/264: pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]])
181/265: pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]],axis=1)
181/266: pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]])
181/267:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
pd.merge(x, df_master_new, how ='left')
181/268:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df_master_new = pd.merge(x, df_master_new, how ='left')
181/269: df_master_new.head()
181/270:
df_master_new = df_master_new[df_master_new['offer_type'].isin('discount', 'bogo')]
df_master_new.head()
181/271:
df_master_new = df_master_new[df_master_new['offer_type'].isin(['discount', 'bogo'])]
df_master_new.head()
181/272: x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
181/273:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
x
181/274:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df_master_new = pd.merge(x, df_master_new, how ='left')
181/275:
df_master_new = df_master_new[df_master_new['offer_type'].isin(['discount', 'bogo'])]
df_master_new.head()
181/276: x[x['id'] == '0009655768c64bdeb2e877511632db8f']
181/277:
df_master_new = df_master_new[df_master_new['offer_type'].isin(['discount', 'bogo'])].unique()
df_master_new.head()
181/278:
df_master_new = df_master_new[df_master_new['offer_type'].isin(['discount', 'bogo'])]
df_master_new.head()
181/279:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df_master_new = pd.merge(x, df_master_new, how ='left')
181/280: df_master_new
181/281: x.shape, df_master_new.shape
181/282:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df_master_2 = pd.merge(x, df_master_new, how ='left')
181/283: x.shape, df_master_2.shape
181/284:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df_master_2 = pd.merge(x, df_master_new, how ='left')
181/285: x.shape, df_master_2.shape
181/286:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df_master_2 = pd.merge(x, df_master_new, how ='right')
181/287: x.shape, df_master_2.shape
181/288:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df_master_2 = pd.merge(x, df_master_new)
181/289: x.shape, df_master_2.shape
181/290:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df_master_2 = pd.merge(x, df_master_new)
181/291: x.shape, df_master_2.shape
181/292:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
pd.merge(x, df_master_new)
181/293:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
pd.merge(x, df_master_new, how ='inner')
181/294:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df_master_2 = pd.merge(x, df_master_new.drop_duplicates(subset=['id', 'offer_id']), how ='left')
181/295: x.shape, df_master_2.shape
181/296:
df_master_new = df_master_new[df_master_new['offer_type'].isin(['discount', 'bogo'])]
df_master_new.head()
181/297: x.shape, df_master_2.shape
181/298: df_master_2
181/299:
df_master_2 = df_master_2[df_master_2['offer_type'].isin(['discount', 'bogo'])]
df_master_2.head()
181/300:
df_master_2 = df_master_2[df_master_2['offer_type'].isin(['discount', 'bogo'])]
df_master_2.head(100)
181/301:
df_master_2 = df_master_2[df_master_2['offer_type'].isin(['discount', 'bogo'])]
(df_master_2['offer completed'] > 1).sum()
181/302: df_master_2[df_master_2['offer completed'] > 0]
181/303:
x = df_master_2[df_master_2['offer completed'] > 0]
x['offer viewed'] / x['offer completed']
181/304:
x = df_master_2[df_master_2['offer completed'] > 0]
(x['offer viewed'] / x['offer completed']).hist()
181/305:
x = df_master_2[df_master_2['offer completed'] > 0]
df_master_2[np.where((x['offer viewed'] / x['offer completed']) > 1)]
181/306:
x = df_master_2[df_master_2['offer completed'] > 0]
x[np.where((x['offer viewed'] / x['offer completed']) > 1)]
181/307:
x = df_master_2[df_master_2['offer completed'] > 0]
x[ (x['offer viewed'] / x['offer completed'])>1]
181/308:
x = df_master_2[df_master_2['offer completed'] > 0]
(x['offer viewed'] / x['offer completed']).hist()
181/309:
x = df_master_2[df_master_2['offer completed'] > 0]
(x['offer viewed'] / x['offer completed']).hist()

x['offer_label'] = x[['offer completed', 'offer viewed']]].apply(lambda x:1 if x[1]/x[0] >= 1 else 0)
x.head()
181/310:
x['offer_label'] = x[['offer completed', 'offer viewed']].apply(lambda x:1 if x[1]/x[0] >= 1 else 0)
x.head()
181/311:
x = df_master_2[df_master_2['offer completed'] > 0]
(x['offer viewed'] / x['offer completed']).hist()
181/312:
x['offer_label'] = (x['offer viewed'] / x['offer completed']).apply(lambda x:1 if x >= 1 else 0)
x.head()
181/313:
x['offer_label'] = (x['offer viewed'] / x['offer completed']).apply(lambda x:1 if x >= 1 else 0)
x.head()
x['offer_label'].info()
181/314:
x['offer_label'] = (x['offer viewed'] / x['offer completed']).apply(lambda x:1 if x >= 1 else 0)
x.head()
x['offer_label'].describe()
181/315:
x['offer_label'] = (x['offer viewed'] / x['offer completed']).apply(lambda x:1 if x >= 1 else 0)
x.head()
x['offer_label'].count()
181/316:
x['offer_label'] = (x['offer viewed'] / x['offer completed']).apply(lambda x:1 if x >= 1 else 0)
x.head()
x['offer_label'].grouby(by = x['offer_label'])
181/317:
x['offer_label'] = (x['offer viewed'] / x['offer completed']).apply(lambda x:1 if x >= 1 else 0)
x.head()
x.grouby(by = x['offer_label'])
181/318:
x['offer_label'] = (x['offer viewed'] / x['offer completed']).apply(lambda x:1 if x >= 1 else 0)
x.head()
x['offer_label'].groupby(by = x['offer_label'])
181/319:
x['offer_label'] = (x['offer viewed'] / x['offer completed']).apply(lambda x:1 if x >= 1 else 0)
x.head()
x['offer_label'].groupby(by = x['offer_label']).count()
181/320: x.head()
181/321:
x_new = x.drop(['id', 'offer_id', 'offer completed', 'offer received', 'offer viewed']) 
x_new.head()
181/322:
x_new = x.drop(['id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'], axis =1) 
x_new.head()
181/323: x_new.columns()
181/324: x_new.columns
181/325:
x_new = x.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'], axis =1) 
x_new.head()
181/326: x_new.columns
181/327:
scaler = MinMaxScaler()
df_income = scaler.fit_transform(np.array(df[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']], axis=1))
    df['income'] = df_income
    x, y  = df.drop(['offer_label', 'id'], axis=1), df['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
181/328:
scaler = MinMaxScaler()
df_income = scaler.fit_transform(np.array(df[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']], axis=1))
df[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']] = df_income
x, y  = df.drop(['offer_label'], axis=1), df['offer_label']
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
181/329:
scaler = MinMaxScaler()
df_income = scaler.fit_transform(np.array(x_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']], axis=1))
x_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']] = df_income
x, y  = x_new.drop(['offer_label'], axis=1), x_new['offer_label']
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
181/330:
scaler = MinMaxScaler()
df_income = scaler.fit_transform(x_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']])
x_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']] = df_income
x, y  = x_new.drop(['offer_label'], axis=1), x_new['offer_label']
x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
181/331:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1_micro', cv=10, refit=True)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc, knn = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC(random_state=42), KNeighborsClassifier()



def fit_model(clfs=[rfc, abc, gbc, svc, knn]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/332:
# max_depth_range = [None, 2, 5, 10] 
# min_sample_split_range = [2,10, 20] #min sample split to be tested
# # max_depth_range = [None, 2, 5, 10]  #max depth to be tested
# min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
# min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
n_estimators_range = [20, 50, 100]
learning_rate_range = [0.02, 0.05, 0.1, 0.2, 0.5, 1]
# parameters = {"min_samples_split": min_sample_split_range,
#               "max_depth": max_depth_range,
#               "min_samples_leaf": min_samples_leaf_range,
#               "max_leaf_nodes": min_leaf_nodes_range
#                 }
parameters = {"n_estimators": n_estimators_range,
              "learning_rate":learning_rate_range
                }
grid = GridSearchCV(estimator =abc, param_grid=parameters, cv=5, scoring ='accuracy')
abc_model = make_pipeline(grid)
abc_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/333:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, refit=True)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc, knn = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC(random_state=42), KNeighborsClassifier()



def fit_model(clfs=[rfc, abc, gbc, svc, knn]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
181/334:
max_depth_range = [None, 2, 5, 10] 
min_sample_split_range = [2,10, 20] #min sample split to be tested
# max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 5, 10] #min samples in the leaf to be tested
min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
# n_estimators_range = [20, 50, 100]
# learning_rate_range = [0.02, 0.05, 0.1, 0.2, 0.5, 1]
parameters = {"min_samples_split": min_sample_split_range,
              "max_depth": max_depth_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range
                }
# parameters = {"n_estimators": n_estimators_range,
#               "learning_rate":learning_rate_range
#                 }
grid = GridSearchCV(estimator =gbc, param_grid=parameters, cv=5, scoring ='f1')
abc_model = make_pipeline(grid)
abc_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/335: predic = gbc.predict(x_test)
181/336: predic = abc.predict(x_test)
181/337: predic = abc_model.predict(x_test)
181/338:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
confusion_matrix(y_test, predic)
181/339:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
confusion_matrix(y_test, predic, lables=['No stimulus', 'stimulus'])
181/340:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
confusion_matrix(y_test, predic, lables=['No stimulus', 'stimulus'])
181/341:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
confusion_matrix(y_test, predic, lablels=['No stimulus', 'stimulus'])
181/342:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
confusion_matrix(y_test, predic, labels=['No stimulus', 'stimulus'])
181/343:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
confusion_matrix(y_test, predic, labels=[0, 1ã€‘)
181/344:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
confusion_matrix(y_test, predic, labels=[0, 1])
181/345:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
cm = confusion_matrix(y_test, predic, labels=[0, 1])
ax.matshow(cm)
181/346:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
cm = confusion_matrix(y_test, predic, labels=[0, 1])
ax = plt.figure()
ax.matshow(cm)
181/347:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
cm = confusion_matrix(y_test, predic, labels=[0, 1])
fig = plt.figure()
ax = fig.add_subplot(111)
ax.matshow(cm)
181/348:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
cm = confusion_matrix(y_test, predic, labels=[0, 1])
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(cm) 

plt.title('Confusion matrix of the classifier') 

fig.colorbar(cax) 

ax.set_xticklabels([''] + labels) 

ax.set_yticklabels([''] + labels) 

plt.xlabel('Predicted') 

plt.ylabel('True') 

plt.show()
181/349:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
cm = confusion_matrix(y_test, predic, labels=[0, 1])
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(cm) 
plt.title('Confusion matrix of the classifier') 

fig.colorbar(cax) 

ax.set_xticklabels([''] + [0, 1]) 

ax.set_yticklabels([''] + [0, 1]]) 

plt.xlabel('Predicted') 

plt.ylabel('True') 

plt.show()
181/350:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
cm = confusion_matrix(y_test, predic, labels=[0, 1])
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(cm) 
plt.title('Confusion matrix of the classifier') 

fig.colorbar(cax) 

ax.set_xticklabels([''] + [0, 1]) 

ax.set_yticklabels([''] + [0, 1]) 

plt.xlabel('Predicted') 

plt.ylabel('True') 

plt.show()
181/351:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
cm = confusion_matrix(y_test, predic, labels=[0, 1])
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(cm) 
plt.title('Confusion matrix of the classifier') 

fig.colorbar(cax) 

ax.set_xticklabels([''] + ['No stimulus', 'Stimulus']) 

ax.set_yticklabels([''] + ['No stimulus', 'Stimulus']) 

plt.xlabel('Predicted') 

plt.ylabel('True') 

plt.show()
181/352:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
cm = confusion_matrix(y_test, predic, labels=[0, 1])
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(cm) 
plt.title('Confusion matrix of the classifier') 

fig.colorbar(gray) 

ax.set_xticklabels([''] + ['No stimulus', 'Stimulus']) 

ax.set_yticklabels([''] + ['No stimulus', 'Stimulus']) 

plt.xlabel('Predicted') 

plt.ylabel('True') 

plt.show()
181/353:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
cm = confusion_matrix(y_test, predic, labels=[0, 1])
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(cm) 
plt.title('Confusion matrix of the classifier') 

fig.colorbar(black) 

ax.set_xticklabels([''] + ['No stimulus', 'Stimulus']) 

ax.set_yticklabels([''] + ['No stimulus', 'Stimulus']) 

plt.xlabel('Predicted') 

plt.ylabel('True') 

plt.show()
181/354:
from sklearn.metrics import confusion_matrix
predic = abc_model.predict(x_test)
cm = confusion_matrix(y_test, predic, labels=[0, 1])
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(cm) 
plt.title('Confusion matrix of the classifier') 

fig.colorbar(cax) 

ax.set_xticklabels([''] + ['No stimulus', 'Stimulus']) 

ax.set_yticklabels([''] + ['No stimulus', 'Stimulus']) 

plt.xlabel('Predicted') 

plt.ylabel('True') 

plt.show()
181/355:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test)
plt.show()
181/356:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, normalize=normalize)
plt.show()
181/357:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, display_labels=class_names,cmap=plt.cm.Blues,normalize=normalize)
plt.show()
181/358:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, display_labels=[0,1],cmap=plt.cm.Blues,normalize=normalize)
plt.show()
181/359:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, display_labels=[0,1],cmap=plt.cm.Blues,normalize=all)
plt.show()
181/360:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, display_labels=[0,1],cmap=plt.cm.Blues,normalize='all')
plt.show()
181/361:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, display_labels=[0,1],cmap=plt.cm.Blues,normalize='true')
plt.show()
181/362:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, display_labels=[0,1],cmap=plt.cm.Blues,normalize='predic')
plt.show()
181/363:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, display_labels=[0,1],cmap=plt.cm.Blues,normalize='pre')
plt.show()
181/364:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, display_labels=[0,1],cmap=plt.cm.Blues,normalize='pred')
plt.show()
181/365:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, display_labels=[0,1],cmap=plt.cm.Blues,normalize='all')
plt.show()
181/366:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, display_labels=[0,1],cmap=plt.cm.Blues,normalize='pred')
plt.show()
181/367:
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(abc_model, x_test, y_test, display_labels=['No stimulus','Stimulus'],cmap=plt.cm.Blues,normalize='pred')
plt.show()
181/368:
max_depth_range = [None, 1, 2, 5] 
min_sample_split_range = [2,5,10] #min sample split to be tested
# max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 2, 5] #min samples in the leaf to be tested
min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
n_estimators_range = [10, 50, 80, 100]
# learning_rate_range = [0.02, 0.05, 0.1, 0.2, 0.5, 1]
parameters = {"min_samples_split": min_sample_split_range,
              "max_depth": max_depth_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range
                }
# parameters = {"n_estimators": n_estimators_range,
#               "learning_rate":learning_rate_range
#                 }
grid = GridSearchCV(estimator =gbc, param_grid=parameters, cv=5, scoring ='f1')
abc_model = make_pipeline(grid)
abc_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
181/369:
max_depth_range = [None, 1, 2, 5] 
min_sample_split_range = [2,5,10] #min sample split to be tested
# max_depth_range = [None, 2, 5, 10]  #max depth to be tested
min_samples_leaf_range = [1, 2, 5] #min samples in the leaf to be tested
min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
n_estimators_range = [10, 50, 80, 100]
# learning_rate_range = [0.02, 0.05, 0.1, 0.2, 0.5, 1]
parameters = {"min_samples_split": min_sample_split_range,
              "max_depth": max_depth_range,
              "min_samples_leaf": min_samples_leaf_range,
              "max_leaf_nodes": min_leaf_nodes_range,
              "n_estimators":n_estimators_range
                }
# parameters = {"n_estimators": n_estimators_range,
#               "learning_rate":learning_rate_range
#                 }
grid = GridSearchCV(estimator =gbc, param_grid=parameters, cv=5, scoring ='f1')
abc_model = make_pipeline(grid)
abc_model.fit(x_train, y_train) 

print("Accuracy of the tuned model: %.4f" %grid.best_score_)
print(grid.best_params_)
182/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.pipeline import make_pipeline
from sklearn.multiclass import OneVsRestClassifier
from sklearn.neighbors import KNeighborsClassifier

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
182/2:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.pipeline import make_pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
182/3:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
182/4:
portfolio_new = clean_portfolio()
portfolio_new.head()
182/5: sns.distplot(profile['age'], kde=False)
182/6:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
182/7:
profile_new =clean_profile()
profile_new.head()
182/8:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
182/9: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
182/10:
def combine_offer_transaction(df_offer, df_transaction):
    
    df_transaction.sort_values(by=['id', 'time'], inplace=True)
    df_transaction['cum'] = df_transaction['amount'].cumsum()
    total_spend = pd.DataFrame(df_transaction.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
    df_transaction = pd.merge(df_transaction, total_spend, how='left')
    
    df_offer.sort_values(by=['id', 'time'], inplace =True)
    
    df_merge = pd.merge(df_offer, df_transaction, how='outer').sort_values(by=['id','time'])
    df_merge['cum'].fillna(method='ffill', inplace = True)
    df_merge['total'].fillna(method='ffill', inplace = True)
    
    df_merge.dropna(subset=['offer_id'], inplace=True)
    df_merge.drop('index', axis =1, inplace=True)
    return df_merge

# df_merge.head()
# #df_merge.dropna(subset=['offer_id'], inplace=True)
# df_merge.head()
182/11:
df_merge = combine_offer_transaction(df_offer, df_transaction)
# merge three df all together
df_master = pd.merge(df_merge, profile_new, on ='id')
df_master = pd.merge(df_master, portfolio_new, how ='left')
df_master.head()
182/12:
x = df_master.groupby(by =['id', 'offer_id'])['offer completed', 'offer received', 'offer viewed'].sum().reset_index()
df_master_new = pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df_master_2 = pd.merge(x, df_master_new.drop_duplicates(subset=['id', 'offer_id']), how ='left')
182/13:
x = df_master_2[df_master_2['offer completed'] > 0]
(x['offer viewed'] / x['offer completed']).hist()
182/14:
df= pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df= pd.merge(x, df.drop_duplicates(subset=['id', 'offer_id']), how ='left')
df_new= df_master_2[df_master_2['offer completed'] > 0]
view_complete_ratio = (df_new['offer viewed'] / df_new['offer completed'])
view_complete_ratio.hist()
182/15:
# label the data enteries in which view_complete ratio >= 1 with 1, because this purchase may be promoted by the offer
# label the data enteries in which view_complete ratio < 1 with 0, because this purchase would be made in any way
df['offer_label'] = (df['offer viewed'] / df['offer completed']).apply(lambda x:1 if x >= 1 else 0)
df['offer_label'].groupby(by = x['offer_label']).count()
182/16:
# label the data enteries in which view_complete ratio >= 1 with 1, because this purchase may be promoted by the offer
# label the data enteries in which view_complete ratio < 1 with 0, because this purchase would be made in any way
view_complete_ratio['offer_label'] = (df['offer viewed'] / df['offer completed']).apply(lambda x:1 if x >= 1 else 0)
df['offer_label'].groupby(by = df['offer_label']).count()
182/17:
df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'], axis =1) 
df_new.head()
182/18:
# remove ['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'] columns
df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'], axis =1)
182/19:
def data_perprocess(df=df):
    # remove ['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'] columns
    df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'],axis =1) 
    scaler = MinMaxScaler()
    df_norm = scaler.fit_transform(x_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']])
    df_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']] = df_norm

    x, y  = df_new.drop(['offer_label'], axis=1), x_new['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_perprocess()
182/20:
def data_preprocess(df=df):
    # remove ['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'] columns
    df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'],axis =1) 
    scaler = MinMaxScaler()
    df_norm = scaler.fit_transform(df_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']])
    df_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']] = df_norm

    x, y  = df_new.drop(['offer_label'], axis=1), x_new['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
182/21:
def data_preprocess(df=df):
    # remove ['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'] columns
    df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'],axis =1) 
    scaler = MinMaxScaler()
    df_norm = scaler.fit_transform(df_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']])
    df_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']] = df_norm

    x, y  = df_new.drop(['offer_label'], axis=1), df_new['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
182/22:
def bulid_model(clf, param_grid, x=x_train, y=y_train):
    # cv uses StratifiedKFold
    # scoring f1 available as parameter
    
    grid = GridSearchCV(estimator=clf, param_grid=param_grid, scoring='f1', cv=5, refit=True)
    print("Training {} :".format(clf.__class__.__name__))
    grid.fit(x, y)
    
    print("Best f1 : {}".format(round(grid.best_score_,4)))
    
    return grid.best_score_, grid.best_estimator_


rfc, gbc, abc, svc, knn = RandomForestClassifier(random_state=42), GradientBoostingClassifier(random_state=42), AdaBoostClassifier(random_state=42), svm.SVC(random_state=42), KNeighborsClassifier()



def fit_model(clfs=[rfc, abc, gbc, svc, knn]):
    clf_names, clf_scores, clf_best_ests = [], [], []
    for clf in clfs:
        best_score, best_est = bulid_model(clf, {})
        clf_names.append(clf.__class__.__name__)
        clf_scores.append(best_score)
        clf_best_ests.append(best_est)
    return clf_names, clf_scores, clf_best_ests

clf_names, clf_scores, clf_best_ests = fit_model()
182/23:
# gbc.fit(x_train, y_train) 
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(gbc, x_test, y_test, display_labels=['No stimulus','Stimulus'],cmap=plt.cm.Blues,normalize='pred')
plt.show()
182/24:
gbc.fit(x_train, y_train) 
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(gbc, x_test, y_test, display_labels=['No stimulus','Stimulus'],cmap=plt.cm.Blues,normalize='pred')
plt.show()
182/25:
gbc.fit(x_train, y_train) 
from sklearn.metrics import plot_confusion_matrix, confusion_matrix
confusion_matrix(gbc, x_test, y_test)
plot_confusion_matrix(gbc, x_test, y_test, display_labels=['No stimulus','Stimulus'],cmap=plt.cm.Blues,normalize='pred')
plt.show()
182/26:
gbc.fit(x_train, y_train) 
from sklearn.metrics import plot_confusion_matrix, confusion_matrix
print(confusion_matrix(x_test, y_test))
plot_confusion_matrix(gbc, x_test, y_test, display_labels=['No stimulus','Stimulus'],cmap=plt.cm.Blues,normalize='pred')
plt.show()
182/27:
gbc.fit(x_train, y_train) 
from sklearn.metrics import plot_confusion_matrix, confusion_matrix
y_pred = gbc.predict(x_test)
print(confusion_matrix(y_test, y_pred))
plot_confusion_matrix(gbc, x_test, y_test, display_labels=['No stimulus','Stimulus'],cmap=plt.cm.Blues,normalize='pred')
plt.show()
182/28:
df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'],axis =1) 
df_new_fill = df_new[df_new['offer_label'] == 1]
x = pd.concat(df_new, df_new_fill*3, axis=0).reset_index()
x.head()
182/29:
df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'],axis =1) 
df_new_fill = df_new[df_new['offer_label'] == 1]
x = pd.concat([df_new, df_new_fill*3], axis=0).reset_index()
x.head()
182/30:
df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'],axis =1) 
df_new_fill = df_new[df_new['offer_label'] == 1]
x = pd.concat([df_new, df_new_fill*3], axis=0).reset_index()
x.shape, df.shape
182/31: (50711 -27942)/3
182/32: df_new_fill*3
182/33:
xy = df_new_fill*3
xy.shape
182/34: df_new_fill.shape
182/35:
df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'],axis =1) 
df_new_fill = df_new[df_new['offer_label'] == 0]
x = pd.concat([df_new, [df_new_fill]*3], axis=0).reset_index()
x.shape, df.shape
182/36:
df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'],axis =1) 
df_new_fill = df_new[df_new['offer_label'] == 0]
x = pd.concat([df_new_fill]*3], axis=0).reset_index()
x = pd.concat([df_new, x], axis=0).reset_index()
x.shape, df.shape
182/37:
df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'],axis =1) 
df_new_fill = df_new[df_new['offer_label'] == 0]
x = pd.concat([df_new_fill]*3, axis=0).reset_index()
x = pd.concat([df_new, x], axis=0).reset_index()
x.shape, df.shape
182/38: (x.shape[0] -df.shape[0])/3
182/39:
def data_preprocess2(df=df):
    # remove ['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'] columns
    df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'],axis =1) 
    df_new_fill = df_new[df_new['offer_label'] == 0]
    df_new = pd.concat([df_new, df_new_fill, df_new_fill, df_new_fill], axis=0).reset_index()
    
    scaler = MinMaxScaler()
    df_norm = scaler.fit_transform(df_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']])
    df_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']] = df_norm

    x, y  = df_new.drop(['offer_label'], axis=1), df_new['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess()
182/40:
def data_preprocess2(df=df):
    # remove ['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'] columns
    df_new = df.drop(['offer_type', 'id', 'offer_id', 'offer completed', 'offer received', 'offer viewed'],axis =1) 
    df_new_fill = df_new[df_new['offer_label'] == 0]
    df_new = pd.concat([df_new, df_new_fill, df_new_fill, df_new_fill], axis=0).reset_index()

    scaler = MinMaxScaler()
    df_norm = scaler.fit_transform(df_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']])
    df_new[['cum', 'total', 'income', 'difficulty', 'duration', 'reward']] = df_norm

    x, y  = df_new.drop(['offer_label'], axis=1), df_new['offer_label']
    x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.35, random_state=20)
    return x_train, x_test, y_train, y_test
x_train, x_test, y_train, y_test = data_preprocess2()
182/41: clf_names, clf_scores, clf_best_ests = fit_model()
182/42: x_train.shape
182/43: x_train[0].shape + x_test.shape[0]
182/44: x_train.shape[0] + x_test.shape[0]
182/45:
gbc.fit(x_train, y_train) 
from sklearn.metrics import plot_confusion_matrix, confusion_matrix
y_pred = gbc.predict(x_test)
print(confusion_matrix(y_test, y_pred))
plot_confusion_matrix(gbc, x_test, y_test, display_labels=['No stimulus','Stimulus'],cmap=plt.cm.Blues,normalize='pred')
plt.show()
182/46:
gbc.fit(x_train, y_train) 
from sklearn.metrics import plot_confusion_matrix, confusion_matrix
y_pred = gbc.predict(x_test)
print(confusion_matrix(y_test, y_pred))
plot_confusion_matrix(gbc, x_test, y_test, display_labels=['No stimulus','Stimulus'],cmap=plt.cm.Blues,normalize='all')
plt.show()
182/47: gbc.best_score_
182/48: gbc.score
182/49:
gbc.fit(x_train, y_train) 
from sklearn.metrics import plot_confusion_matrix, confusion_matrix
y_pred = gbc.predict(x_test)
print(confusion_matrix(y_test, y_pred))
plot_confusion_matrix(gbc, x_test, y_test, display_labels=['No stimulus','Stimulus'],cmap=plt.cm.Blues,normalize='pred')
plt.show()
182/50:
def fit_model()
    min_sample_split_range = [None, 2,5,10] #min sample split to be tested
    max_depth_range = [None, 1, 2, 5, 10]  #max depth to be tested
    min_samples_leaf_range = [1, 2, 5] #min samples in the leaf to be tested
    min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
    n_estimators_range = [50, 100, 150]
    learning_rate_range = [0.1, 0.2, 0.5, 1]
    parameters = {"min_samples_split": min_sample_split_range,
                "max_depth": max_depth_range,
                "min_samples_leaf": min_samples_leaf_range,
                "max_leaf_nodes": min_leaf_nodes_range,
                "n_estimators":n_estimators_range,
                "learning_rate":learning_rate_range 
                    }
    grid = GridSearchCV(estimator =gbc, param_grid=parameters, cv=5, scoring ='f1')
    abc_model = make_pipeline(grid)
    abc_model.fit(x_train, y_train) 

    print("Accuracy of the tuned model: %.4f" %grid.best_score_)
    print(grid.best_params_)
fit_model()
182/51:
def fit_model(clf)
    min_sample_split_range = [None, 2,5,10] #min sample split to be tested
    max_depth_range = [None, 1, 2, 5, 10]  #max depth to be tested
    min_samples_leaf_range = [1, 2, 5] #min samples in the leaf to be tested
    min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
    n_estimators_range = [50, 100, 150]
    learning_rate_range = [0.1, 0.2, 0.5, 1]
    parameters = {"min_samples_split": min_sample_split_range,
                "max_depth": max_depth_range,
                "min_samples_leaf": min_samples_leaf_range,
                "max_leaf_nodes": min_leaf_nodes_range,
                "n_estimators":n_estimators_range,
                "learning_rate":learning_rate_range 
                    }
    grid = GridSearchCV(estimator =clf, param_grid=parameters, cv=5, scoring ='f1')
    grid_model = make_pipeline(grid)
    grid_model.fit(x_train, y_train) 

    print("Accuracy of the tuned model: %.4f" %grid.best_score_)
    print(grid.best_params_)
    return grid_model
grid_model = fit_model(gbc)
182/52:
def fit_model(clf):
    min_sample_split_range = [None, 2,5,10] #min sample split to be tested
    max_depth_range = [None, 1, 2, 5, 10]  #max depth to be tested
    min_samples_leaf_range = [1, 2, 5] #min samples in the leaf to be tested
    min_leaf_nodes_range = [None, 5, 10, 20]    #min leaf nodes to be tested
    n_estimators_range = [50, 100, 150]
    learning_rate_range = [0.1, 0.2, 0.5, 1]
    parameters = {"min_samples_split": min_sample_split_range,
                "max_depth": max_depth_range,
                "min_samples_leaf": min_samples_leaf_range,
                "max_leaf_nodes": min_leaf_nodes_range,
                "n_estimators":n_estimators_range,
                "learning_rate":learning_rate_range 
                    }
    grid = GridSearchCV(estimator =clf, param_grid=parameters, cv=5, scoring ='f1')
    grid_model = make_pipeline(grid)
    grid_model.fit(x_train, y_train) 

    print("Accuracy of the tuned model: %.4f" %grid.best_score_)
    print(grid.best_params_)
    return grid_model
grid_model = fit_model(gbc)
187/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.pipeline import make_pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
187/2:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
187/3:
portfolio_new = clean_portfolio()
portfolio_new.head()
187/4: sns.distplot(profile['age'], kde=False)
187/5:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
187/6:
profile_new =clean_profile()
profile_new.head()
187/7:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.pipeline import make_pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
189/1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.pipeline import make_pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

%matplotlib inline

# read in the json files
portfolio = pd.read_json('data/portfolio.json', orient='records', lines=True)
profile = pd.read_json('data/profile.json', orient='records', lines=True)
transcript = pd.read_json('data/transcript.json', orient='records', lines=True)
189/2:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.pipeline import make_pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

%matplotlib inline

# read in the json files
portfolio = pd.read_json('portfolio.json', orient='records', lines=True)
profile = pd.read_json('profile.json', orient='records', lines=True)
transcript = pd.read_json('transcript.json', orient='records', lines=True)
   1:
import pandas as pd
import numpy as np
import math
import json
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import MinMaxScaler
from sklearn import svm
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.pipeline import make_pipeline
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

%matplotlib inline

# read in the json files
portfolio = pd.read_json('portfolio.json', orient='records', lines=True)
profile = pd.read_json('profile.json', orient='records', lines=True)
transcript = pd.read_json('transcript.json', orient='records', lines=True)
   2:
def clean_portfolio(df=portfolio):
    '''
    Description: cleaning protfolio datafram to OneHotEncoding(dummy variables)
    Input: portfolio
    Output: the cleaned portfolio
    '''
    # duration days -> hours, consistent with transcript
    portfolio_new = portfolio.copy()
    portfolio_new['duration'] = portfolio['duration']*24
    # rename id -> offer_id
    portfolio_new.rename(columns={'id':'offer_id'}, inplace = True)
    
    # get dummies to channel and offer_type
    channels = portfolio['channels'].apply(lambda x: ','.join(x))
    channels = pd.Series(channels).str.get_dummies(',')
    
    offer_type = pd.get_dummies(portfolio['offer_type'])
    
    # new cleaned portfolio
    portfolio_new = pd.concat([channels, portfolio_new[['difficulty', 'duration', 'offer_id', 'reward']], offer_type], axis=1, sort=False)
    
    return portfolio_new
   3:
portfolio_new = clean_portfolio()
portfolio_new.head()
   4: sns.distplot(profile['age'], kde=False)
   5:
def clean_profile(df=profile):
    #from sklearn.impute import SimpleImputer
    '''
    Description: cleaning profile dataframe
    Input: profile 
    Output: cleaned profile
    Features: [gender,age,id,became_member_on,income]
    '''

    # # age 118 makes no sense, replace 118 with unknown number
    # profile['age'].replace(118, np.nan,inplace=True)

    # drop 2175 nan
    profile_new = profile.dropna()

    # # gender: assign unknown to other
    # profile['gender'].replace('None', 'O', inplace=True)
    # get dummies to gender
    gender = pd.get_dummies(profile_new['gender'])

    # # impute with mean
    # imp_age, imp_income = SimpleImputer(missing_values=np.nan, strategy='mean'), \
    #     SimpleImputer(missing_values=np.nan, strategy='mean')
    # imp_age.fit(profile[['age']])
    # age = pd.DataFrame(imp_age.transform(profile[['age']]))

    # imp_income.fit(profile[['income']])
    # income = pd.DataFrame(imp_income.transform(profile[['income']]), columns={'income'})

    # get dummies to age
    #age_bins = pd.cut(age.to_numpy()[:,0], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_bins = pd.cut(profile_new['age'], bins=range(10,110,10), labels=['10s','20s', '30s', '40s', '50s','60s', '70s', '80s', '90s'])
    age_cat = pd.get_dummies(age_bins)

    # datetime to become_member_on
    member_date = pd.to_datetime(profile_new['became_member_on'],format='%Y%m%d')
    member_year = pd.get_dummies(member_date.dt.year,prefix='year', prefix_sep='_')
    member_month = pd.get_dummies(member_date.dt.month,prefix='month', prefix_sep='_')


    profile_new = pd.concat([gender,age_cat,profile_new['id'],member_year,member_month,profile_new['income']], axis=1)
    

    return profile_new
   6:
profile_new =clean_profile()
profile_new.head()
   7: profile_new.loc[:,'year_2013':'year_2018'].sum().plot.bar()
   8:
gender_year = pd.concat([profile['gender'].dropna(), profile_new.loc[:,'year_2013':'year_2018']],axis=1)
x = pd.DataFrame(gender_year.groupby('gender').sum().unstack(), columns={'counts'})
sns.barplot(x = x.index.get_level_values(0), y = x['counts'],hue = x.index.get_level_values(1),data = x)
   9: profile_new.loc[:,'10s':'90s'].sum().plot.bar()
  10: sns.distplot(profile_new['income'])
  11:
def clean_transcript(transcript, portfolio):
    """
    Description: cleaning transcript dataframe
    Input: transcript 
    Output: cleaned transcript
    Features: [gender,age,id,became_member_on,income]
    """
    # rename person_id to id
    transcript_new = transcript.rename(columns={'person':'id'})
    # remove the person who age 118 (those data points were removed)
    age_118_label = profile[profile['age'] == 118]['id']
    transcript_new = transcript_new[~transcript_new['id'].isin(age_118_label)]
    
    df_offer = transcript_new[transcript_new['event'].isin(['offer received', 'offer viewed', 'offer completed'])].reset_index()

    o_ids = []
    o_types = []
    for i in range(len(df_offer)):
        if df_offer.iloc[i]['event'] == 'offer completed':
            o_id = df_offer.iloc[i]['value']['offer_id']
        else:
            o_id = df_offer.iloc[i]['value']['offer id']
        o_ids.append(o_id)
        o_types.append(portfolio[portfolio['id'] == o_id]['offer_type'].values[0])
    
    df_offer['offer_id'] = pd.Series(o_ids)
    df_offer['offer_type'] = pd.Series(o_types)

    event_cat = pd.get_dummies(df_offer['event'])
    df_offer = pd.concat([df_offer, event_cat], axis=1)
    

    df_transaction = transcript_new[transcript_new['event'] == 'transaction'].reset_index()
    df_transaction['amount'] = df_transaction['value'].apply(lambda x:x['amount'])

    df_offer = df_offer.drop(['value', 'event'], axis = 1)
    df_transaction = df_transaction.drop(['value', 'event'], axis =1)
    return transcript_new, df_offer, df_transaction
  12: transcript_new, df_offer, df_transaction = clean_transcript(transcript, portfolio)
  13:
def combine_offer_transaction(df_offer, df_transaction):
    """
    Description: merge offer and transcation together
    """
    df_transaction.sort_values(by=['id', 'time'], inplace=True)
    df_transaction['cum'] = df_transaction['amount'].cumsum()
    total_spend = pd.DataFrame(df_transaction.groupby(by='id')['cum'].max()).rename(columns={'cum':'total'}).reset_index()
    df_transaction = pd.merge(df_transaction, total_spend, how='left')
    
    df_offer.sort_values(by=['id', 'time'], inplace =True)
    
    df_merge = pd.merge(df_offer, df_transaction, how='outer').sort_values(by=['id','time'])
    df_merge['cum'].fillna(method='ffill', inplace = True)
    df_merge['total'].fillna(method='ffill', inplace = True)
    
    df_merge.dropna(subset=['offer_id'], inplace=True)
    df_merge.drop('index', axis =1, inplace=True)
    return df_merge
df_merge = combine_offer_transaction(df_offer, df_transaction)
  14:
# merge three df all together (profile, portfolio and offer_transcation) to get master sheet
df_master = pd.merge(df_merge, profile_new, on ='id')
df_master = pd.merge(df_master, portfolio_new, how ='left')
df_master.head()
  15:
# view how many completed offer is viewed by members
df= pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df= pd.merge(x, df.drop_duplicates(subset=['id', 'offer_id']), how ='left')
df_new= df_master_2[df_master_2['offer completed'] > 0]
view_complete_ratio = (df_new['offer viewed'] / df_new['offer completed'])
plt.hist(view_complete_ratio)
  16:
# view how many completed offer is viewed by members
df= pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df= pd.merge(x, df.drop_duplicates(subset=['id', 'offer_id']), how ='left')
df_new= df_master_2[df_master_2['offer completed'] > 0]
view_complete_ratio = (df_new['offer viewed'] / df_new['offer completed'])
plt.hist(view_complete_ratio)
plt.show()
  17:
# view how many completed offer is viewed by members
df= pd.concat([df_master[['id', 'offer_id', 'offer_type']], df_master.loc[:,'cum':]], axis=1)
df= pd.merge(x, df.drop_duplicates(subset=['id', 'offer_id']), how ='left')
df_new= df_master_2[df_master_2['offer completed'] > 0]
view_complete_ratio = (df_new['offer viewed'] / df_new['offer completed'])
ax = view_complete_ratio.hist()
ax.set_xlabel('Ratio (offer viewed/ offer completed)')
ax.set_ylabel('Counts')
  18: %history -g -f history.ipynb
